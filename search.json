[
  {
    "objectID": "posts/2017-08-13-mf-autograd-adagrad.html",
    "href": "posts/2017-08-13-mf-autograd-adagrad.html",
    "title": "Adagrad based matrix factorization",
    "section": "",
    "text": "Customary imports\n\nimport autograd.numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import gridspec\n\n%matplotlib inline\n\n\n\nCreating the matrix to be decomposed\n\nA = np.array([[3, 4, 5, 2],\n                   [4, 4, 3, 3],\n                   [5, 5, 4, 3]], dtype=np.float32).T\n\n\n\nMasking one entry\n\nA[0, 0] = np.NAN\n\n\nA\n\narray([[ nan,   4.,   5.],\n       [  4.,   4.,   5.],\n       [  5.,   3.,   4.],\n       [  2.,   3.,   3.]], dtype=float32)\n\n\n\n\nDefining the cost function\n\ndef cost(param_list):\n    W, H = param_list\n    pred = np.dot(W, H)\n    mask = ~np.isnan(A)\n    return np.sqrt(((pred - A)[mask].flatten() ** 2).mean(axis=None))\n\n\n\nDecomposition params\n\nrank = 2\nlearning_rate=0.01\nn_steps = 10000\n\n\n\nAdagrad routine\n\ndef adagrad_gd(param_init, cost, niter=5, lr=1e-2, eps=1e-8, random_seed=0):\n    \"\"\"\n    param_init: List of initial values of parameters\n    cost: cost function\n    niter: Number of iterations to run\n    lr: Learning rate\n    eps: Fudge factor, to avoid division by zero\n    \"\"\"\n    from copy import deepcopy\n    from autograd import grad\n    # Fixing the random_seed\n    np.random.seed(random_seed)\n    \n    # Function to compute the gradient of the cost function\n    grad_cost = grad(cost)\n    params = deepcopy(param_init)\n    param_array, grad_array, lr_array, cost_array = [params], [], [[lr*np.ones_like(_) for _ in params]], [cost(params)]\n    # Initialising sum of squares of gradients for each param as 0\n    sum_squares_gradients = [np.zeros_like(param) for param in params]\n    for i in range(niter):\n        out_params = []\n        gradients = grad_cost(params)\n        # At each iteration, we add the square of the gradients to `sum_squares_gradients`\n        sum_squares_gradients= [eps + sum_prev + np.square(g) for sum_prev, g in zip(sum_squares_gradients, gradients)]\n        # Adapted learning rate for parameter list\n        lrs = [np.divide(lr, np.sqrt(sg)) for sg in sum_squares_gradients]\n        # Paramter update\n        params = [param-(adapted_lr*grad_param) for param, adapted_lr, grad_param in zip(params, lrs, gradients)]\n        param_array.append(params)\n        lr_array.append(lrs)\n        grad_array.append(gradients)\n        cost_array.append(cost(params))\n        \n    return params, param_array, grad_array, lr_array, cost_array\n\n\n\nRunning Adagrad\n\nFixing initial parameters\nI’m poorly initialising H here to see how the learning rates vary for W and H.\n\nnp.random.seed(0)\nshape = A.shape\nH_init =  -5*np.abs(np.random.randn(rank, shape[1]))\nW_init =  np.abs(np.random.randn(shape[0], rank))\nparam_init = [W_init, H_init]\n\n\nH_init\n\narray([[ -8.82026173,  -2.00078604,  -4.89368992],\n       [-11.204466  ,  -9.33778995,  -4.8863894 ]])\n\n\n\nW_init\n\narray([[ 0.95008842,  0.15135721],\n       [ 0.10321885,  0.4105985 ],\n       [ 0.14404357,  1.45427351],\n       [ 0.76103773,  0.12167502]])\n\n\n\n# Cost for initial set of parameters\ncost(param_init)\n\n11.651268820608442\n\n\n\nlr = 0.1\neps=1e-8\nniter=2000\nada_params, ada_param_array, ada_grad_array, ada_lr_array, ada_cost_array = adagrad_gd(param_init, cost, niter=niter, lr=lr, eps=eps)\n\n\n\nCost v/s # iterations\n\npd.Series(ada_cost_array).plot(logy=True)\nplt.ylabel(\"Cost (log scale)\")\nplt.xlabel(\"# Iterations\")\n\n<matplotlib.text.Text at 0x10ece7610>\n\n\n\n\n\n\n\nFinal set of parameters and recovered matrix\n\nW_final, H_final = ada_params\npred = np.dot(W_final, H_final)\npred_df = pd.DataFrame(pred).round()\npred_df\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      5.0\n      4.0\n      5.0\n    \n    \n      1\n      4.0\n      4.0\n      5.0\n    \n    \n      2\n      5.0\n      3.0\n      4.0\n    \n    \n      3\n      2.0\n      3.0\n      3.0\n    \n  \n\n\n\n\n\n\nLearning rate evolution for W\n\nW_lrs = np.array(ada_lr_array)[:, 0]\n\n\nW_lrs = np.array(ada_lr_array)[:, 0]\nfig= plt.figure(figsize=(4, 2))\ngs = gridspec.GridSpec(1, 2, width_ratios=[8, 1]) \nax = plt.subplot(gs[0]),  plt.subplot(gs[1])\nmax_W, min_W = np.max([np.max(x) for x in W_lrs]), np.min([np.min(x) for x in W_lrs])\n\ndef update(iteration):\n    ax[0].cla()\n    ax[1].cla()\n    sns.heatmap(W_lrs[iteration], vmin=min_W, vmax=max_W, ax=ax[0], annot=True, fmt='.4f', cbar_ax=ax[1])\n    ax[0].set_title(\"Learning rate update for W\\nIteration: {}\".format(iteration))\n    fig.tight_layout()\n\nanim = FuncAnimation(fig, update, frames=np.arange(0, 200, 10), interval=500)\nanim.save('W_update.gif', dpi=80, writer='imagemagick')\nplt.close()\n\n\n\n\nLearning rate evolution for H\n\nH_lrs = np.array(ada_lr_array)[:, 1]\n\nfig= plt.figure(figsize=(4, 2))\ngs = gridspec.GridSpec(1, 2, width_ratios=[10, 1]) \nax = plt.subplot(gs[0]),  plt.subplot(gs[1])\nmax_H, min_H = np.max([np.max(x) for x in H_lrs]), np.min([np.min(x) for x in H_lrs])\n\ndef update(iteration):\n    ax[0].cla()\n    ax[1].cla()\n    sns.heatmap(H_lrs[iteration], vmin=min_H, vmax=max_H, ax=ax[0], annot=True, fmt='.2f', cbar_ax=ax[1])\n    ax[0].set_title(\"Learning rate update for H\\nIteration: {}\".format(iteration))\n    fig.tight_layout()\n\nanim = FuncAnimation(fig, update, frames=np.arange(0, 200, 10), interval=500)\nanim.save('H_update.gif', dpi=80, writer='imagemagick')\nplt.close()"
  },
  {
    "objectID": "posts/2022-02-20-condition-pyro.html",
    "href": "posts/2022-02-20-condition-pyro.html",
    "title": "Pyro Conditioning",
    "section": "",
    "text": "Generative model for PPCA in Pyro\n\nimport pyro.distributions as dist\nimport pyro.distributions.constraints as constraints\nimport pyro\n\npyro.clear_param_store()\n\n\ndef ppca_model(data, latent_dim):\n    N, data_dim = data.shape\n    W = pyro.sample(\n        \"W\",\n        dist.Normal(\n            loc=torch.zeros([latent_dim, data_dim]),\n            scale=5.0 * torch.ones([latent_dim, data_dim]),\n        ),\n    )\n    Z = pyro.sample(\n        \"Z\",\n        dist.Normal(\n            loc=torch.zeros([N, latent_dim]),\n            scale=torch.ones([N, latent_dim]),\n        ),\n    )\n\n    mean = Z @ W\n\n    return pyro.sample(\"obs\", pyro.distributions.Normal(mean, 1.0), obs=data)\n\n\npyro.render_model(\n    ppca_model, model_args=(torch.randn(150, 2), 1), render_distributions=True\n)\n\n\n\n\n\nppca_model(x_sample[0], 3).shape\n\ntorch.Size([2, 100])\n\n\n\nfrom pyro import poutine\nwith pyro.plate(\"samples\", 10, dim=-3):\n    trace = poutine.trace(ppca_model).get_trace(x_sample[0], 1)\n\n\ntrace.nodes['W']['value'].squeeze()\n\ntorch.Size([10, 100])\n\n\n\ndata_dim = 3\nlatent_dim = 2\n\nW = pyro.sample(\n        \"W\",\n        dist.Normal(\n            loc=torch.zeros([latent_dim, data_dim]),\n            scale=5.0 * torch.ones([latent_dim, data_dim]),\n        ),\n    )\n\n\nN = 150\nZ = pyro.sample(\n        \"Z\",\n        dist.Normal(\n            loc=torch.zeros([N, latent_dim]),\n            scale=torch.ones([N, latent_dim]),\n        ),\n    )\n\n\nZ.shape, W.shape\n\n(torch.Size([150, 2]), torch.Size([2, 3]))\n\n\n\n(Z@W).shape\n\ntorch.Size([150, 3])"
  },
  {
    "objectID": "posts/2021-06-14-setup-ipad.html",
    "href": "posts/2021-06-14-setup-ipad.html",
    "title": "My iPad Setup",
    "section": "",
    "text": "My laptop is broken. I am away from office. I have an iPad Pro 2020. I got my office desktop’s magic keyboard and trackpad. In this post I am discussing if iPad can help given that I do not have (physical) access to my main computers. Would I recommend this over a main computer - No! But, can you do some things on the iPad reasonably well enough given keyboard and trackpad - Yes!"
  },
  {
    "objectID": "posts/2021-06-14-setup-ipad.html#setting-up-the-terminal-app-a-shell",
    "href": "posts/2021-06-14-setup-ipad.html#setting-up-the-terminal-app-a-shell",
    "title": "My iPad Setup",
    "section": "Setting up the terminal app (a-Shell)",
    "text": "Setting up the terminal app (a-Shell)\n\nConfiguration\nFirst, after installing a-Shell, I like to set the font size, terminal background and foreground color. Here is how the a-shell app looks like\n\nconfig -b black -f white -s 20\n\n\nText editing and bookmarks\nSometimes I like using vim for editing documents and interfacing with WorkingCopy. a-Shell provides vim!\nI like to setup a bookmark to the WorkingCopy folder so that I can direcly edit files in that location.\nI do so by: writing pickFolder in a-Shell and setting it to WorkingCopy folder. Now I can set a bookmark to this location. I do so by: bookmark git in the current location (set by pickFolder)\n\n\n\nGit\nInterestingly, the latest testflight version of a-shell also provides a “Git-like” interface called libgit2. Configuring it requires specific steps that I’m writing below. Some of these steps are borrowed from this nice tutorial and some are specific to a-shell that I was able to get working courtesy a Twitter discussion with the creator of a-shell.\n\nNow, the steps.\nFirst, we need to create a new ssh key.\nWe do so by\nssh-keygen -t rsa -b 4096 -C \"email@domain.com\"\nWhile configuring I did not setup the passphrase.\nThe private and public keys are stored in .ssh with the name id_rsa\n$ ls -lah .ssh|grep \"id\"\n-rw-------   1 mobile  mobile   3.3K Jun 14 15:43 id_rsa\n-rw-------   1 mobile  mobile   747B Jun 14 15:43 id_rsa.pub\nNext, I copied the public key in a newly generated ssh key in Github and gave it a name.\nNext, I modified .gitconfig as follows\n$ cat .gitconfig \n[user]\n        email = MY EMAIL ID\n        name = MY NAME\n        identityFile =  id_rsa \nNow, I was almost done!\nI was able to push and pull from some old Github repositories but the same did not work with the newer repositories. Again, after a discussion with the creator of a-shell, I figured, this was due to the fact that Github changed the name of the default branch as “main” instead of the earlier “master” whereas libgit2 implementation was expecting “master”.\nAs a quickfix I renamed the branch on my Github repo as “master” and for now set the default branch to be named “master”.\nFinally, I am able to pull and push to the repositories. The next image infact is showing commits and pushes made to the repository generating the blog post you are reading.\n\n\n\nSSH/SFTP\nThere are a couple of amazing apps: Termius and SecureShell. Both have very neat interfaces. I found the intergrations with the Files app very good!\nThe GIF below shows the SecureShell app in action where I transfer a file from my local storage (iPad) to remote server, process that file, and quickly copy the processed file back to local storage.\n\nAnother great functionality of the SecureShell is the “Offline” folder.\nAnother setting that I use is the Powerline fonts on my remote systems. Using Fontcase, I installed the corresponding powerline fonts on the iPad so that my SecureShell session looks great.\n\n\n\nSome other amazing tools\nI like the “view” utility in a-shell a lot. It can quickly help you preview various filetypes.\n\nAlso, as a quick tip, one can use Command + W to quickly exit the preview and use the back and forward keys to cycle through the files. This is very useful!\nI also like the fact that the convert tool now comes in inbuilt. It can convert between a variety of formats easily.\npbcopy and pbpaste are very convenient utilities to copy to and from the clipboard. Here is how I copied the content of factorial.py into the clipboard.\npbcopy < factorial.py  \n\n\nShortcuts\na-Shell interfaces nicely with Shortcuts. The following gif shows an interface where I take an input from Shortcuts app -> Pass it to a Python script and execute it inside a-shell -> Store the results in a text file -> View the content of the text file in Shortcuts.\n\nThe link to this shortcut is here\nThe following is the simple Python script I used called factorial.py\nimport math\nimport sys\n\nnum = int(sys.argv[1])\nprint(f\"The factorial of {num} is {math.factorial(num)}\")\nThe following is an image of the shortcut.\n\nBased on the suggestion here, I used pbcopy to copy the content to the clipboard and use it directly. It reduces the number of lines!"
  },
  {
    "objectID": "posts/2021-06-14-setup-ipad.html#using-the-workingcopy-app",
    "href": "posts/2021-06-14-setup-ipad.html#using-the-workingcopy-app",
    "title": "My iPad Setup",
    "section": "Using the WorkingCopy App",
    "text": "Using the WorkingCopy App\nWorkingCopy is a very nicely made Git app on the iPad. It is one of the best made apps I have used!\nI’ll let the pictures do the talking."
  },
  {
    "objectID": "posts/2021-06-14-setup-ipad.html#editors",
    "href": "posts/2021-06-14-setup-ipad.html#editors",
    "title": "My iPad Setup",
    "section": "Editors",
    "text": "Editors\nI use one of the following for editing:\n\nKoder App\nWorkingCopy App\nvim (in a-Shell)\nTextastic\nTypewriter for Markdown — is an editor only for markdown but gives a nice quick preview. The image below shows the screenshot from Typewriter for Markdown app."
  },
  {
    "objectID": "posts/2018-01-07-cs-phd-lessons.html",
    "href": "posts/2018-01-07-cs-phd-lessons.html",
    "title": "CS Ph.D. lessons to my younger self",
    "section": "",
    "text": "Dear younger self,\nAs you embark this Ph.D. journey, I wanted to share some lessons and experiences. While I do not claim to be able to follow this 100 %, I am more aware than I was before. I think I might have done a better job at my Ph.D. if I’d taken note of the points I am going to mention now (in no particular order).\n\nTake care of your health\nYou’re joining fresh after completing your undergraduate studies. You are full of energy. But, your body won’t remain the same. You’ll realise that there is a difference between the early and late 20s. You’ll be debugging your code, and will feel it’ll take only 5 minutes more, and after three hours realise you’re well past your routine, your neck may hurt! Very soon, your body might hunch forward because of long sitting hours staring at your screen, and you may start gaining weight. Remember that when optimising for a happy life, you can’t neglect your health for long. As one of my mentors would say, take time off. Set it on your calendar! Ensure that you get some form of exercise routinely.\nBy health, I mean not only your physical health but also your mental health. There’ll be times that you’d be exhausted and dejected. No matter what you do, you don’t seem to make any progress. You’ll start developing doubts about your abilities. You’ll start questioning your decision to pursue a Ph.D. Remember that you’re not alone! Taking time off would help. It would also greatly help to have friends to talk and share. Don’t keep all your anxieties and worries to yourself!\n\n\nMeet well!\nYou’d be participating in a lot of meetings over the tenure of your Ph.D. Most often you’d be meeting your research advisor. You’d be very excited to discuss every possible detail of your work with your advisor. But, remember, that your advisor probably has some courses to take. They might have a group of students working under them, and have multiple projects running simultaneously. So, you’d need to plan and strategise to make the best use of the meeting time you get with your advisor. Here’s a template that I tried to follow. First, set the context by giving a summary of previous meeting discussion. Setting the context will ensure that you both are on the same page. Present a brief agenda. Then, present the main findings. Have some thoughts on why your method works or does not work. Over the course of your Ph.D., you’ll realise that your advisor is an expert in deducing why something works, and why something doesn’t. It’ll be precious for you to observe your advisor and learn and apply this yourself. In my earlier years, I’d often just go and show all my results to my advisor. You really shouldn’t stop there. Think about what you should conclude from your findings. Come up with specific questions for your advisor. Ensure that you make the best use of the limited meeting time you get with your advisor.\n\n\nPaper rejection is tough, but not the end of the world\nIt’s likely that you’ll have a few papers rejected during your Ph.D. I couldn’t believe when I got my first paper rejected. How could it be! In anguish, I felt the reviewer didn’t do a good job. I took it very personally! With more rejections, I got somewhat better at it. I also learned lessons along the way. One of the lessons that stuck with me was to not ponder too much on the reviews for 1-2 days after the paper was rejected. I wasn’t able to take the reviews with an unbiased attitude anyway. After 1-2 days, I would often appreciate the points made by the reviewers. Sometimes, I wouldn’t, but, then reviewing is complex, anyway! It should also be noted that of all my total submissions, only about one-third or so got accepted. Top CS conferences are very competitive, so paper rejections are likely to be more common than not!\n\n\nBetter emails go a long way\nYou’d be writing a ton of emails during your Ph.D., way more often than meeting in person. It’s important that you learn the art and science of writing better emails. Poorly written emails are less likely to get a response. At the beginning of my Ph.D., I wrote poor emails. The subject line wasn’t indicative of what to expect in the post, which made it incredibly hard to find relevant threads later! I often jumbled up multiple projects/topics in one email. I often wrote very long emails. Many of my emails were a brain dump and not organised well. Eventually, by observing how my advisor and other senior people wrote emails, I improved in this important skill. I think that many of the points about meetings are also applicable to emails. You need to set the context, discuss how you proceeded, and why you did so, summarise the observation, form some conclusions and questions, and if possible be specific on what inputs you need.\n\n\nEmbrace the scientific method\nI was an engineer by training before I started my Ph.D. So, I’d be thinking like - “this seems cool. Let’s try this approach. Should be fun to solve”. I went this way for some time, till I had a meeting with a senior faculty. The first question he asked me was - “what’s your hypothesis?”. I stood dumbfounded. I realised that thus far I’d been a solution-first researcher without doing so in a scientific way. I’d try and pick up approaches, if they didn’t work, move on. After a bit of rewiring, I improved my understanding and application of the scientific method. So, my work would now be more structured. I’d be wasting less time on random trails or unimportant problems. The key lesson is to have a checklist (mental or physical) of common pitfalls - like, improving the accuracy of a system from 95% to 99% on an unimportant or extremely contrived problem; or, starting data collection without much thought to the final experiments and analysis you’d plan to conduct to prove your hypothesis; or, the worst of all, having no hypothesis to start off with!\n\n\nMid-PhD. crisis time: you’re not alone\nI’d heard that every Ph.D. student goes through some crisis period. I thought I’d be an exception. How wrong I was. This period occurred after I’d been academically weaned. My coursework had finished. There was little barring research to keep me busy. Paper rejections stopped surprising me. Ideas not working stopped surprising me. I started questioning if I should quit Ph.D. midway. I started looking at forums on this topic and realised that this problem was common. During this period, I kept in touch with folks who’d completed their PhDs. Everyone suggested that this is normal, and a Ph.D. wouldn’t be so valuable if it weren’t this difficult! I feel that staying in touch with people who could relate to me was important. It was also helpful that despite the so-called inputs not converting to output, my advisors continued encouraging me, meeting regularly, and discussing scientifically correct ways of finding solutions to the Ph.D. problem. Miraculously this phase ended and led to the most successful phase of my Ph.D. where I was able to submit and get accepted a few top-tier conference papers. The key lesson is to stick it out, don’t feel alone or worthless, keep talking to cheerful people and keep reporting progress to your advisor on a regular basis.\n\n\nIt all boils down to addition and subtraction: Blog posts\nI won’t lie, I often get intimidated by research papers and the huge amount of techniques in our field. Yes, I still do. One of best teachers at my university spoke - “it all boils down to addition and subtraction.” This has stuck with me. I took a programming and visualisation approach to understanding concepts in my field. At a higher level, I’d be thinking that if I had to teach this concept to someone, how would I go about it. For example, if say, I wanted to study dynamic time warping, I started with some trivial problems where I’d use the concept. On such trivial problems, it would be easy to understand what the algorithm would be doing. I’d often end up writing detailed Jupyter/IPython notebooks showing how the algorithm works, programming and visualsing the various steps. All these formed a part of my technical blog, which I would update on a regular basis. The learning in writing these blog posts was immense. While these blog posts are public, I think I am the biggest beneficiary. Not only does one gain a good understanding of the concept involved, but one also gains confidence about the subject and one’s ability to understand! The key lesson is to document your learnings, understandings, and try to abstract out your specific problem and think of teaching the concept to someone who doesn’t know much about your problem.\n\n\nTeaching is learning\nA teaching assistantship is often dreaded. Why waste my valuable time on it? It turns out, I learned a lot from my TA experiences. I realised that learning with an intention to share the learning ensured that I learned well. I didn’t cut corners. Moreover, with an emphasis on teaching well, I often had to think hard about making concepts relatable. This exercise helped me a lot! In my first semester, I was a TA for an introduction to programming course. Before the course, I thought I knew Python reasonably well. After the course, I realised that now I knew it way better than earlier. Since Python turned out to be the language I did most of my research coding in, it turned out to be a great learning experience. Besides, I made a lot of friends with my students in the course! The key lesson here is somewhat related to the lesson on blog posts. Thinking how to explain stuff usually always helped me get a better understanding!\n\n\nGood research is only half done!\nTalks are a great way to advertise your research and get some good feedback. It’s easy to go very excited and fit everything from the paper into a few slides in a 15-20 minute presentation. This is a great recipe for disaster! Good presentations often leave the audience feeling thankful that they attended the talk. Bad talks ensure that people open their laptops and start answering emails they’ve not replied for ages! A good talk requires preparation. I repeat. A good talk requires preparation. Even if you’re a pro. Over the course of the years, I developed my style learning from my advisors, other faculties, other presenters. There were a lot of lessons involved! One of the key lessons for me was to jot down a script for my slides. While I felt I could mostly do a good job speaking without speaker notes, I think I was better with them! Of course, it took a lot of effort! Another important lesson was to deliver a talk in terms of things the audience could relate with, and thus keeping them engaged. I also maintained that the slides are there to support me and not replace me. Thus, I would never put much text into them! I’d put a lot of efforts in maintaining consistency across slides, using similar figures, conventions. All of this to ensure that the viewer doesn’t lose interest!\nOf course, despite all these efforts, I would usually always deliver a pathetic first talk. I was lucky that most of these first pathetic talks came in front of colleagues and not the main audience. So, “Practice! Practice! And practice!” is the mantra.\n\n\nVolunteer to review papers\nYour advisor would likely be reviewing a lot of papers throughout the year. Many of these would probably be in your sub-area of CS. It was an excellent exercise for me when I’d help my advisor review some of the papers. Taking part in the review process makes you appreciate the time and effort put in by the TPC and the conference management team! No one is paid for all this effort! I particularly remember excitedly telling my advisor that I’d champion one of the papers he gave me to review. He smiled and said that paper wouldn’t probably go through. The discussion that followed helped me learn the traits of a good and a bad paper from a reviewer’s perspective. Once you get the hang of it and do the same critical analysis of your paper, you’d be able to improve your paper!\n\n\nOpen source is the right way\nCS research often involves writing code for our approach, some baselines, data management and analysis. This set of code leads to the generation of results and analysis in our papers. Now, when I started my Ph.D. and started working on a problem, I thought that it should be trivial to compare our work to previous literature. It turns out; I was wrong. Reproducibility or the act of generating results from previous literature is a huge challenge. Making your code reproducible, such that others can use it for their analysis and paper takes a lot of effort. I felt that it was the right thing to make my code open source and easy for others to use. One of my most cited paper came as a result of introducing more transparency, comparability, and reproducibility. I’m also not surprised that I’m the biggest benefactor by making my code good enough to be made public. The steps I took to make the code more readable, and reproducible, helped me a lot going back to my code to tweak or re-run some experiments. In the age of Github, there aren’t really many excuses for not putting efforts towards - “Let’s make scientific articles comparable again.”\n\n\nFunding for conferences/PhD scholarships\nWhile we may crib about our stipends not being comparable to industry folks, let’s not forget that a Ph.D. can be very expensive. Coming from India, the travel costs to conferences would be high! Yes, very few of the international conferences I attended happened in India. However, some research labs like MSR and Google and some government agencies provide funding for “good” conferences. Many conferences also provide economic support. I was lucky that I could cut some of the costs for my advisor and department by getting travel fundings. Various organisations also offer Ph.D. fellowships. I was fortunate to receive one from TCS. These fellowships not only allow for some industry mentors but also include financial support. It’s one less thing to worry if we can get some of the finances worked out. The key lesson isn’t a particularly surprising one - apply for fellowship and grants whenever you can!\n\n\nGood paper writing takes time and practice\nBefore joining grad school, I was a great writer. After completing it, I could manage some writing. Confused? When I entered grad school did I realise how pathetic my scientific writing skills were. While I’d been speaking and writing English since I was four or so, it wasn’t my first language. The red coloured paper returned by my advisor on my first draft was an eye opener (technically, it was \\color in LaTeX and not a hard copy!). For someone like me who loves to learn by observation, the process of taking my first draft and seeing it turn into a well-written document courtesy my advisor was wonderful. The key lesson is to observe what makes a good draft. I came up with a structure that I would follow in almost all of my papers:\n\nWhat is the general problem?\nHow can we convert it to a CS problem?\nWhat’s the related work and where does it fail to address the problem?\nWhy would our approach work? and what’s the gist of it?\nWhat’s the experimental setup I’ve used for evaluation?\nHow well does our approach perform?\nIn light of above, what can be conclude about the problem given our approach?\n\nI’d write using such a structure in the abstract and just expand it into different sections in the paper.\nOh, and yes, don’t forget Grammarly and some scripts from Matt Might! Something that really helped me was to run my drafts, even before the final version with my colleagues to get some reviews.\n\n\nAccept limitations of your work and be honest about them\nWe’ve all seen those few points on the graph that make our algorithm look bad. Only if we didn’t have those points, our technique would look so amazing! There may be a tendency to “avoid” the limitations. My key learning on this front has been to accept the limitations of my work and be very honest about them. Science loses if we “hide” facts. Rather, it’s the limitations which make things interesting. They force us to go back and review everything. Is this an issue with our approach, or implementation, or dataset, or something more fundamental? Maintaining this integrity was always a top priority! There’ve been instances where paper reviewers have shown appreciation for us being clear and honest about the limitations.\n\n\nSometimes it helps to disconnect\nWe live in an age of data deluge. There are so many forums to network and brand your work. There are so many forums like Reddit and HackerNews to remain on top of the latest in the field. While I tried to remain up to date with the latest, it helped when I would sometimes disconnect. This used to be the way I studied in primary and secondary school. So, if any idea or interesting problem comes to mind, I would sometimes try and think it through before googling it. Similarly, if the code gives some error, I found that my programming and understanding improved if I would spend some time thinking before googling! The key lesson is to think before you search.\n\n\nIs the grass really greener on the other side\nAs a Ph.D. student sitting in India, I’d often think how great it would have been to be a Ph.D. student at say MIT, Stanford, or some other top CS university! Maybe, I would have a “better” publication record. Procrastinating in such situations is easy. My key learning in this aspect came from a discussion I had about 15 years back in my high school when my teacher told me that it’s the students who make the school. So, the key lesson was to accept that I may not be from the most well-known school in the world, but nothing stops me from doing world-class research. So, accepting what I had, and trying to change what I could was the main learning. Of course, research is highly collaborative these days. Eventually, by the time I had graduated, I had worked with people from Imperial London, Univ. of Southampton, University of Virginia, UCLA and CMU.\n\n\nInvest in good hardware\nI get it. Our Ph.D. stipends aren’t super amazing. I chose to buy the “best” laptop I could in a reasonable budget. Why bother with Apple-like expensive laptops. I could do with a regular laptop and just install Linux on it. It turns out, I was wrong. Within two years, my laptop had a broken hinge, I had driver issues (on Linux), the laptop wasn’t super light, the battery wasn’t all that great. Then, it took me a hard time to move to a superior laptop. It was expensive. But, it more than made up in terms of the increased productivity. The battery would often last a work day; my back pain got better due to a lighter laptop. I was no longer afraid of updating my system. So, while I am talking about a laptop here, the lesson is more general. The cost of a “good” piece of hardware can be easily recovered many times over in terms of increased productivity.\nTake care!\n\nMore to come\n\nNetworking at conferences, discussion buddy, elevator pitch, attending thesis and faculty job talks.."
  },
  {
    "objectID": "posts/2022-01-29-kl-divergence.html",
    "href": "posts/2022-01-29-kl-divergence.html",
    "title": "Understanding KL-Divergence",
    "section": "",
    "text": "Basic Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nimport tensorflow_probability as tfp\nimport pandas as pd\n\ntfd = tfp.distributions\ntfl = tfp.layers\ntfb = tfp.bijectors\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import Callback\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\n\nCreating distributions\n\nCreating \\(p\\sim\\mathcal{N}(1.00, 4.00)\\)\n\np = tfd.Normal(1, 4)\n\n2022-02-04 14:55:14.596076: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nz_values = tf.linspace(-5, 15, 200)\nz_values = tf.cast(z_values, tf.float32)\nprob_values_p = p.prob(z_values)\nplt.plot(z_values, prob_values_p, label=r\"$p\\sim\\mathcal{N}(1.00, 4.00)$\")\nsns.despine()\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\n\n\n\n\n\n\nCreating \\(q\\sim\\mathcal{N}(loc, scale)\\)\n\ndef create_q(loc, scale):\n    return tfd.Normal(loc, scale)\n\n\n\nGenerating a few qs for different location and scale value\n\nq = {}\nq[(0, 1)] = create_q(0.0, 1.0)\n\nfor loc in [0, 1]:\n    for scale in [1, 2]:\n        q[(loc, scale)] = create_q(float(loc), float(scale))\n\n\nplt.plot(z_values, prob_values_p, label=r\"$p\\sim\\mathcal{N}(1.00, 4.00)$\", lw=3)\nplt.plot(\n    z_values,\n    create_q(0.0, 2.0).prob(z_values),\n    label=r\"$q_1\\sim\\mathcal{N}(0.00, 2.00)$\",\n    lw=2,\n    linestyle=\"--\",\n)\nplt.plot(\n    z_values,\n    create_q(1.0, 3.0).prob(z_values),\n    label=r\"$q_2\\sim\\mathcal{N}(1.00, 3.00)$\",\n    lw=2,\n    linestyle=\"-.\",\n)\n\nplt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\nsns.despine()\nplt.tight_layout()\nplt.savefig(\n    \"dkl.png\",\n    dpi=150,\n)\n\n\n\n\n\n#### Computing KL-divergence\n\nq_0_2_dkl = tfd.kl_divergence(create_q(0.0, 2.0), p)\nq_1_3_dkl = tfd.kl_divergence(create_q(1.0, 3.0), p)\n\nprint(f\"D_KL (q(0, 2)||p) = {q_0_2_dkl:0.2f}\")\nprint(f\"D_KL (q(1, 3)||p) = {q_1_3_dkl:0.2f}\")\n\nD_KL (q(0, 2)||p) = 0.35\nD_KL (q(1, 3)||p) = 0.07\n\n\nAs mentioned earlier, clearly, \\(q_2\\sim\\mathcal{N}(1.00, 3.00)\\) seems closer to \\(p\\)\n\n\n\nOptimizing the KL-divergence between q and p\nWe could create a grid of (loc, scale) pairs and find the best, as shown below.\n\nplt.plot(z_values, prob_values_p, label=r\"$p\\sim\\mathcal{N}(1.00, 4.00)$\", lw=5)\n\n\nfor loc in [0, 1]:\n    for scale in [1, 2]:\n        q_d = q[(loc, scale)]\n        kl_d = tfd.kl_divergence(q[(loc, scale)], p)\n        plt.plot(\n            z_values,\n            q_d.prob(z_values),\n            label=rf\"$q\\sim\\mathcal{{N}}({loc}, {scale})$\"\n            + \"\\n\"\n            + rf\"$D_{{KL}}(q||p)$ = {kl_d:0.2f}\",\n        )\nplt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\nsns.despine()\n\n\n\n\nOr, we could use continuous optimization to find the best loc and scale parameters for q.\n\nto_train_q = tfd.Normal(\n    loc=tf.Variable(-1.0, name=\"loc\"),\n    scale=tfp.util.TransformedVariable(1.0, bijector=tfb.Exp(), name=\"scale\"),\n)\n\n\nto_train_q.trainable_variables\n\n2022-02-04 14:55:19.564807: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n\n\n(<tf.Variable 'loc:0' shape=() dtype=float32, numpy=-1.0>,\n <tf.Variable 'scale:0' shape=() dtype=float32, numpy=0.0>)\n\n\n\n@tf.function\ndef loss_and_grads(q_dist):\n    with tf.GradientTape() as tape:\n        loss = tfd.kl_divergence(q_dist, p)\n    return loss, tape.gradient(loss, q_dist.trainable_variables)\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\nfor i in range(500):\n    loss, grads = loss_and_grads(to_train_q)\n    optimizer.apply_gradients(zip(grads, to_train_q.trainable_variables))\n\n\nto_train_q.loc, to_train_q.scale\n\n(<tf.Variable 'loc:0' shape=() dtype=float32, numpy=0.98873746>,\n <TransformedVariable: name=scale, dtype=float32, shape=[], fn=\"exp\", numpy=3.9999995>)\n\n\nAfter training, we are able to recover the scale and loc very close to that of \\(p\\)\n\n\nAnimation!\n\nfrom matplotlib import animation\n\nfig = plt.figure(tight_layout=True, figsize=(8, 4))\nax = fig.gca()\n\nto_train_q = tfd.Normal(\n    loc=tf.Variable(5.0, name=\"loc\"),\n    scale=tfp.util.TransformedVariable(0.1, bijector=tfb.Exp(), name=\"scale\"),\n)\n\n\ndef animate(i):\n    ax.clear()\n    ax.plot(z_values, prob_values_p, label=r\"$p\\sim\\mathcal{N}(1.00, 4.00)$\", lw=5)\n    loss, grads = loss_and_grads(to_train_q)\n    optimizer.apply_gradients(zip(grads, to_train_q.trainable_variables))\n    loc = to_train_q.loc.numpy()\n    scale = to_train_q.scale.numpy()\n\n    ax.plot(\n        z_values,\n        to_train_q.prob(z_values),\n        label=rf\"$q\\sim \\mathcal{{N}}({loc:0.2f}, {scale:0.2f})$\",\n    )\n    d_kl = tfd.kl_divergence(to_train_q, p)\n\n    ax.set_title(rf\"Iteration: {i}, $D_{{KL}}(q||p)$: {d_kl:0.2f}\")\n    ax.legend(bbox_to_anchor=(1.1, 1), borderaxespad=0)\n    ax.set_ylim((0, 1))\n    ax.set_xlim((-5, 15))\n\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"PDF\")\n    sns.despine()\n\n\nani = animation.FuncAnimation(fig, animate, frames=150)\nplt.close()\n\n\nani.save(\"kl_qp.gif\", writer=\"imagemagick\", fps=15, dpi=100)\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\nFinding the KL divergence for two distributions from different families\nLet us rework our example with p coming from a mixture of Gaussian distribution and q being Normal.\n\np_s = tfd.MixtureSameFamily(\n    mixture_distribution=tfd.Categorical(probs=[0.5, 0.5]),\n    components_distribution=tfd.Normal(\n        loc=[-0.2, 1], scale=[0.1, 0.5]  # One for each component.\n    ),\n)  # And same here.\n\np_s\n\n<tfp.distributions.MixtureSameFamily 'MixtureSameFamily' batch_shape=[] event_shape=[] dtype=float32>\n\n\n\nplt.plot(z_values, p_s.prob(z_values))\nsns.despine()\n\n\n\n\nLet us create two Normal distributions q_1 and q_2 and plot them to see which looks closer to p_s.\n\nq_1 = create_q(3, 1)\nq_2 = create_q(3, 4.5)\n\n\nprob_values_p_s = p_s.prob(z_values)\nprob_values_q_1 = q_1.prob(z_values)\nprob_values_q_2 = q_2.prob(z_values)\n\nplt.plot(z_values, prob_values_p_s, label=r\"MOG\")\nplt.plot(z_values, prob_values_q_1, label=r\"$q_1\\sim\\mathcal{N} (3, 1.0)$\")\nplt.plot(z_values, prob_values_q_2, label=r\"$q_2\\sim\\mathcal{N} (3, 4.5)$\")\n\nsns.despine()\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\nplt.tight_layout()\nplt.savefig(\n    \"dkl-different.png\",\n    dpi=150,\n)\n\n\n\n\n\ntry:\n    tfd.kl_divergence(q_1, p_s)\nexcept Exception as e:\n    print(e)\n\nNo KL(distribution_a || distribution_b) registered for distribution_a type Normal and distribution_b type MixtureSameFamily\n\n\nAs we see above, we can not compute the KL divergence directly. The core idea would now be to leverage the Monte Carlo sampling and generating the expectation. The following function does that.\n\ndef kl_via_sampling(q, p, n_samples=100000):\n    # Get samples from q\n    sample_set = q.sample(n_samples)\n    # Use the definition of KL-divergence\n    return tf.reduce_mean(q.log_prob(sample_set) - p.log_prob(sample_set))\n\n\nkl_via_sampling(q_1, p_s), kl_via_sampling(q_2, p_s)\n\n(<tf.Tensor: shape=(), dtype=float32, numpy=9.465648>,\n <tf.Tensor: shape=(), dtype=float32, numpy=46.48004>)\n\n\nAs we can see from KL divergence calculations, q_1 is closer to our Gaussian mixture distribution.\n\n\nOptimizing the KL divergence for two distributions from different families\nWe saw that we can calculate the KL divergence between two different distribution families via sampling. But, as we did earlier, will we be able to optimize the parameters of our target surrogate distribution? The answer is No! As we have introduced sampling. However, there is still a way – by reparameterization!\nOur surrogate q in this case is parameterized by loc and scale. The key idea here is to generate samples from a standard normal distribution (loc=0, scale=1) and then apply an affine transformation on the generated samples to get the samples generated from q. See my other post on sampling from normal distribution to understand this better.\nThe loss can now be thought of as a function of loc and scale.\n\nn_samples = 1000\n\n\ndef loss(loc, scale):\n    q = tfd.Normal(loc=loc, scale=scale)\n    q_1 = tfd.Normal(loc=0.0, scale=1.0)\n    sample_set = q_1.sample(n_samples)\n    sample_set = loc + scale * sample_set\n    return tf.reduce_mean(q.log_prob(sample_set) - p_s.log_prob(sample_set))\n\nHaving defined the loss above, we can now optimize loc and scale to minimize the KL-divergence.\n\noptimizer = tf.optimizers.Adam(learning_rate=0.05)\n\n\nn_iter = 150\nlocation_array = np.empty(n_iter, dtype=np.float32)\nscale_array = np.empty(n_iter, dtype=np.float32)\nloss_array = np.empty(n_iter, dtype=np.float32)\n\n\nloc = tf.Variable(3.0, dtype=tf.float32)\nscale = tf.Variable(4.0, dtype=tf.float32)\nfor i in range(n_iter):\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(loc)\n        tape.watch(scale)\n        lo = loss(loc, scale)\n    [dl_loc, dl_scale] = tape.gradient(lo, [loc, scale])\n    if i % 50 == 0:\n        tf.print(lo, loc, scale)\n    location_array[i] = loc.numpy()\n    scale_array[i] = scale.numpy()\n    loss_array[i] = lo.numpy()\n    optimizer.apply_gradients(zip([dl_loc, dl_scale], [loc, scale]))\n\n38.7589951 3 4\n0.4969607 0.736858189 0.680736303\n0.528585315 0.774057031 0.617758751\n\n\n\nq_s = tfd.Normal(loc=loc, scale=scale)\nq_s\n\n<tfp.distributions.Normal 'Normal' batch_shape=[] event_shape=[] dtype=float32>\n\n\n\nprob_values_p_s = p_s.prob(z_values)\nprob_values_q_s = q_s.prob(z_values)\n\nplt.plot(z_values, prob_values_p_s, label=r\"p\")\nplt.plot(z_values, prob_values_q_s, label=r\"q\")\n\nsns.despine()\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\n\n\n\n\n\nprob_values_p_s = p_s.prob(z_values)\n\nfig = plt.figure(tight_layout=True, figsize=(8, 4))\nax = fig.gca()\n\n\ndef a(iteration):\n\n    ax.clear()\n    loc = location_array[iteration]\n    scale = scale_array[iteration]\n    q_s = tfd.Normal(loc=loc, scale=scale)\n\n    prob_values_q_s = q_s.prob(z_values)\n\n    ax.plot(z_values, prob_values_p_s, label=r\"p\")\n    ax.plot(z_values, prob_values_q_s, label=r\"q\")\n    ax.set_title(f\"Iteration {iteration}, Loss: {loss_array[iteration]:0.2f}\")\n\n\nani_mg = animation.FuncAnimation(fig, a, frames=n_iter)\nplt.close()\n\n\nplt.plot(location_array, label=\"loc\")\nplt.plot(scale_array, label=\"scale\")\nplt.xlabel(\"Iterations\")\nsns.despine()\nplt.legend()\n\n<matplotlib.legend.Legend at 0x1add94bb0>\n\n\n\n\n\n\nani_mg.save(\"kl_qp_mg.gif\", writer=\"imagemagick\")\n\n\n\n\nOptimizing the KL-divergence between two 2d distributions\nLet us now repeat the same procedure but for two 2d Normal distributions.\n\np_2d = tfd.MultivariateNormalFullCovariance(\n    loc=[0.0, 0.0], covariance_matrix=[[1.0, 0.5], [0.5, 2]]\n)\n\nto_train_q_2d_2 = tfd.MultivariateNormalDiag(\n    loc=tf.Variable([2.0, -2.0], name=\"loc\"),\n    scale_diag=tfp.util.TransformedVariable(\n        [1.0, 2.0], bijector=tfb.Exp(), name=\"scale\"\n    ),\n)\n\nWARNING:tensorflow:From /Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py:342: MultivariateNormalFullCovariance.__init__ (from tensorflow_probability.python.distributions.mvn_full_covariance) is deprecated and will be removed after 2019-12-01.\nInstructions for updating:\n`MultivariateNormalFullCovariance` is deprecated, use `MultivariateNormalTriL(loc=loc, scale_tril=tf.linalg.cholesky(covariance_matrix))` instead.\n\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n\n\ndef make_pdf_2d_gaussian(mu, sigma, ax, title):\n    N = 60\n    X = np.linspace(-3, 3, N)\n    Y = np.linspace(-3, 4, N)\n    X, Y = np.meshgrid(X, Y)\n\n    # Pack X and Y into a single 3-dimensional array\n    pos = np.empty(X.shape + (2,))\n    pos[:, :, 0] = X\n    pos[:, :, 1] = Y\n\n    F = tfd.MultivariateNormalFullCovariance(loc=mu, covariance_matrix=sigma)\n    Z = F.prob(pos)\n\n    sns.despine()\n    ax.set_xlabel(r\"$x_1$\")\n    ax.set_ylabel(r\"$x_2$\")\n    ax.set_aspect(\"equal\")\n    if title:\n        ax.set_title(f\"$\\mu$ = {mu}\\n $\\Sigma$ = {np.array(sigma)}\")\n        ax.contour(X, Y, Z, cmap=\"viridis\", alpha=1, zorder=2)\n    else:\n        ax.contourf(X, Y, Z, cmap=\"plasma\", alpha=0.1)\n\n\nfig, ax = plt.subplots()\nmake_pdf_2d_gaussian([0.0, 1.0], [[1.0, 0.5], [0.5, 2]], ax, False)\nmake_pdf_2d_gaussian(\n    to_train_q_2d_2.loc.numpy(), to_train_q_2d_2.covariance().numpy(), ax, True\n)\n\n\n\n\nAs we can see above, the two distributions look very different. We can calculate the KL-divergence as before.\n\ntfd.kl_divergence(to_train_q_2d_2, p_2d)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=4.8723755>\n\n\n\nfig = plt.figure(tight_layout=True, figsize=(8, 4))\nax = fig.gca()\n\n\ndef animate(i):\n    ax.clear()\n    with tf.GradientTape() as tape:\n        loss = tfd.kl_divergence(to_train_q_2d_2, p_2d)\n        grads = tape.gradient(loss, to_train_q_2d_2.trainable_variables)\n        optimizer.apply_gradients(zip(grads, to_train_q_2d_2.trainable_variables))\n    loc = np.round(to_train_q_2d_2.loc.numpy(), 1)\n    scale = np.round(to_train_q_2d_2.covariance().numpy(), 1)\n\n    make_pdf_2d_gaussian(loc, scale, ax, True)\n    make_pdf_2d_gaussian([0.0, 1.0], [[1.0, 0.5], [0.5, 2]], ax, False)\n\n    ax.set_xlabel(r\"$x_1$\")\n    ax.set_ylabel(r\"$x_2$\")\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n\n    # Only show ticks on the left and bottom spines\n    ax.yaxis.set_ticks_position(\"left\")\n    ax.xaxis.set_ticks_position(\"bottom\")\n\n\nani2 = animation.FuncAnimation(fig, animate, frames=100)\nplt.close()\n\n\nani2.save(\"kl_qp_2.gif\", writer=\"imagemagick\", fps=15, dpi=100)\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\nto_train_q_2d_2.loc, to_train_q_2d_2.covariance()\n\n(<tf.Variable 'loc:0' shape=(2,) dtype=float32, numpy=array([ 0.01590762, -0.01590773], dtype=float32)>,\n <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n array([[0.87550056, 0.        ],\n        [0.        , 1.7570419 ]], dtype=float32)>)\n\n\n\ntfd.kl_divergence(to_train_q_2d_2, p_2d)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=0.0670591>\n\n\nWe can now see that the KL-divergence has reduced significantly from where we started, but it will unlikely improve as ou r q distribution is a multivariate diagonal normal distribution, meaning there is no correlation between the two dimensions.\n\n\nTo-FIX\nEverything below here needs to be fixed\n\nKL-Divergence and ELBO\nLet us consider linear regression. We have parameters \\(\\theta \\in R^D\\) and we define a prior over them. Let us assume we define prior \\(p(\\theta)\\sim \\mathcal{N_D} (\\mu, \\Sigma)\\). Now, given our dataset \\(D = \\{X, y\\}\\) and a parameter vector \\(\\theta\\), we can deifine our likelihood as \\(p(D|\\theta)\\) or $p(y|X, ) = {i=1}^{n} p(y_i|x_i, ) = {i=1}^{n} (y_i|x_i^T, ^2) $\nAs per Bayes rule, we can obtain the posterior over \\(\\theta\\) as:\n\\(p(\\theta|D) = \\dfrac{p(D|\\theta)p(\\theta)}{p(D)}\\)\nNow, in general \\(p(D)\\) is hard to compute.\nSo, in variational inference, our aim is to use a surrogate distribution \\(q(\\theta)\\) such that it is very close to \\(p(\\theta|D)\\). We do so by minimizing the KL divergence between \\(q(\\theta)\\) and \\(p(\\theta|D)\\).\nAim: \\[q^*(\\theta) = \\underset{q(\\theta) \\in \\mathcal{Q}}{\\mathrm{argmin~}} D_{KL}[q(\\theta)||p(\\theta|D)]\\]\nNow, \\[D_{KL}[q(\\theta)||p(\\theta|D)] = \\mathbb{E}_{q(\\theta)}[\\log\\frac{q(\\theta)}{p(\\theta|D)}]\\] Now, \\[ = \\mathbb{E}_{q(\\theta)}[\\log\\frac{q(\\theta)p(D)}{p(\\theta, D)}]\\] Now, \\[ = \\mathbb{E}_{q(\\theta)}[\\log q(\\theta)]- \\mathbb{E}_{q(\\theta)}[\\log p(\\theta, D)] + \\mathbb{E}_{q(\\theta)}[\\log p(D)] \\] \\[= \\mathbb{E}_{q(\\theta)}[\\log q(\\theta)]- \\mathbb{E}_{q(\\theta)}[\\log p(\\theta, D)] + \\log p(D) \\]\nNow, \\(p(D) \\in \\{0, 1\\}\\). Thus, \\(\\log p(D) \\in \\{-\\infty, 0 \\}\\)\nNow, let us look at the quantities:\n\\[\\underbrace{D_{KL}[q(\\theta)||p(\\theta|D)]}_{\\geq 0} = \\underbrace{\\mathbb{E}_{q(\\theta)}[\\log q(\\theta)]- \\mathbb{E}_{q(\\theta)}[\\log p(\\theta, D)]}_{-\\text{ELBO(q)}} +  \\underbrace{\\log p(D)}_{\\leq 0}\\]\nThus, we know that \\(\\log p(D) \\geq \\text{ELBO(q)}\\)\nThus, finally we can rewrite the optimisation from\n\\[q^*(\\theta) = \\underset{q(\\theta) \\in \\mathcal{Q}}{\\mathrm{argmin~}} D_{KL}[q(\\theta)||p(\\theta|D)]\\]\nto\n\\[q^*(\\theta) = \\underset{q(\\theta) \\in \\mathcal{Q}}{\\mathrm{argmax~}} \\text{ELBO(q)}\\]\nNow, given our linear regression problem setup, we want to maximize the ELBO.\nWe can do so by the following. As a simple example, let us assume \\(\\theta \\in R^2\\)\n\nAssume some q. Say, a Normal distribution. So, \\(q\\sim \\mathcal{N}_2\\)\nDraw samples from q. Say N samples.\nInitilize ELBO = 0.0\nFor each sample:\n\nLet us assume drawn sample is \\([\\theta_1, \\theta_2]^T\\)\nCompute log_prob of prior on \\([\\theta_1, \\theta_2]^T\\) or lp = p.log_prob(θ1, θ2)\nCompute log_prob of likelihood on \\([\\theta_1, \\theta_2]^T\\) or ll = l.log_prob(θ1, θ2)\nCompute log_prob of q on \\([\\theta_1, \\theta_2]^T\\) or lq = q.log_prob(θ1, θ2)\nELBO = ELBO + (ll+lp-q)\n\nReturn ELBO/N\n\n\ndef lr(x, stddv_datapoints):\n    num_datapoints, data_dim = x.shape\n    th = yield tfd.Normal(\n        loc=tf.zeros([data_dim + 1]), scale=tf.ones([data_dim + 1]), name=\"theta\"\n    )\n\n    x_dash = tf.concat([tf.ones_like(x), x], 1)\n    y = yield tfd.Normal(\n        loc=tf.linalg.matvec(x_dash, th), scale=stddv_datapoints, name=\"y\"\n    )\n\n\nx = tf.linspace(-5.0, 5.0, 100)\nx = tf.expand_dims(x, 1)\n\n\nimport functools\n\nstddv_datapoints = 1\n\nconcrete_lr_model = functools.partial(lr_2, x=x, stddv_datapoints=stddv_datapoints)\n\nmodel = tfd.JointDistributionCoroutineAutoBatched(concrete_lr_model)\n\n\ntf.random.set_seed(0)\nth_sample, data_sample = model.sample()\n\nplt.scatter(x[:, 0], data_sample, s=10)\nplt.plot(x[:, 0], tf.reshape(x*th_sample[1] + th_sample[0], [-1]))\nprint(th_sample)\n\ntf.Tensor([1.5110626  0.42292204], shape=(2,), dtype=float32)\n\n\n\n\n\n\nmodel.log_prob(th_sample, data_sample)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=-150.26591>\n\n\n\nloc = tf.Variable([1.0, 1.0], dtype=tf.float32)\nscale = tf.Variable([1.0, 1.0])\n\nq_to_learn = tfd.Normal(loc=loc, scale=scale, name=\"q_theta_learn\")\n\n\nn_samples = 1000\n\n\nloc = tf.Variable([1.0, 0.4], dtype=tf.float32)\nscale = tfp.util.TransformedVariable([.7, .6], bijector=tfb.SoftClip(0.5, 1.0))\ndef loss():\n    q_to_learn = tfd.Normal(loc=loc, scale=scale, name=\"q_theta_learn\")\n    q_1 = tfd.Normal(loc=[0.0,0.0], scale=[1.0, 1.0])\n    sample_set = q_1.sample(n_samples)\n    log_joint = tf.reduce_sum(model.log_prob(sample_set, data_sample))\n    log_q = tf.reduce_sum(q_to_learn.log_prob(sample_set))\n    return log_q - log_joint\n\n\ntrace6000 = tfp.math.minimize(loss_fn=loss, num_steps=6000, \n                  optimizer=tf.optimizers.Adam(0.0001))\n\n\nloc\n\n<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1.4298061 , 0.84917635], dtype=float32)>\n\n\n\nscale\n\n<TransformedVariable: name=soft_clip, dtype=float32, shape=[2], fn=\"soft_clip\", numpy=array([0.6512191, 0.5705266], dtype=float32)>\n\n\n\nplt.plot(trace)\n\n\n\n\n\nloc, th_sample, scale\n#scale =  tfp.util.TransformedVariable([1., 1.], bijector=tfb.Exp())\n\n(<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([2.0690155, 1.496477 ], dtype=float32)>,\n <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5110626 , 0.42292204], dtype=float32)>,\n <TransformedVariable: name=soft_clip, dtype=float32, shape=[2], fn=\"soft_clip\", numpy=array([0.5936036, 0.5403929], dtype=float32)>)\n\n\n\nplt.scatter(x[:, 0], data_sample, s=10)\nplt.plot(x[:, 0], tf.reshape(x*loc[1] + loc[0], [-1]))\n\n\n\n\n\nloss_mc\n\n<function __main__.loss_mc(loc, scale)>\n\n\n\nloc = tf.Variable([1.0, 0.4], dtype=tf.float32)\nscale = tfp.util.TransformedVariable([.7, .6], bijector=tfb.SoftClip(0.5, 1.0))\n\ndef loss_mc(loc, scale):\n    q_to_learn = tfd.Normal(loc=loc, scale=scale, name=\"q_theta_learn\")\n    q_1 = tfd.Normal(loc=[0.0,0.0], scale=[1.0, 1.0])\n    sample_set = q_1.sample(n_samples)\n    log_joint = tf.reduce_sum(model.log_prob(sample_set, data_sample))\n    log_q = tf.reduce_sum(q_to_learn.log_prob(sample_set))\n    return log_q - log_joint\n\ntarget_log_prob_fn = lambda th: model.log_prob((th, data_sample))\n\n\n\n\ndata_dim=2\nqt_mean = tf.Variable(tf.random.normal([data_dim]))\nqt_stddv = tfp.util.TransformedVariable(\n    1e-4 * tf.ones([data_dim]), bijector=tfb.Softplus()\n)\n\n\ndef factored_normal_variational_model():\n    qt = yield tfd.Normal(loc=qt_mean, scale=qt_stddv, name=\"qt\")\n\n\nsurrogate_posterior = tfd.JointDistributionCoroutineAutoBatched(\n    factored_normal_variational_model\n)\n\nlosses = tfp.vi.fit_surrogate_posterior(\n    target_log_prob_fn=target_log_prob_fn,\n    surrogate_posterior=surrogate_posterior,\n    optimizer=tf.optimizers.Adam(learning_rate=0.05),\n    num_steps=100,\n)\n\n/Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/internal/vectorization_util.py:87: UserWarning: Saw Tensor seed Tensor(\"seed:0\", shape=(2,), dtype=int32), implying stateless sampling. Autovectorized functions that use stateless sampling may be quite slow because the current implementation falls back to an explicit loop. This will be fixed in the future. For now, you will likely see better performance from stateful sampling, which you can invoke by passing a Python `int` seed.\n  warnings.warn(\n/Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/internal/vectorization_util.py:87: UserWarning: Saw Tensor seed Tensor(\"seed:0\", shape=(2,), dtype=int32), implying stateless sampling. Autovectorized functions that use stateless sampling may be quite slow because the current implementation falls back to an explicit loop. This will be fixed in the future. For now, you will likely see better performance from stateful sampling, which you can invoke by passing a Python `int` seed.\n  warnings.warn(\n\n\n\nplt.plot(losses)\n\n\n\n\n\n\n\nStructTuple(\n  qt=<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5720905, 0.4626296], dtype=float32)>\n)\n\n\n\nqt_mean, qt_stddv\n\n(<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1.5777218 , 0.46246716], dtype=float32)>,\n <TransformedVariable: name=softplus, dtype=float32, shape=[2], fn=\"softplus\", numpy=array([0.01456336, 0.01367522], dtype=float32)>)\n\n\n\nplt.scatter(x[:, 0], data_sample, s=10)\nplt.plot(x[:, 0], tf.reshape(x*qt_mean[1] + qt_mean[0], [-1]))\n\n\n\n\n\npost_samples = surrogate_posterior.sample(200)\n\npost_samples.qt[0:5]\n\nWARNING:tensorflow:Note that RandomStandardNormal inside pfor op may not give same output as inside a sequential loop.\n\n\n<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\narray([[1.5795265 , 0.500741  ],\n       [1.5515635 , 0.46671686],\n       [1.5585055 , 0.4617632 ],\n       [1.5856469 , 0.44141397],\n       [1.5763292 , 0.45420292]], dtype=float32)>\n\n\n\nplt.scatter(x[:, 0], data_sample, s=10)\n\nplt.plot(x[:, 0], tf.reshape(x*qt_mean[1] + qt_mean[0], [-1]))\n\n\n\n\nReferences\n\nhttps://www.youtube.com/watch?v=HUsznqt2V5I\nhttps://www.youtube.com/watch?v=x9StQ8RZ0ag&list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&index=9\nhttps://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-09-13-02-Minimizing-KL-Divergence.ipynb#scrollTo=gd_ev8ceII8q\nhttps://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/09/13/02-Minimizing-KL-Divergence.html"
  },
  {
    "objectID": "posts/2022-02-21-coordinate-descent-failure.html",
    "href": "posts/2022-02-21-coordinate-descent-failure.html",
    "title": "Coordinate descent failure example",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport seaborn as sns\nfrom functools import partial\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nX = torch.linspace(-5, 5, 200).unsqueeze(-1)\nY = torch.linspace(-5, 5, 200).unsqueeze(0)\nshape = torch.Size((X.shape[0], Y.shape[1]))\nX = X.expand(shape)\nY = Y.expand(shape)\n\n\ndef f(x, y):\n    return torch.abs(x+y) + 3*torch.abs(y-x)\n\nplt.contourf(X.numpy(), Y.numpy(), f(X, Y).numpy(), [1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.colorbar()\n\n\nplt.gca().set_aspect(\"equal\")\n\n\n\n\n\ndef plot_point(x, y):\n    plt.contour(X.numpy(), Y.numpy(), f(X, Y).numpy(), levels=[1, 4, 7, 10, 13, 16, 19] )\n\n    plt.scatter(x, y, zorder=5, color='k', s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.gca().set_aspect(\"equal\")\n\n\nx_start, y_start = -4, 2\nplot_point(x_start, y_start)\n\n\n\n\nFix x2 = 2\n\nx_start, y_start = -4, 2\nplot_point(x_start, y_start)\nplt.axhline(2, color='red')\n\n<matplotlib.lines.Line2D at 0x1366d2e80>\n\n\n\n\n\n\ng = partial(f, y=2)\ng\n\nfunctools.partial(<function f at 0x136323310>, y=2)\n\n\n\nx_learn = torch.tensor(0., requires_grad = True)\noptim = torch.optim.Adam([x_learn], lr=0.0005)\n\nfor i in range(3000):\n    loss = g(x_learn)\n    loss.backward()\n    optim.step()\n    if i%100==0:\n        print(i, x_learn.item(), loss.item())\n\n0 0.0004999999655410647 8.0\n100 0.06779850274324417 7.865949630737305\n200 0.14714795351028442 7.707312107086182\n300 0.22784578800201416 7.545924663543701\n400 0.3085353970527649 7.384539604187012\n500 0.38880792260169983 7.223984718322754\n600 0.4684963822364807 7.0645952224731445\n700 0.5475249886512756 6.906523704528809\n800 0.6258596777915955 6.74984073638916\n900 0.7034878730773926 6.594569683074951\n1000 0.7804093360900879 6.440712928771973\n1100 0.8566311597824097 6.288255214691162\n1200 0.9321646094322205 6.137174606323242\n1300 1.0070239305496216 5.987442493438721\n1400 1.0812252759933472 5.839026927947998\n1500 1.1547856330871582 5.691894054412842\n1600 1.2277231216430664 5.546006679534912\n1700 1.3000539541244507 5.401332855224609\n1800 1.371799349784851 5.257830619812012\n1900 1.4429757595062256 5.115466594696045\n2000 1.5136009454727173 4.974205493927002\n2100 1.583693265914917 4.834010124206543\n2200 1.6532690525054932 4.694848537445068\n2300 1.7223458290100098 4.556684970855713\n2400 1.7909395694732666 4.419487953186035\n2500 1.8590670824050903 4.283223628997803\n2600 1.926743507385254 4.147861957550049\n2700 1.9939842224121094 4.01337194442749\n2800 2.058466672897339 4.231417655944824\n2900 2.11668062210083 4.464505195617676\n\n\nHard to optimize!\n\nx_dummy = torch.linspace(-5, 5, 400)\nplt.plot(x_dummy, g(x_dummy))\nplt.ylabel(f\"f(x, 2)\")\nplt.xlabel(\"x\")\n\nText(0.5, 0, 'x')\n\n\n\n\n\n\nx_start, y_start = 2, 2\nplot_point(x_start, y_start)\nplt.axvline(2, color='red')\n\n<matplotlib.lines.Line2D at 0x1369aa190>\n\n\n\n\n\nhttps://stats.stackexchange.com/questions/146317/coordinate-vs-gradient-descent"
  },
  {
    "objectID": "posts/2013-07-01-hmm_continuous.html",
    "href": "posts/2013-07-01-hmm_continuous.html",
    "title": "HMM Simulation for Continuous HMM",
    "section": "",
    "text": "In this notebook we shall create a continuous Hidden Markov Model [1] for an electrical appliance. Problem description:\nIn all it matches the description of a continuous Hidden Markov Model. The different components of the Discrete HMM are as follows:\nNext, we import the basic set of libraries used for matrix manipulation and for plotting.\nNext, we define the different components of HMM which were described above.\nNow based on these probability we need to produce a sequence of observed and hidden states. We use the notion of weighted sampling, which basically means that terms/states with higher probabilies assigned to them are more likely to be selected/sampled. For example,let us consider the starting state. For this we need to use the pi matrix, since that encodes the likiliness of starting in a particular state. We observe that for starting in Fair state the probability is .667 and twice that of starting in Biased state. Thus, it is much more likely that we start in Fair state. We use Fitness Proportionate Selection [3] to sample states based on weights (probability). For selection of starting state we would proceed as follows:\nWe test the above function by making a call to it 1000 times and then we try to see how many times do we get a 0 (Fair) wrt 1 (Biased), given the pi vector.\nThus, we can see that we get approximately twice the number of Fair states as Biased states which is as expected.\nNext, we write the following functions:\nThus, using these functions and the HMM paramters we decided earlier, we create length 1000 sequence for hidden and observed states.\nNow, we create helper functions to plot the two sequence in a way we can intuitively understand the HMM."
  },
  {
    "objectID": "posts/2013-07-01-hmm_continuous.html#references",
    "href": "posts/2013-07-01-hmm_continuous.html#references",
    "title": "HMM Simulation for Continuous HMM",
    "section": "References",
    "text": "References\n\nhttp://en.wikipedia.org/wiki/Hidden_Markov_model\nhttp://www.stanford.edu/class/stats366/hmmR2.html\nhttp://en.wikipedia.org/wiki/Fitness_proportionate_selection\nhttp://eli.thegreenplace.net/2010/01/22/weighted-random-generation-in-python/\nhttp://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list"
  },
  {
    "objectID": "posts/2022-02-14-logistic-regression.html",
    "href": "posts/2022-02-14-logistic-regression.html",
    "title": "Logistic Regression using PyTorch distributions",
    "section": "",
    "text": "Generative model for logistic regression\n\nx = dist.Normal(loc = torch.tensor([0., 0.]), scale=torch.tensor([1., 2.]))\nx_sample = x.sample([100])\nx_sample.shape\n\nx_dash = torch.concat((torch.ones(x_sample.shape[0], 1), x_sample), axis=1)\n\n\ntheta = dist.MultivariateNormal(loc = torch.tensor([0., 0., 0.]), covariance_matrix=0.5*torch.eye(3))\ntheta_sample = theta.sample()\n\np = torch.sigmoid(x_dash@theta_sample)\n\ny = dist.Bernoulli(probs=p)\ny_sample = y.sample()\n\n\nplt.scatter(x_sample[:, 0], x_sample[:, 1], c = y_sample, s=40, alpha=0.5)\nsns.despine()\n\n\n\n\n\ntheta_sample\n\ntensor([ 0.6368, -0.7526,  1.4652])\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nlr_l2 = LogisticRegression()\nlr_none = LogisticRegression(penalty='none')\n\n\nlr_l2.fit(x_sample, y_sample)\nlr_none.fit(x_sample, y_sample)\n\nLogisticRegression(penalty='none')\n\n\n\ndef plot_fit(x_sample, y_sample, theta, model_name):\n\n    \n    # Retrieve the model parameters.\n    b = theta[0]\n    w1, w2 = theta[1], theta[2]\n    # Calculate the intercept and gradient of the decision boundary.\n    c = -b/w2\n    m = -w1/w2\n\n    # Plot the data and the classification with the decision boundary.\n    xmin, xmax = x_sample[:, 0].min()-0.2, x_sample[:, 0].max()+0.2\n    ymin, ymax =  x_sample[:, 1].min()-0.2, x_sample[:, 1].max()+0.2\n    xd = np.array([xmin, xmax])\n    yd = m*xd + c\n    plt.plot(xd, yd, 'k', lw=1, ls='--')\n    plt.fill_between(xd, yd, ymin, color='tab:blue', alpha=0.2)\n    plt.fill_between(xd, yd, ymax, color='tab:orange', alpha=0.2)\n\n    plt.scatter(*x_sample[y_sample==0].T, s=20, alpha=0.5)\n    plt.scatter(*x_sample[y_sample==1].T, s=20, alpha=0.5)\n    plt.xlim(xmin, xmax)\n    plt.ylim(ymin, ymax)\n    plt.ylabel(r'$x_2$')\n    plt.xlabel(r'$x_1$')\n    theta_print = np.round(theta, 1)\n    plt.title(f\"{model_name}\\n{theta_print}\")\n    sns.despine()\n\n\nplot_fit(\n    x_sample,\n    y_sample,\n    theta_sample,\n    r\"Generating $\\theta$\",\n)\n\n\n\n\n\nplot_fit(\n    x_sample,\n    y_sample,\n    np.concatenate((lr_l2.intercept_.reshape(-1, 1), lr_l2.coef_), axis=1).flatten(),\n    r\"Sklearn $\\ell_2$ penalty \",\n)\n\n\n\n\n\nplot_fit(\n    x_sample,\n    y_sample,\n    np.concatenate((lr_none.intercept_.reshape(-1, 1), lr_none.coef_), axis=1).flatten(),\n    r\"Sklearn No penalty \",\n)\n\n\n\n\n\n\nMLE estimate PyTorch\n\ndef neg_log_likelihood(theta, x, y):\n    x_dash = torch.concat((torch.ones(x.shape[0], 1), x), axis=1)\n    p = torch.sigmoid(x_dash@theta)\n    y_dist = dist.Bernoulli(probs=p)\n\n    return -torch.sum(y_dist.log_prob(y))\n\n\nneg_log_likelihood(theta_sample, x_sample, y_sample)\n\ntensor(33.1907)\n\n\n\ntheta_learn_loc = torch.tensor([1., 1., 1.], requires_grad=True)\nneg_log_likelihood(theta_learn_loc, x_sample, y_sample)\n\nplot_fit(\n    x_sample,\n    y_sample,\n    theta_learn_loc.detach(),\n    r\"Torch without training\",\n)\n\n\n\n\n\ntheta_learn_loc = torch.tensor([0., 0., 0.], requires_grad=True)\nloss_array = []\nloc_array = []\n\nopt = torch.optim.Adam([theta_learn_loc], lr=0.05)\nfor i in range(101):\n    loss_val = neg_log_likelihood(theta_learn_loc, x_sample, y_sample)\n    loss_val.backward()\n    loc_array.append(theta_learn_loc)\n    loss_array.append(loss_val.item())\n\n    if i % 10 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss_val.item():0.2f}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 69.31\nIteration: 10, Loss: 44.14\nIteration: 20, Loss: 35.79\nIteration: 30, Loss: 32.73\nIteration: 40, Loss: 31.67\nIteration: 50, Loss: 31.25\nIteration: 60, Loss: 31.08\nIteration: 70, Loss: 31.00\nIteration: 80, Loss: 30.97\nIteration: 90, Loss: 30.95\nIteration: 100, Loss: 30.94\n\n\n\nplot_fit(\n    x_sample,\n    y_sample,\n    theta_learn_loc.detach(),\n    r\"Torch MLE\",\n)\n\n\n\n\n\n\nMAP estimate PyTorch\n\nprior_theta = dist.MultivariateNormal(loc = torch.tensor([0., 0., 0.]), covariance_matrix=2*torch.eye(3))\n\nlogprob = lambda theta: -prior_theta.log_prob(theta)\n\n\ntheta_learn_loc = torch.tensor([0., 0., 0.], requires_grad=True)\nloss_array = []\nloc_array = []\n\nopt = torch.optim.Adam([theta_learn_loc], lr=0.05)\nfor i in range(101):\n    loss_val = neg_log_likelihood(theta_learn_loc, x_sample, y_sample) + logprob(theta_learn_loc)\n    loss_val.backward()\n    loc_array.append(theta_learn_loc)\n    loss_array.append(loss_val.item())\n\n    if i % 10 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss_val.item():0.2f}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 73.11\nIteration: 10, Loss: 48.06\nIteration: 20, Loss: 39.89\nIteration: 30, Loss: 37.01\nIteration: 40, Loss: 36.10\nIteration: 50, Loss: 35.78\nIteration: 60, Loss: 35.67\nIteration: 70, Loss: 35.64\nIteration: 80, Loss: 35.62\nIteration: 90, Loss: 35.62\nIteration: 100, Loss: 35.62\n\n\n\nplot_fit(\n    x_sample,\n    y_sample,\n    theta_learn_loc.detach(),\n    r\"Torch MAP\",\n)\n\n\n\n\n\n\nReferences\n\n\nPlotting code borrwed from here: https://scipython.com/blog/plotting-the-decision-boundary-of-a-logistic-regression-model/"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2020-06-26-gp-understand.html",
    "href": "posts/2020-06-26-gp-understand.html",
    "title": "Understanding Kernels in Gaussian Processes Regression",
    "section": "",
    "text": "Covariance functions, aka kernels\nWe will define a covariance function, from hereon referred to as a kernel, using GPy. The most commonly used kernel in machine learning is the Gaussian-form radial basis function (RBF) kernel. It is also commonly referred to as the exponentiated quadratic or squared exponential kernel – all are equivalent.\nThe definition of the (1-dimensional) RBF kernel has a Gaussian-form, defined as:\n\\[\n    \\kappa_\\mathrm{rbf}(x,x') = \\sigma^2\\exp\\left(-\\frac{(x-x')^2}{2\\mathscr{l}^2}\\right)\n\\]\nIt has two parameters, described as the variance, \\(\\sigma^2\\) and the lengthscale \\(\\mathscr{l}\\).\nIn GPy, we define our kernels using the input dimension as the first argument, in the simplest case input_dim=1 for 1-dimensional regression. We can also explicitly define the parameters, but for now we will use the default values:\n\n# Create a 1-D RBF kernel with default parameters\nk = GPy.kern.RBF(lengthscale=0.5, input_dim=1, variance=4)\n# Preview the kernel's parameters\nk\n\n\n\n  rbf.       valueconstraintspriors\n  variance     4.0    +ve          \n  lengthscale  0.5    +ve          \n\n\n\n\nfig, ax = plt.subplots()\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nls = [0.0005, 0.05, 0.25, 0.5, 1., 2., 4.]\n\nX = np.linspace(0.,1.,500)# 500 points evenly spaced over [0,1]\nX = X[:,None]\nmu = np.zeros((500))\n\ndef update(iteration):\n    ax.cla()\n    k = GPy.kern.RBF(1)\n    k.lengthscale = ls[iteration]\n    # Calculate the new covariance function at k(x,0)\n    C = k.K(X,X)\n    Z = np.random.multivariate_normal(mu,C,40)\n    for i in range(40):\n        ax.plot(X[:],Z[i,:],color='k',alpha=0.2)\n    ax.set_title(\"$\\kappa_{rbf}(x,x')$\\nLength scale = %s\" %k.lengthscale[0]);\n    ax.set_ylim((-4, 4))\n\n\n\nnum_iterations = len(ls)\nanim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations-1, 1), interval=500)\nplt.close()\n\nrc('animation', html='jshtml')\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nIn the animation above, as you increase the length scale, the learnt functions keep getting smoother.\n\nfig, ax = plt.subplots()\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nvar = [0.0005, 0.05, 0.25, 0.5, 1., 2., 4., 9.]\n\nX = np.linspace(0.,1.,500)# 500 points evenly spaced over [0,1]\nX = X[:,None]\nmu = np.zeros((500))\n\ndef update(iteration):\n    ax.cla()\n    k = GPy.kern.RBF(1)\n    k.variance = var[iteration]\n    # Calculate the new covariance function at k(x,0)\n    C = k.K(X,X)\n    Z = np.random.multivariate_normal(mu,C,40)\n    for i in range(40):\n        ax.plot(X[:],Z[i,:],color='k',alpha=0.2)\n    ax.set_title(\"$\\kappa_{rbf}(x,x')$\\nVariance = %s\" %k.variance[0]);\n    ax.set_ylim((-4, 4))\n\n\n\nnum_iterations = len(ls)\nanim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations-1, 1), interval=500)\nplt.close()\n\nrc('animation', html='jshtml')\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nIn the animation above, as you increase the variance, the scale of values increases.\n\nX1 = np.array([1, 2, 3]).reshape(-1, 1)\n\ny1 = np.array([0, 1, 0]).reshape(-1, 1)\ny2 = np.array([0, -1, 0]).reshape(-1, 1)\ny3 = np.array([0, 10, 0]).reshape(-1, 1)\ny4 = np.array([0, 0.3, 0]).reshape(-1, 1)\n\n\nk = GPy.kern.RBF(lengthscale=0.5, input_dim=1, variance=4)\n\nm = GPy.models.GPRegression(X1, y1, k)\n#m.Gaussian_noise = 0.0\nm.optimize()\nprint(k)\nm.plot();\n\n  rbf.         |                value  |  constraints  |  priors\n  variance     |    0.262031485550043  |      +ve      |        \n  lengthscale  |  0.24277532672486218  |      +ve      |        \n\n\n\n\n\n\nk = GPy.kern.RBF(lengthscale=0.5, input_dim=1, variance=4)\n\nm = GPy.models.GPRegression(X1, y2, k)\n#m.Gaussian_noise = 0.0\nm.optimize()\nprint(k)\nm.plot();\n\n  rbf.         |                value  |  constraints  |  priors\n  variance     |    0.262031485550043  |      +ve      |        \n  lengthscale  |  0.24277532672486218  |      +ve      |        \n\n\n\n\n\n\n\nIn the above two examples, the y values are: 0, 1, 0 and 0, -1, 0. This shows smoothness. Thus, length scale can be big (0.24)\n\nk = GPy.kern.RBF(lengthscale=0.5, input_dim=1, variance=4)\n\nm = GPy.models.GPRegression(X1, y3, k)\n#m.Gaussian_noise = 0.0\nm.optimize()\nprint(k)\nm.plot();\n\n  rbf.         |                value  |  constraints  |  priors\n  variance     |   16.918792970578004  |      +ve      |        \n  lengthscale  |  0.07805339389352635  |      +ve      |        \n\n\n\n\n\n\n\nIn the above example, the y values are: 0, 10, 0. The data set is not smooth. Thus, length scale learnt uis very small (0.24). Noise variance of RBF kernel also increased to accomodate the 10.\n\nk = GPy.kern.RBF(lengthscale=0.5, input_dim=1, variance=4)\n\nm = GPy.models.GPRegression(X1, y4, k)\n#m.Gaussian_noise = 0.0\nm.optimize()\nprint(k)\nm.plot();\n\n  rbf.         |                 value  |  constraints  |  priors\n  variance     |  5.90821963086592e-06  |      +ve      |        \n  lengthscale  |     2.163452641925496  |      +ve      |        \n\n\n\n\n\n\n\nIn the above examples, the y values are: 0, 0.3, 0. The data set is the smoothest amongst the four. Thus, length scale learnt is large (2.1). Noise variance of RBF kernel is also small."
  },
  {
    "objectID": "posts/2014-07-01-mcmc_coins.html",
    "href": "posts/2014-07-01-mcmc_coins.html",
    "title": "Coin tosses and MCMC",
    "section": "",
    "text": "twitter: https://twitter.com/nipun_batra/status/460286604796383233\n\nThus, I started to investiage how good would MCMC methods perform in comparison to EM.\n\nProblem statement\nFor a detailed understanding, refer to the previous post. In short, the problem is as follows.\nYou have two coins - A and B. Both of them have their own biases (probability of obtaining a head (or a tail )). We pick a coin at random and toss it up 10 times. We repeat this procedure 5 times, totaling in 5 observation sets of 10 observations each. However, we are not told which coin was tossed. So, looking at the data and some rough initial guess about the respective coin biases, can we tell something about the likely biases? Let us work it up using PyMC.\n\n\nCustomary imports\n\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\n%matplotlib inline\n\n\n\nObservations\nWe use 1 for Heads and 0 for tails.\n\n# 5 observation sets of 10 observations each\nobservations = np.array([[1,0,0,0,1,1,0,1,0,1],\n                         [1,1,1,1,0,1,1,1,1,1],\n                         [1,0,1,1,1,1,1,0,1,1],\n                         [1,0,1,0,0,0,1,1,0,0],\n                         [0,1,1,1,0,1,1,1,0,1]])\nobservations_flattened = observations.flatten()\n\n\n\nNumber of heads in each obervation set\n\nn_heads_array = np.sum(observations, axis=1)\nplt.bar(range(len(n_heads_array)),n_heads_array.tolist());\nplt.title(\"Number of heads vs Observation set\")\nplt.xlabel(\"Observation set\")\nplt.ylabel(\"#Heads observed\");\n\n\n\n\n\n\nGround truth\nThe true sequence of coin tosses which was hidden from us. True indicated coin A and False indicates coin B.\n\n# Ground truth\ncoins_id = np.array([False,True,True,False,True])\n\nNumber of observation sets and number of observations in each set. This allows us to modify data easily (as opposed to hard coding stuff).\n\nn_observation_sets = observations.shape[0]\nn_observation_per_set = observations.shape[1]\n\n\n\nModel for the problem statement\nWe pick up a simple prior on the bias of coin A.\n\\(\\theta_a\\) ~ \\(\\beta(h_a,t_a)\\)\nSimilarly, for coin B.\n\\(\\theta_b\\) ~ \\(\\beta(h_b,t_b)\\)\nFor any given observation set, we assign equal probability to it coming from coin A or B.\n\\(coin choice\\) ~ DiscreteUniform(0,1)\nThus, if coin choice is known, then the associated bias term is fixed.\n\\(\\theta\\) = \\(\\theta_a\\) if \\(coin choice\\) =1 else \\(\\theta_b\\)\nLike, we did in the previous post, we use Binomial distribution with the bias as \\(\\theta\\). For each observation set, we calculate the number of heads observed and model it as our observed variable obs.\n\\(obs\\) = Binomial(n_tosses_per_observation_set, p = \\(\\theta\\) )\nLet us draw this model using daft.\n\nimport daft\n\npgm = daft.PGM([3.6, 2.7], origin=[1, 0.65])\npgm.add_node(daft.Node(\"theta_a\", r\"$\\theta_a$\", 4, 3, aspect=1.8))\npgm.add_node(daft.Node(\"theta_b\", r\"$\\theta_b$\", 3, 3, aspect=1.2))\npgm.add_node(daft.Node(\"theta\", r\"$\\theta$\", 3.5, 2, aspect=1.8))\npgm.add_node(daft.Node(\"coin_choice\", r\"coin_choice\", 2, 2, aspect=1.8))\npgm.add_node(daft.Node(\"obs\", r\"obs\", 3.5, 1, aspect=1.2, observed=True))\npgm.add_edge(\"theta_a\", \"theta\")\npgm.add_edge(\"theta_b\", \"theta\")\npgm.add_edge(\"coin_choice\", \"theta\")\npgm.add_edge(\"theta\", \"obs\")\npgm.render();\n\n\n\n\nThe following segment codes the above model.\n\n# Prior on coin A (For now we choose 2 H, 2 T)\ntheta_a = pm.Beta('theta_a',2,2)\n\n# Prior on coin B (For now we choose 2 H, 2 T)\ntheta_b = pm.Beta('theta_b',2,2)\n\n# Choosing either A or B \ncoin_choice_array = pm.DiscreteUniform('coin_choice',0,1, size = n_observation_sets)\n\n# Creating a theta (theta_a if A is tossed or theta_b if B is tossed)\n@pm.deterministic\ndef theta(theta_a = theta_a, theta_b=theta_b, coin_choice_array=coin_choice_array):\n    #print coin_choice_array\n    out = np.zeros(n_observation_sets)\n    for i, coin_choice in enumerate(coin_choice_array):\n        if coin_choice:\n            out[i] = theta_a\n        else:\n            out[i] = theta_b                   \n    return out\n\nLet us examine how theta would be related to coin choice and other variables we have defined.\n\ntheta_a.value\n\narray(0.46192564399429575)\n\n\n\ntheta_b.value\n\narray(0.5918506713420255)\n\n\n\ncoin_choice_array.value\n\narray([1, 1, 0, 0, 1])\n\n\n\ntheta.value\n\narray([ 0.46192564,  0.46192564,  0.59185067,  0.59185067,  0.46192564])\n\n\nSo, whenever we see coin A, we put it’s bias in theta and likewise if we observe coin B. Now, let us create a model for our observations which is binomial as discussed above.\n\nobservation_model = pm.Binomial(\"obs\", n=n_observation_per_set, p=theta, value = n_heads_array, observed=True)\nmodel = pm.Model([observation_model, theta_a, theta_b, coin_choice_array])\n\nLet us view a few samples from the observation_model which returns the number of heads in 5 sets of 10 tosses.\n\nobservation_model.random()\n\narray([4, 4, 7, 2, 6])\n\n\n\nobservation_model.random()\n\narray([4, 4, 7, 5, 2])\n\n\n\nmcmc = pm.MCMC(model)\nmcmc.sample(40000, 10000, 1)\n\n [-----------------100%-----------------] 40000 of 40000 complete in 7.8 sec\n\n\nLet us have a look at our posteriors for \\(\\theta_a\\) and \\(\\theta_b\\)\n\nplt.hist(mcmc.trace('theta_a')[:], bins=20,alpha=0.7,label = r\"$\\theta_a$\");\nplt.hist(mcmc.trace('theta_b')[:], bins=20,alpha=0.4,label= r\"$\\theta_b$\");\nplt.legend();\n\n\n\n\nLooks like both \\(\\theta_a\\) and \\(\\theta_b\\) peak around the same value. But, wasn’t it expected? We gave both of them the same priors. This was also the case when we initialed EM with same values for both \\(\\theta_a\\) and \\(\\theta_b\\). So, let us add some informative priors an see how we do. Like in EM experiment, we knew that one of the coin was more biased than the other. So, let us make that the case and rerun the experiment.\n\n\nMore informative priors\n\n# Prior on coin A (more likely to have heads)\ntheta_a = pm.Beta('theta_a',4,2)\n\n# Prior on coin B (more likely to have tails)\ntheta_b = pm.Beta('theta_b',2,4)\n\n# Choosing either A or B (for 5 observations)\ncoin_choice_array = pm.DiscreteUniform('coin_choice',0,1, size = 5)\n\n# Creating a theta (theta_a if A is tossed or theta_b if B is tossed)\n@pm.deterministic\ndef theta(theta_a = theta_a, theta_b=theta_b, coin_choice_array=coin_choice_array):\n    #print coin_choice_array\n    out = np.zeros(n_observation_sets)\n    for i, coin_choice in enumerate(coin_choice_array):\n        if coin_choice:\n            out[i] = theta_a\n        else:\n            out[i] = theta_b                   \n    return out\n\n\nobservation_model = pm.Binomial(\"obs\", n=10, p=theta, value = n_heads_array, observed=True)\nmodel = pm.Model([observation_model, theta_a, theta_b, coin_choice_array])\n\n\nmcmc = pm.MCMC(model)\nmcmc.sample(40000, 10000, 1)\n\n [-----------------100%-----------------] 40000 of 40000 complete in 8.0 sec\n\n\n\nplt.hist(mcmc.trace('theta_a')[:], bins=20,alpha=0.7,label = r\"$\\theta_a$\");\nplt.hist(mcmc.trace('theta_b')[:], bins=20,alpha=0.4,label= r\"$\\theta_b$\");\nplt.legend();\n\n\n\n\nQuiet a clear distinction now! \\(\\theta_a\\) seems to peak around 0.8 and \\(\\theta_b\\) around 0.5. This matches our results from EM. However, unlike EM, we have much more than point estimates.\nFeel free to leave your comments and suggestions by."
  },
  {
    "objectID": "posts/2019-08-20-gaussian-processes.html",
    "href": "posts/2019-08-20-gaussian-processes.html",
    "title": "Gaussian Processes",
    "section": "",
    "text": "Introduction\nThere exist some great online resources for Gaussian Processes (GPs) including an excellent recent Distill.Pub article. This blog post is an attempt with a programatic flavour. In this notebook, we will build the intuition and learn some basics of GPs. This notebook is heavily inspired by the awesome tutorial by Richard Turner. Here is the link to the slides and video. Lectures videos and notes from Nando De Freitas’ course are an amazing resource for GPs (and anything ML!).\n\n\nSome imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n\n\nA function to make the Matplotlib plots prettier\n\nSPINE_COLOR = 'gray'\n\ndef format_axes(ax):\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\n\nOne dimensional Gaussian/Normal\nWe will start the discussion with 1d Gaussians. Let us write some simple code to generate/sample data from \\(\\mathcal{N}(\\mu=0, \\sigma=1)\\)\n\none_dim_normal_data = np.random.normal(0, 1, size=10000)\n\nLet us now visualise the data in a 1d space using scatter plot\n\nplt.scatter(one_dim_normal_data, np.zeros_like(one_dim_normal_data), alpha=0.2, c='gray', edgecolors='k', marker='o')\nformat_axes(plt.gca())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f9d08faed90>\n\n\n\n\n\nAs we would expect, there are a lot of samples close to zero (mean) and as we go further away from zero, the number of samples keeps reducing. We can also visualise the same phenomenon using a normed histogram shown below.\n\nplt.hist(one_dim_normal_data, density=True, bins=20, color='gray')\nformat_axes(plt.gca())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f9d090a4a60>\n\n\n\n\n\nWe can notice that there is a high probability of drawing samples close to the mean and the probability is low far from the mean.\nHowever, since histograms come with their own set of caveats, let us use kernel desnity estimation for obtaining the probability density of 1d Gaussian.\n\nfrom sklearn.neighbors import KernelDensity\n\nx_d = np.linspace(-4, 4, 100)\n\n# instantiate and fit the KDE model\nkde = KernelDensity(bandwidth=1.0, kernel='gaussian')\nkde.fit(one_dim_normal_data[:, None])\n\n# score_samples returns the log of the probability density\nlogprob = kde.score_samples(x_d[:, None])\n\nplt.fill_between(x_d, np.exp(logprob), alpha=0.2, color='gray')\nplt.plot(one_dim_normal_data, np.full_like(one_dim_normal_data, -0.01), '|k', markeredgewidth=0.1)\nformat_axes(plt.gca())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f9d0a08c340>\n\n\n\n\n\nWe can now see a smoother version of the histogram and can again verify the properties of 1D Gaussian. Let us now vary the variance of 1D Gaussian and make the same plots to enhance our understanding of the concept.\n\nfig, ax = plt.subplots(ncols=3, sharey=True, figsize=(9, 3))\nx_d = np.linspace(-6, 6, 400)\n\nfor i, var in enumerate([0.5, 1, 2]):\n    one_dim_normal_data = np.random.normal(0, var, size=10000)\n    kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n    kde.fit(one_dim_normal_data[:, None])\n\n    # score_samples returns the log of the probability density\n    logprob = kde.score_samples(x_d[:, None])\n\n    ax[i].fill_between(x_d, np.exp(logprob), alpha=0.2, color='gray')\n    ax[i].plot(one_dim_normal_data, np.full_like(one_dim_normal_data, -0.01), '|k', markeredgewidth=0.1)\n    format_axes(ax[i])\n    ax[i].set_title(f\"Variance = {var}\")\n\n\n\n\nWe can see that how increasing the variance makes the data more spread.\n\n\nBi-variate Gaussian\nHaving discussed the case of 1d Gaussian, now let us move to multivariate Gaussians. As a special case, let us first consider bi-variate or 2d Gaussian. It’s parameters are the mean vector which will have 2 elements and a covariance matrix.\nWe can write the distribution as: \\[\n\\begin{pmatrix}\nX_1 \\\\\nX_2\n\\end{pmatrix}  \\sim \\mathcal{N} \\left( \\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{pmatrix} , \\begin{pmatrix}\na & \\rho \\\\\n\\rho & b\n\\end{pmatrix} \\right)\n\\]\nwhere \\(\\mu_1\\), \\(\\mu_2\\) are the means for \\(X_1\\) and \\(X_2\\) respectively; \\(a\\) is the standard deviation for \\(X_1\\), \\(b\\) is the standard deviation for \\(X_2\\) and \\(\\rho\\) is the correlation between \\(X_1\\) and \\(X_2\\)\nLet us now draw some data from: \\[\n\\begin{pmatrix}\nX_1 \\\\\nX_2\n\\end{pmatrix}  \\sim \\mathcal{N} \\left( \\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix} , \\begin{pmatrix}\n1 & 0.7 \\\\\n0.7 & 1\n\\end{pmatrix} \\right)\n\\]\n\ndata = np.random.multivariate_normal(mean = np.array([0, 0]), cov = np.array([[1, 0.7], [0.7, 1]]), size=(10000, ))\n\n\nplt.scatter(data[:, 0], data[:, 1], alpha=0.05,c='gray')\nplt.axhline(0, color='k', lw=0.2)\nplt.axvline(0, color='k', lw=0.2)\nplt.xlabel(r\"$X_1$\")\nplt.ylabel(r\"$X_2$\")\n\nformat_axes(plt.gca())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f9d0a235610>\n\n\n\n\n\nWe can see from the plot above that the data is distributed around mean [0, 0]. We can also see the positive correlation between \\(X_1\\) and \\(X_2\\)\n\n\nMarginalisation for bivariate Gaussian\nLet us look into an interesting plot provided by Seaborn.\n\nimport pandas as pd\ndata_df = pd.DataFrame(data, columns=[r'$X_1$',r'$X_2$'])\n\n\nimport seaborn as sns\ng = sns.jointplot(x= r'$X_1$', y=r'$X_2$', data=data_df, kind=\"reg\",color='gray')\n\n\n\n\nThe central plot is exactly the same as the scatter plot we made earlier. But, we see two additional 1d KDE plots at the top and the right. What do these tell us? These tell us the marginal 1d distributions of \\(X_1\\) and \\(X_2\\).\nThe marginal distribution of \\(X_1\\) is the distribution of \\(X_1\\) considering all values of \\(X_2\\) and vice versa. One of the interesting properties of Gaussian distributions is that the marginal distribution of a Gaussian is also a Gaussian distribution. MathematicalMonk on Youtube has a great set of lectures on this topic that I would highly recommend!\nWhat would you expect the marginal distribution of \\(X_1\\) to look like? No prizes for guessing.\nGiven \\[\n\\begin{pmatrix}\nX_1 \\\\\nX_2\n\\end{pmatrix}  \\sim \\mathcal{N} \\left( \\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{pmatrix} , \\begin{pmatrix}\na & \\rho \\\\\n\\rho & b\n\\end{pmatrix} \\right)\n\\]\nwe have the marginal distribution of: \\[X_1 \\sim \\mathcal{N}(\\mu_1, a)\\] and \\[X_2 \\sim \\mathcal{N}(\\mu_2, b)\\]\n\ndef plot_jointplot_2d(a, b, rho):\n    data = np.random.multivariate_normal(mean = np.array([0, 0]), cov = np.array([[a, rho], [rho, b]]), size=(10000, ))\n    data_df = pd.DataFrame(data, columns=[r'$X_1$',r'$X_2$'])\n    g = sns.jointplot(x= r'$X_1$', y=r'$X_2$', data=data_df, kind=\"reg\",color='gray')\n\nOk, let us know try to plot a few jointplots for different covariance matrices. We would be passing in the values of \\(a\\), \\(b\\) and \\(\\rho\\) which would make up the covariance matrix as:\n\\[\\begin{pmatrix}\na & \\rho \\\\\n\\rho & b\n\\end{pmatrix}\\]\nWe would make these plots for mean zero.\n\nplot_jointplot_2d(1, 1, -0.7)\n\n\n\n\nIn the plot above, for \\(a=1\\), \\(b=1\\) and \\(\\rho=0.7\\) we can see the negative correlation (but high) between \\(X_1\\) and \\(X_2\\).\nLet us now increase the variance in \\(X_1\\) and keep all other paramaters constant.\n\nplot_jointplot_2d(2, 1, -0.7)\n\n\n\n\nOne can see from the plot above that the variance in \\(X_1\\) is much higher now and the plot extends from -6 to +6 for \\(X_1\\) while earlier it was restricted from -4 to 4.\n\nplot_jointplot_2d(1, 1, 0.0)\n\n\n\n\nOne can see from the plot above that the correlation between \\(X_1\\) and \\(X_2\\) is zero.\n\nSurface plots for bi-variate Gaussian\nWe will now look into surface plots for bi-variate Gaussian. This is yet another way to plot and understand Gaussian distributions. I borrow code from an excellent tuorial on plotting bivariate Gaussians.\n\nfrom scipy.stats import multivariate_normal\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n\n\ndef make_pdf_2d_gaussian(mu, sigma):\n    N = 60\n    X = np.linspace(-3, 3, N)\n    Y = np.linspace(-3, 4, N)\n    X, Y = np.meshgrid(X, Y)\n\n    # Pack X and Y into a single 3-dimensional array\n    pos = np.empty(X.shape + (2,))\n    pos[:, :, 0] = X\n    pos[:, :, 1] = Y\n\n    F = multivariate_normal(mu, sigma)\n    Z = F.pdf(pos)\n\n\n\n    # Create a surface plot and projected filled contour plot under it.\n    fig = plt.figure()\n    ax = fig.gca(projection='3d')\n    ax.plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True,\n                    cmap=cm.Greys)\n    \n    ax.set_xlabel(r\"$X_1$\")\n    ax.set_ylabel(r\"$X_2$\")\n    ax.set_zlabel(\"PDF\")\n\n    cset = ax.contourf(X, Y, Z, zdir='z', offset=-0.15, cmap=cm.Greys)\n\n    # Adjust the limits, ticks and view angle\n    ax.set_zlim(-0.15,0.25)\n    ax.set_zticks(np.linspace(0,0.2,5))\n    ax.view_init(27, -15)\n    ax.set_title(f'$\\mu$ = {mu}\\n $\\Sigma$ = {sigma}')\n\n\nmu = np.array([0., 0.])\nsigma = np.array([[ 1. , -0.5], [-0.5,  1]])\n\nmake_pdf_2d_gaussian(mu, sigma)\n\n\n\n\nFrom the plot above, we can see the surface plot showing the probability density function for the Gaussian with mean\n\\[\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\]\nand covariance matrix:\n\\[\\begin{pmatrix}\n1 & -0.5 \\\\\n-0.5 & 1\n\\end{pmatrix}\\]\nIt can be seen that the probability peaks arounds \\(X_1=0\\) and \\(X_2=0\\). The bottom plot shows the same concept using contour plots which we will heavily use from now on. The different circles in the bottom contour plot denote the loci of same probability density. Since the contour plot requires a lesser dimension, it will be easier to use in our further analysis.\nAlso, from the contour plots, we can see the correlation between \\(X_1\\) and \\(X_2\\).\n\nmu = np.array([0., 0.])\nsigma = np.array([[ 1. , 0], [0,  1]])\n\nmake_pdf_2d_gaussian(mu, sigma)\n\n\n\n\nIn the plot above, we can see that \\(X_1\\) and \\(X_2\\) are not correlated.\n\n\nContour plots for 2D Gaussians\nHaving seen the relationship between the surface plots and the contour plots, we will now exclusively focus on the contour plots. Here is a simple function to generate the contour plot for 2g gaussian with mean and covariance as the arguments.\n\ndef plot_2d_contour_pdf(mu, sigma):\n    X = np.linspace(-3, 3, 60)\n    Y = np.linspace(-3, 4, 60)\n    X, Y = np.meshgrid(X, Y)\n\n    # Pack X and Y into a single 3-dimensional array\n    pos = np.empty(X.shape + (2,))\n    pos[:, :, 0] = X\n    pos[:, :, 1] = Y\n\n    F = multivariate_normal(mu, sigma)\n    Z = F.pdf(pos)\n    plt.xlabel(r\"$X_1$\")\n    plt.ylabel(r\"$X_2$\")\n    \n    plt.title(f'$\\mu$ = {mu}\\n $\\Sigma$ = {sigma}')\n    plt.contourf(X, Y, Z, zdir='z', offset=-0.15, cmap=cm.Greys)\n    plt.colorbar()\n    format_axes(plt.gca())\n\n\nmu = np.array([0., 0.])\nsigma = np.array([[ 1. , 0.5], [0.5,  1.]])\nplot_2d_contour_pdf(mu, sigma)\n\n\n\n\nThe plot above shows the contour plot for 2d gaussian with mean [0, 0] and covariance [[ 1. , 0.5], [0.5, 1.]]. We can see the correlation between \\(X_1\\) and \\(X_2\\)\n\n\n\nSample from 2d gaussian and visualising it on XY plane\nWe will now sample a point from a 2d Gaussian and describe a new way of visualising it.\n\n\nThe left most plot shows the covariance matrix.\nThe middle plot shows the contour plot. The dark point marked in the contour plot is a sampled point (at random) from this 2d Gaussian distribution.\nThe right most plot is an alternative representation of the sampled point. The x-axis corresponds to the labels \\(X_1\\) and \\(X_2\\) and the corresponding y-axis are the coordinates of the point in the \\(X_1\\), \\(X_2\\) dimension shown in the contour plot.\n\nWe will now write a function to generate a random sample from a 2d gaussian given it’s mean and covariance matrix.\n\ndef plot_2d_contour_pdf_dimensions(mu, sigma, random_num):\n    fig, ax  = plt.subplots(ncols=3, figsize=(12, 4))\n\n    X = np.linspace(-3, 3, 60)\n    Y = np.linspace(-3, 3, 60)\n    X, Y = np.meshgrid(X, Y)\n\n    # Pack X and Y into a single 3-dimensional array\n    pos = np.empty(X.shape + (2,))\n    pos[:, :, 0] = X\n    pos[:, :, 1] = Y\n\n    F = multivariate_normal(mu, sigma)\n    Z = F.pdf(pos)\n    random_point = F.rvs(random_state=random_num)\n    \n    sns.heatmap(sigma, ax=ax[0], annot=True)\n    ax[1].contour(X, Y, Z, cmap=cm.Greys)\n    ax[1].scatter(random_point[0], random_point[1], color='k',s=100)\n    ax[1].set_xlabel(r\"$X_1$\")\n    ax[1].set_ylabel(r\"$X_2$\")\n    \n    data_array = pd.Series(random_point, index=['X1','X2'])\n    data_array.plot(ax=ax[2], kind='line', marker='o',color='k')\n    plt.xticks(np.arange(len(data_array.index)), data_array.index.values)\n    ax[2].set_ylim(-3, 3)\n    \n    format_axes(ax[0])\n    format_axes(ax[1])\n    format_axes(ax[2])\n    ax[0].set_title(\"Covariance Matrix\")\n    ax[1].set_title(\"Contour of pdf\")\n    ax[2].set_title(\"Visualising the point\")\n    plt.suptitle(f\"Random state = {random_num}\", y=1.1)\n    plt.tight_layout()\n    import os\n    if not os.path.exists(\"images\"):\n        os.makedirs(\"images\")\n    if not os.path.exists(f\"images/{sigma[0, 1]}\"):\n        os.makedirs(f\"images/{sigma[0, 1]}\")\n    plt.savefig(f\"images/{sigma[0, 1]}/{random_num}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n\nWe will now create 20 such samples and animate them\n\nfor i in range(20):\n    plot_2d_contour_pdf_dimensions( mu, np.array([[ 1. , 0.1], [0.1,  1.]]), i)\n\n\n!convert -delay 20 -loop 0 images/0.1/*.jpg sigma-0-1.gif\n\n\nSince the correlation between the two variables \\(X_1\\) and \\(X_2\\) was low (0.1), we can the see that rightmost plot jumping a lot, i.e. to say that the values of \\(X_1\\) and \\(X_2\\) are not tighly constrained to move together.\n\nfor i in range(20):\n    plot_2d_contour_pdf_dimensions( mu, np.array([[ 1. , 0.7], [0.7,  1.]]), i)\n\n\n!convert -delay 20 -loop 0 images/0.7/*.jpg sigma-0-7.gif\n\n\nThe above GIF shows the same plot/animation for the 2d Gaussian where the correlation between the two variables is high (0.7). Thus, we can see that the two variables tend to move up and down together.\n\n\nConditional Bivariate Distribution\nAll excellent till now. Now, let us move to the case in which some variable’s values are known. We would then look to find the distribution of the other variables conditional on the value of the known variable. I borrow some text from Wikipedia on the subject.\n\\[\n\\begin{pmatrix}\nX_1 \\\\\nX_2\n\\end{pmatrix}  \\sim \\mathcal{N} \\left( \\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix} , \\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{pmatrix} \\right)\n\\]\nThe conditional expectation of \\(X_2\\) given \\(X_1\\) is: $(X_2 X_1=x_1)= x_1 $\nand the conditional variance is: \\(\\operatorname{var}(X_2 \\mid X_1 = x_1) = 1-\\rho^2\\)\nSo, the question now is: suppose we fix \\(X_1 = 1\\), what is the distribution of \\(X_2\\). Again, Gaussians are amazing - the conditional distributionon is again a Gaussian. Let us make some plots to understand better. The following plots would be showing the distribution of \\(X_2\\) with fixed \\(X_1\\)\n\ndef plot_2d_contour_pdf_dimensions_fixed_x1(sigma, random_num, x1 = 1):\n    mu = np.zeros(2)\n    fig, ax  = plt.subplots(ncols=3, figsize=(12, 4))\n\n    X = np.linspace(-3, 3, 60)\n    Y = np.linspace(-3, 3, 60)\n    X, Y = np.meshgrid(X, Y)\n\n    # Pack X and Y into a single 3-dimensional array\n    pos = np.empty(X.shape + (2,))\n    pos[:, :, 0] = X\n    pos[:, :, 1] = Y\n\n    F = multivariate_normal(mu, sigma)\n    Z = F.pdf(pos)\n    \n    rho = sigma[0, 1]\n    F_cond_x1 = multivariate_normal(rho*x1, 1-rho**2)\n    random_point_x2 = F_cond_x1.rvs(random_state=random_num)\n    sns.heatmap(sigma, ax=ax[0], annot=True)\n    ax[1].contour(X, Y, Z, cmap=cm.Greys)\n    ax[1].scatter(x1, random_point_x2, color='k',s=100)\n    ax[1].set_xlabel(r\"$X_1$\")\n    ax[1].set_ylabel(r\"$X_2$\")\n    \n    data_array = pd.Series([x1, random_point_x2], index=['X1','X2'])\n    data_array.plot(ax=ax[2], kind='line', color='k')\n    ax[2].scatter(x=0, y=x1, color='red', s=100)\n    ax[2].scatter(x=1, y=random_point_x2, color='k', s=100)\n    \n\n    plt.xticks(np.arange(len(data_array.index)), data_array.index.values)\n    ax[2].set_ylim(-3, 3)\n    format_axes(ax[0])\n    format_axes(ax[1])\n    format_axes(ax[2])\n    ax[0].set_title(\"Covariance Matrix\")\n    ax[1].set_title(\"Contour of pdf\")\n    ax[2].set_title(\"Visualising the point\")\n    plt.suptitle(f\"Random state = {random_num}\", y=1.1)\n    plt.tight_layout()\n    import os\n    if not os.path.exists(\"images/conditional/\"):\n        os.makedirs(\"images/conditional/\")\n    if not os.path.exists(f\"images/conditional/{sigma[0, 1]}\"):\n        os.makedirs(f\"images/conditional/{sigma[0, 1]}\")\n    plt.savefig(f\"images/conditional/{sigma[0, 1]}/{random_num}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n\n\nfor i in range(20):\n    plot_2d_contour_pdf_dimensions_fixed_x1(np.array([[ 1. , 0.1], [0.1,  1.]]), i)\n\n\n!convert -delay 20 -loop 0 images/conditional/0.1/*.jpg conditional-sigma-0-1.gif\n\n\nThe above animation shows the movement of \\(X_2\\) with \\(X_1=1\\). The \\(X_1=1\\) is shown in red in the righmost plot. In the middle plot, we can confirm that the movement is only in the \\(X_2\\) dimension. Further, since the correlation between \\(X_1\\) and \\(X_2\\) is weak, the righmost plot seems to wiggle or jump a lot!\n\nfor i in range(20):\n    plot_2d_contour_pdf_dimensions_fixed_x1(np.array([[ 1. , 0.7], [0.7,  1.]]), i)\n\n\n!convert -delay 20 -loop 0 images/conditional/0.7/*.jpg conditional-sigma-0-7.gif\n\n\nIn the plot above, we repeat the same p|rocedure but with a covariance matrix having a much higher correlation between \\(X_1\\) and \\(X_2\\). From the righmost plot, we can clearly see that the jumps in \\(X2\\) are far lesser. This is expected, since the two variables are correlated!\n\nVisualising the same procedure for 5 dimensional Gaussian\nWe will now repeat the same procedure we did for 2d case in 5 dimensions.\n\ncovariance_5d = np.array([[1, 0.9, 0.8, 0.6, 0.4],\n                          [0.9, 1, 0.9, 0.8, 0.6],\n                          [0.8, 0.9, 1, 0.9, 0.8],\n                          [0.6, 0.8, 0.9, 1, 0.9],\n                          [0.4, 0.6, 0.8, 0.9, 1]])\n\n\ndef plot_5d_contour_pdf_dimensions(cov, random_num):\n    fig, ax  = plt.subplots(ncols=2, figsize=(6, 3))\n\n    mu = np.zeros(5)\n    F = multivariate_normal(mu, cov)\n    random_point = F.rvs(random_state=random_num)\n    \n    sns.heatmap(cov, ax=ax[0], annot=True)\n    \n    \n    data_array = pd.Series(random_point, index=['X1','X2','X3','X4', 'X5'])\n    data_array.plot(ax=ax[1], kind='line', marker='o',color='k')\n    plt.xticks(np.arange(len(data_array.index)), data_array.index.values)\n    ax[1].set_ylim(-3, 3)\n    for i in range(2):\n        format_axes(ax[i])\n    \n    ax[0].set_title(\"Covariance Matrix\")\n    ax[-1].set_title(\"Visualising the point\")\n    plt.suptitle(f\"Random state = {random_num}\", y=1.1)\n    plt.tight_layout()\n    import os\n    if not os.path.exists(\"images/5d/\"):\n        os.makedirs(\"images/5d\")\n    \n    plt.savefig(f\"images/5d/{random_num}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n\n\nplot_5d_contour_pdf_dimensions(covariance_5d, 2)\n\n\nfor i in range(20):\n    plot_5d_contour_pdf_dimensions(covariance_5d, i)\n\n\n!convert -delay 20 -loop 0 images/5d/*.jpg 5d.gif\n\n\nFrom the visualisation above we can see that:\n\nsince X1 and X2 are highly correlated, they move up and down together\nbut, X1 and X5 have low correlation, thus, they can seem to wiggle almost independently of each other.\n\nWe are now getting somewhere. If the correlation between the variables is very high, we will get a smooth curve joining them. Right? Almost getting to the point where we can draw the introductory plot shown at the top of the post.\n\n\n\nConditional Multivariate Distribution\nOk, now let us draw the conditional distribution over this higher 5d space. We will fix the values of some of the variables and see the distribution of the others.\nBorrowing from Wikipedia\nIf \\(N\\)-dimensional \\(x\\) is partitioned as follows\n\\[\n\\mathbf{x}\n=\n\\begin{bmatrix}\n\\mathbf{x}_A \\\\\n\\mathbf{x}_B\n\\end{bmatrix}\n\\text{ with sizes }\\begin{bmatrix} q \\times 1 \\\\ (N-q) \\times 1 \\end{bmatrix}\n\\]\nand accordingly \\(μ\\) and \\(Σ\\) are partitioned as follows\n\\[\n\\boldsymbol\\mu\n=\n\\begin{bmatrix}\n\\boldsymbol\\mu_A \\\\\n\\boldsymbol\\mu_B\n\\end{bmatrix}\n\\text{ with sizes }\\begin{bmatrix} q \\times 1 \\\\ (N-q) \\times 1 \\end{bmatrix}\n\\]\n\\[\n\\boldsymbol\\Sigma\n=\n\\begin{bmatrix}\n\\boldsymbol\\Sigma_{AA} & \\boldsymbol\\Sigma_{AB} \\\\\n\\boldsymbol\\Sigma_{BA} & \\boldsymbol\\Sigma_{BB}\n\\end{bmatrix}\n\\text{ with sizes }\\begin{bmatrix} q \\times q & q \\times (N-q) \\\\ (N-q) \\times q & (N-q) \\times (N-q) \\end{bmatrix}\n\\]\nthen the distribution of \\(x_A\\) conditional on \\(x_B=b\\) is multivariate normal \\((x_A|x_B=b)\\sim \\mathcal{N}(\\bar{\\mu}, \\bar{\\Sigma})\\)\n\\[\n\\bar{\\boldsymbol\\mu}\n=\n\\boldsymbol\\mu_A + \\boldsymbol\\Sigma_{AB} \\boldsymbol\\Sigma_{BB}^{-1}\n\\left(\n\\mathbf{B} - \\boldsymbol\\mu_B\n\\right)\n\\]\nand covariance matrix\n\\[\n\\overline{\\boldsymbol\\Sigma}\n=\n\\boldsymbol\\Sigma_{AA} - \\boldsymbol\\Sigma_{AB} \\boldsymbol\\Sigma_{BB}^{-1} \\boldsymbol\\Sigma_{BA}.\n\\]\nLet us for our example take \\(X_5 = -2\\).\nWe have:\n\\(x_A = [x_1, x_2, x_3, x_4]\\) and \\(x_B = [x_5]\\)\nAssuming the covariance matrix of size 5 X 5 is referred as \\(C\\)\n\\[\n\\boldsymbol\\Sigma_{AA}\n=\n\\begin{bmatrix}\nC_{11} & C_{12} & C_{13} & C_{14}\\\\\nC_{21} & C_{22} & C_{23} & C_{24}\\\\\nC_{31} & C_{32} & C_{33} & C_{34}\\\\\nC_{41} & C_{42} & C_{43} & C_{44}\\\\\n\\end{bmatrix} \\\\\n\\]\n\\[\n\\boldsymbol\\Sigma_{AB}\n=\n\\begin{bmatrix}\nC_{15}\\\\\nC_{25}\\\\\nC_{35}\\\\\nC_{45}\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\boldsymbol\\Sigma_{BA}\n=\n\\begin{bmatrix}\nC_{51}& C_{52} & C_{53} & C_{54}\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\boldsymbol\\Sigma_{BB}\n=\n\\begin{bmatrix}\nC_{55}\\\\\n\\end{bmatrix}\n\\]\nPutting in the numbers we get:\n\nsigma_AA = covariance_5d[:4, :4]\n\n\nsigma_AA\n\narray([[1. , 0.9, 0.8, 0.6],\n       [0.9, 1. , 0.9, 0.8],\n       [0.8, 0.9, 1. , 0.9],\n       [0.6, 0.8, 0.9, 1. ]])\n\n\n\nsigma_AB = covariance_5d[:4, 4].reshape(-1, 1)\n\n\nsigma_AB\n\narray([[0.4],\n       [0.6],\n       [0.8],\n       [0.9]])\n\n\n\nsigma_BA = covariance_5d[4, :4].reshape(1, -1)\n\n\nsigma_BA\n\narray([[0.4, 0.6, 0.8, 0.9]])\n\n\n\nsigma_BB = covariance_5d[4, 4].reshape(-1, 1)\n\n\nsigma_BB\n\narray([[1.]])\n\n\nNow, calculating \\(\\bar{\\mu}\\)\n\nmu_bar = np.zeros((4, 1)) + sigma_AB@np.linalg.inv(sigma_BB)*(-2)\n\n\nmu_bar\n\narray([[-0.8],\n       [-1.2],\n       [-1.6],\n       [-1.8]])\n\n\nSince, \\(x_5\\) has highest correlation with \\(x_4\\) it makes sense for \\(x_5=-2\\) to have the mean of \\(x_4\\) to be close to -2.\nNow, calculating \\(\\bar{\\Sigma}\\)\n\nsigma_bar = sigma_AA - sigma_AB@np.linalg.inv(sigma_BB)@sigma_BA\n\n\nsigma_bar\n\narray([[0.84, 0.66, 0.48, 0.24],\n       [0.66, 0.64, 0.42, 0.26],\n       [0.48, 0.42, 0.36, 0.18],\n       [0.24, 0.26, 0.18, 0.19]])\n\n\nNow, we have the new mean and covariance matrices for \\(x_A = [x_1, x_2, x_3, x_4]\\) and \\(x_B = [x_5] = [-2]\\). Let us now draw some samples fixing \\(x_5 = -2\\)\n\ncov = sigma_bar\nmu = mu_bar.flatten()\ndef plot_5d_samples_fixed_x2(random_num):\n    fig, ax  = plt.subplots(ncols=2, figsize=(6, 3))\n    \n    \n    F = multivariate_normal(mu, cov)\n    \n    sns.heatmap(cov, ax=ax[0], annot=True)\n    random_point = F.rvs(random_state=random_num)\n    \n    \n    data_array = pd.Series(random_point, index=['X1','X2','X3','X4'])\n    data_array['X5'] = -2\n    data_array.plot(ax=ax[1], kind='line', marker='.',color='k')\n    plt.scatter([4], [-2], color='red', s=100)\n    plt.xticks(np.arange(len(data_array.index)), data_array.index.values)\n    ax[1].set_ylim(-3, 3)\n    for i in range(2):\n        format_axes(ax[i])\n    \n    ax[0].set_title(\"Covariance Matrix\")\n    ax[-1].set_title(\"Visualising the point\")\n    plt.suptitle(f\"Random state = {random_num}\", y=1.1)\n    plt.tight_layout()\n    import os\n    if not os.path.exists(\"images/5d/conditional/1\"):\n        os.makedirs(\"images/5d/conditional/1\")\n    \n    plt.savefig(f\"images/5d/conditional/1/{random_num}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n    \n\n\nfor i in range(20):\n    plot_5d_samples_fixed_x2(i)\n\n\n!convert -delay 20 -loop 0 images/5d/conditional/1/*.jpg 5d-conditional-1.gif\n\n\n\n\nLet’s increase to 20 dimensions now!\nWe can not surely write the covariance matrix for 20 dimensions. Let us use a small trick called the kernel function to create this matrix. We will come it later. For now, let us think of this function as a function which:\n\noutputs low numbers for \\(x_1\\) and \\(x_2\\) if they differ by a lot\noutputs high number for \\(x_1\\) and \\(x_2\\) if they are very close\n\n\ndef rbf_kernel(x_1, x_2, sig):\n    return np.exp((-(x_1-x_2)**2)/2*(sig**2))\n\n\nrbf_kernel(1, 1, 0.4)\n\n1.0\n\n\nSince 1=1, the above function evaluates to 1 showing that 1 is similar to 1\n\nrbf_kernel(1, 2, 0.4)\n\n0.9231163463866358\n\n\nSince 1 and 2 are close, the function above evaluates to close to 1\n\nrbf_kernel(1, 2, 1)\n\n0.6065306597126334\n\n\nOk, we use the same first two arguments 1 and 2 but change the last one to 1 from 0.4 and we see that the function evaluates to a much smaller number. Thus, we can see that increase the sig parameter leads to quicker dropoff in similarity between pair of points. Or, in other words, higher sig means that the influence of a point x_1 reduces quicker.\nLet us now create the covariance matrix of size (20, 20) using this kernel function.\n\nC = np.zeros((20, 20))\n\n\nfor i in range(20):\n    for j in range(20):\n        C[i, j] = rbf_kernel(i, j, 0.5)\n\nLet us plot the heatmap of the covariance matrix\n\nsns.heatmap(C)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f9d11ccbee0>\n\n\n\n\n\nThe above heatmap confirms that there is correlation between nearby points, but close to zero or zero correlation otherwise.\n\nLet us draw some samples from this 20 dimensional Gaussian\n\ndef plot_20d_samples(random_num):\n    fig, ax  = plt.subplots(figsize=(10, 3))\n    \n    \n    F = multivariate_normal(np.zeros(20), C)\n    random_point = F.rvs(random_state=random_num)\n    index = [f'X{i}' for i in range(1, 21)]\n    data_array = pd.Series(random_point, index=index)\n    data_array.plot(ax=ax, kind='line', marker='.',color='k')\n    plt.xticks(np.arange(len(data_array.index)), data_array.index.values)\n    \n    plt.suptitle(f\"Random state = {random_num}\", y=1.1)\n    plt.tight_layout()\n    import os\n    if not os.path.exists(\"images/20d/\"):\n        os.makedirs(\"images/20d/\")\n    \n    plt.ylim(-3, 3)\n    plt.savefig(f\"images/20d/{random_num}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n\n\nfor i in range(50):\n    plot_20d_samples(i)\n\n\n!convert -delay 20 -loop 0 images/20d/*.jpg 20d.gif\n\n\nFrom the animation above, we can see different family of functions of mean zero across these 20 points. We’re really getting close now!\n\n\nLet us now condition on a few elements\nWe will create a new ordering of these variables such that the known variables occur towards the end. This allows for easy calculations for conditioning.\n\norder = [2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 1, 5, 10]\n\n\nnew_C = np.zeros_like(C)\n\n\nold_order = range(20)\n\n\nfor i in range(20):\n    for j in range(20):\n        new_C[i, j] = C[order[i], order[j]]\n\n\nsns.heatmap(new_C, xticklabels=order, yticklabels=order, cmap='jet')\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f9d10b88d00>\n\n\n\n\n\nNow, we can condition on (x1 = 1, x2 = 3, x6 = -3, X11 = 1). We will use the same procedure we used above in the case of 5d.\n\nB = np.array([1, 3, -3, 1]).reshape(-1, 1)\nB\n\narray([[ 1],\n       [ 3],\n       [-3],\n       [ 1]])\n\n\n\nsigma_AA_20d = new_C[:-B.size, :-B.size]\nsigma_AA_20d.shape\n\n(16, 16)\n\n\n\nsigma_BB_20d = new_C[-B.size:, -B.size:]\nsigma_BB_20d.shape\n\n(4, 4)\n\n\n\nsigma_AB_20d = new_C[:-B.size, -B.size:]\nsigma_AB_20d.shape\n\n(16, 4)\n\n\n\nsigma_BA_20d = new_C[-B.size:, :-B.size]\nsigma_BA_20d.shape\n\n(4, 16)\n\n\n\nmu_bar_20d = np.zeros((20-B.size, 1)) + sigma_AB_20d@np.linalg.inv(sigma_BB_20d)@(B)\n\n\nsigma_bar_20d = sigma_AA_20d - sigma_AB_20d@np.linalg.inv(sigma_BB_20d)@sigma_BA_20d\n\n\nsns.heatmap(sigma_bar_20d, xticklabels=order[:-B.size], yticklabels=order[:-B.size], cmap='jet')\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f9d091fd0a0>\n\n\n\n\n\n\ndef plot_20d_samples_known_x(random_num):\n    fig, ax  = plt.subplots(figsize=(10, 3))\n    \n    \n    F = multivariate_normal(mu_bar_20d.flatten(), sigma_bar_20d)\n    random_point = F.rvs(random_state=random_num)\n    index = [f'X{i+1}' for i in order[:-B.size]]\n    data_array = pd.Series(random_point, index=index)\n    data_array['X1'] = 1\n    data_array['X2'] = 3\n    data_array['X6'] = -3\n    data_array['X11'] = -1\n    \n    data_array = data_array[[f'X{i+1}' for i in range(20)]]\n    data_array.plot(ax=ax, kind='line', marker='.',color='k')\n    plt.xticks(np.arange(len(data_array.index)), data_array.index.values)\n    plt.scatter([0, 1,5, 10], [1, 3, -3, -1], color='red',s=100)\n\n    plt.suptitle(f\"Random state = {random_num}\", y=1.1)\n    plt.tight_layout()\n    import os\n    if not os.path.exists(\"images/20d/conditional/\"):\n        os.makedirs(\"images/20d/conditional/\")\n    plt.grid()\n    plt.ylim(-4, 4)\n    plt.savefig(f\"images/20d/conditional/{random_num}.jpg\", bbox_inches=\"tight\")\n    plt.close()\n\n\nfor i in range(50):\n    plot_20d_samples_known_x(i)\n\n\n!convert -delay 20 -loop 0 images/20d/conditional/*.jpg 20d-conditional.gif\n\n\nFrom the plot above, we can see the known points in red and the other points wiggle to show the families of functions that we fit. Let us now draw a lot of samples and plot the mean and variance in these samples for the unknown X variables. We could have obtained the mean and variance directly using Gaussian marginalisation, but, for now let us just draw many samples.\n\nF = multivariate_normal(mu_bar_20d.flatten(), sigma_bar_20d)\ndfs = {}\nfor random_num in range(100):\n    random_point = F.rvs(random_state=random_num)\n    index = [f'X{i+1}' for i in order[:-B.size]]\n    data_array = pd.Series(random_point, index=index)\n    data_array['X1'] = 1\n    data_array['X2'] = 3\n    data_array['X6'] = -3\n    data_array['X11'] = -1\n    \n    data_array = data_array[[f'X{i+1}' for i in range(20)]]\n    dfs[random_num] = data_array\n\n\nfig, ax = plt.subplots(figsize=(10, 3))\npd.DataFrame(dfs).mean(axis=1).plot(yerr=pd.DataFrame(dfs).std(axis=1),marker='o', color='k')\nplt.xticks(np.arange(len(data_array.index)), data_array.index.values)\nplt.scatter([0, 1,5, 10], [1, 3, -3, -1], color='red',s=100)\nformat_axes(plt.gca())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f9d0fcc8d00>\n\n\n\n\n\nFrom the plot above, we can see the uncertainty (standard deviation) and the mean values for different variables. As expected, the uncertainty close to the known points (red) is low. Also, owing to the smooth nature of the covariance function we can see the means of unknown points close to known points are fairly similar.\nTo summarise: We can very clearly see that there is low variance in zones where we have the known values and high variance otherwise. The farther we go away from a known value, the more is the variance!\n\n\n\nKernels!\nWe will now take a small plunge into the world of kernels. As mentioned earlier, we will limit the discussion to generating to covariance matrix.\nWe will be redefining the function mentioned above to include two parameters l and s\n\ns is the scale of variance\nl is the influence of the point to neighbouring points\n\n\ndef sig(x1, x2, l, s):\n    return s**2*(np.exp((-1/2*(l**2))*((x1-x2)**2)))\n\n\nCov_matrix = np.zeros((100, 100))\n\n\nfig, ax = plt.subplots(ncols=4, sharex=True, sharey=True)\ns = 1\nfor ix, l in enumerate([0.001, 0.01, 0.1, 1]):\n    for i in range(100):\n        for j in range(100):\n            Cov_matrix[i, j] = sig(i, j, l, 1)\n    im = ax[ix].imshow(Cov_matrix, cmap='jet')\n    ax[ix].set_title(f\"l={l}\")\nfig.subplots_adjust(right=0.8)\ncbar_ax = fig.add_axes([0.85, 0.35, 0.05, 0.3])\nfig.colorbar(im, cax=cbar_ax)\nplt.suptitle(f\"Covariance matrix for varying l and s = {s}\")\n\nText(0.5, 0.98, 'Covariance matrix for varying l and s = 1')\n\n\n\n\n\nIn the plot above, we can the covariance matrices for fixed s=1 and varying l. It can be seen that for very low l, the correlations between far away points is also significant. At l=1, this ceases to be the case.\n\nfig, ax = plt.subplots(ncols=4, sharex=True, sharey=True, figsize=(12, 3))\nfor ix, s in enumerate([1, 10, 20, 30]):\n    for i in range(100):\n        for j in range(100):\n            Cov_matrix[i, j] = sig(i, j, 0.1, s)\n    sns.heatmap(Cov_matrix, cmap='jet', ax=ax[ix])\n    ax[ix].set_title(f\"s={s}\")\nplt.suptitle(\"Covariance matrix for varying s and l = 0.1\")\n\nText(0.5, 0.98, 'Covariance matrix for varying s and l = 0.1')\n\n\n\n\n\nOk, this is great. We can see the different scales on the colorbars with increasing s and fixing l\nNow, let us try and redo the 20 point dataset with varying kernel parameters with conditioning on some known data.\n\ndef fit_plot_gp(kernel_s, kernel_l, known_data, total_data_points, save=False):\n    \"\"\"\n    kernel_s: sigma^2 param of kernel\n    kernel_l: l (width) param of kernel\n    known_data: {pos: value}\n    total_data_points\n    \"\"\"\n    o = list(range(20))\n    for key in known_data.keys():\n        o.remove(key)\n    o.extend(list(known_data.keys()))\n    \n    C = np.zeros((total_data_points, total_data_points))\n    for i in range(total_data_points):\n        for j in range(total_data_points):\n            C[i, j] = sig(i, j, kernel_l, kernel_s)\n        \n    \n    # Making known variables shift\n    new_C = np.zeros_like(C)\n    for i in range(20):\n        for j in range(20):\n            new_C[i, j] = C[o[i], o[j]]\n    B = np.array(list(known_data.values())).reshape(-1, 1)    \n    sigma_BA_20d = new_C[-B.size:, :-B.size]\n    sigma_AB_20d = new_C[:-B.size, -B.size:]\n    sigma_BB_20d = new_C[-B.size:, -B.size:]\n    sigma_AA_20d = new_C[:-B.size, :-B.size]\n\n    mu_bar_20d = np.zeros((20-B.size, 1)) + sigma_AB_20d@np.linalg.inv(sigma_BB_20d)@(B)\n    sigma_bar_20d = sigma_AA_20d - sigma_AB_20d@np.linalg.inv(sigma_BB_20d)@sigma_BA_20d\n    F = multivariate_normal(mu_bar_20d.flatten(), sigma_bar_20d)\n    dfs = {}\n    for random_num in range(100):\n        random_point = F.rvs(random_state=random_num)\n        index = [f'X{i+1}' for i in o[:-B.size]]\n        data_array = pd.Series(random_point, index=index)\n        for k, v in known_data.items():\n            data_array[f'X{k+1}'] = v\n        \n\n        data_array = data_array[[f'X{i+1}' for i in range(20)]]\n        dfs[random_num] = data_array\n    fig, ax = plt.subplots(figsize=(10, 3))\n    mean_vector = pd.DataFrame(dfs).mean(axis=1)\n    mean_vector.plot(marker='.', color='k')\n    yerr=pd.DataFrame(dfs).std(axis=1)\n    \n    plt.fill_between(range(len(mean_vector)), mean_vector+yerr, mean_vector-yerr, color='gray',alpha=0.4)\n    plt.xticks(np.arange(len(data_array.index)), data_array.index.values)\n    plt.scatter(list(known_data.keys()), list(known_data.values()), color='gray',s=200,zorder=1)\n    format_axes(plt.gca())\n    plt.title(f\" l = {kernel_l} and s = {kernel_s}\")\n    import os\n    if save:\n        if not os.path.exists(\"images/20d/conditional-points/\"):\n            os.makedirs(\"images/20d/conditional-points/\")\n        plt.grid()\n        plt.xticks(np.arange(len(data_array.index)), np.arange(len(data_array.index)))\n        plt.ylim(-4, 4)\n        plt.title(f\"Known data: {known_data}\")\n        plt.savefig(f\"images/20d/conditional-points/{len(known_data.keys())}.jpg\", bbox_inches=\"tight\")\n        plt.close()\n        \n\n\nknown_d = {0:-2, 1:3, 9:-1, 14:-1}\n\n\nfit_plot_gp(1, 0.5, known_d, 20)\n\n\n\n\nThe above plot shows the uncertainty and the family of functions for l=0.5 and s=1.\n\nfit_plot_gp(5, 0.5, known_d, 20)\n\n\n\n\nKeeping l=0.5, the above plot shows how increasing s increases the uncertainty of estimation.\n\nfit_plot_gp(1, 1, known_d, 20)\n\n\n\n\nThe above plot shows how increasing l reduces the influence between far away points.\n\nfit_plot_gp(1, 100, known_d, 20)\n\n\n\n\nThe above plot increases l to a very large value. Seems to be just moving around the mean?\n\nnp.random.seed(0)\norder_points_added = np.random.choice(range(20), size=9, replace=False)\nk = {}\nfor i in range(9):\n    k[order_points_added[i]] = np.random.choice(range(-3, 3))\n    fit_plot_gp(1, 0.5, k, 20, True)\n\n\n!convert -delay 40 -loop 0 images/20d/conditional-points/*.jpg 20d-conditional-main.gif\n\nLet us create a small animation where we keep on adding points and see how the uncertainty and estimation changes\n\n\n\nCreating a scikit-learn like function containing fit and predict\nI’ll now bring in the formal definitions, summarise the discussion and write a function akin to scikit-learn which can accept train data to estimate for test data.\n\nFormally defining GPs\nA Gaussian process is fully specified by a mean function m(x) and covariance function K(x, x') :\n\\[\nf(x) \\sim GP (m(x),K(x, x')\n\\]\nLet us consider a case of noiseless GPs now\n\n\nNoiseless GPs\nGiven train data \\[D = {(x_i, y_i), i = 1:N}\\]\nGiven a test set \\(X_{*}\\) of size $N_* d $ containing \\(N_*\\) points in \\({\\rm I\\!R}^d\\), we want to predict function outputs \\(y_{*}\\)\nWe can write:\n\\[\n\\begin{pmatrix}\ny \\\\\ny_*\n\\end{pmatrix}  \\sim \\mathcal{N} \\left( \\begin{pmatrix}\n\\mu \\\\\n\\mu_*\n\\end{pmatrix} , \\begin{pmatrix}\nK & K_* \\\\\nK_*^T & K_{**}\n\\end{pmatrix} \\right)\n\\]\nwhere\n\\[\nK = Ker(X, X) \\in {\\rm I\\!R}^{N\\times N}\\\\\nK_* = Ker(X, X_*) \\in {\\rm I\\!R}^{N\\times N_*}\\\\\nK_{**} = Ker(X_*, X_*) \\in {\\rm I\\!R}^{N_*\\times N_*}\\\\\n\\]\nWe had previously used the kernel which we will continue to use\ndef sig(x1, x2, l, s):\n    return s**2*(np.exp((-1/2*(l**2))*((x1-x2)**2)))\nWe can then write:\n\\[\np(y_*|X_*, X, y) \\sim \\mathcal{N}(\\mu', \\Sigma') \\\\\n\\mu' = \\mu_* + K_*^TK^{-1}(x-\\mu) \\\\\n\\Sigma' = K_{**} - K_*^TK^{-1}K_*\n\\]\n\nclass NoiselessGP_inversion:\n    def __init__(self, l=0.1, s=1, prior_mean=0):\n        self.l = l\n        self.s = s     \n        self.prior_mean = prior_mean\n        \n    def prior_sample(self, x, n):\n        \"\"\"\n        Sample GP on x\n        \"\"\"\n        self.sample_k = self.create_cov_matrix(x, x, self.l, self.s)\n        for i in range(n):\n            pass\n      \n    \n    def kernel(self, a, b, l, s):\n        \"\"\"\n        Borrowed from Nando De Freita's lecture code\n        https://www.cs.ubc.ca/~nando/540-2013/lectures/gp.py\n        \"\"\"\n        sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T)\n        return s**2*np.exp(-.5 * (1/l) * sqdist)\n    \n    def fit(self, train_x, train_y):\n        self.train_x = train_x\n        self.train_y = train_y\n        self.N = len(train_x)\n        self.K = self.kernel(train_x, train_x, self.l, self.s)\n        \n                \n    def predict(self, test_x):\n        self.N_star = len(test_x)\n        self.K_star = self.kernel(self.train_x, test_x, self.l, self.s)\n        self.K_star_star = self.kernel(test_x, test_x, self.l, self.s)\n        self.posterior_mu = self.prior_mean + self.K_star.T@np.linalg.inv(self.K)@(self.train_y-self.prior_mean)\n        self.posterior_sigma = self.K_star_star - self.K_star.T@np.linalg.inv(self.K)@self.K_star\n        return self.posterior_mu, self.posterior_sigma\n\n\nclf = NoiselessGP_inversion()\n\n\ntrain_x = np.array([-4, -3, -2, -1, 1]).reshape(5,1)\ntrain_y = np.sin(train_x)\n\ntest_x = np.linspace(-5, 5, 50).reshape(-1, 1)\ntest_y = np.sin(test_x)\n\n\nplt.plot(train_x, train_y,'ko-')\n\n\n\n\n\nclf.fit(train_x, train_y)\n\n\nposterior_mu, posterior_var = clf.predict(test_x)\n\n\nplt.plot(test_x, clf.posterior_mu,'k',label='Predicted',lw=1)\nplt.plot(test_x, test_y, 'purple',label='GT',lw=2)\nplt.plot(train_x, train_y, 'ko',label='Training Data')\n\nplt.fill_between(test_x.flatten(), \n                 (clf.posterior_mu.flatten() - clf.posterior_sigma.diagonal().flatten()),\n                 (clf.posterior_mu.flatten() + clf.posterior_sigma.diagonal().flatten()),\n                 color='gray', alpha=0.3\n                )\nplt.legend()\nformat_axes(plt.gca())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f9d0f567340>\n\n\n\n\n\n\n\n\nCholesky decomposition\nWe had previously used matrix inversion to do the computation for computing the posterior mean and variance in our GP. However, the matrices involved may be poorly conditioned and thus Cholesky decomposition is often favoured.\nFrom Wikipedia, the Cholesky decomposition of a matrix \\(A\\) is given as: \\[\n\\mathbf{A} = \\mathbf{L L}^T\n\\]\nwhere \\(L\\) is a real lower triangular matrix.\nWe can thus re-write the posterior mean and covariance as:\n\\[\np(y_*|X_*, X, y) \\sim \\mathcal{N}(\\mu', \\Sigma') \\\\\nK = LL^T \\\\\n\\]\nWe are now going to use the \\ as follows: if \\(A\\omega = B\\), then \\(\\omega\\) = \\(A\\) \\ \\(B\\)\nWe now have: \\[\n\\alpha = K^{-1}(x-\\mu) \\\\\nor, \\alpha = {LL^T}^{-1}(x-\\mu) \\\\\nor, \\alpha = L^{-T}L^{-1}(x-\\mu) \\\\\nLet, L^{-1}(x-\\mu) = \\gamma\\\\\nThus, L\\gamma = x-\\mu \\\\\nThus, \\gamma = L \\setminus (x-\\mu)\\\\\\\nThus, \\alpha = L^{T} \\setminus (L \\setminus (x-\\mu))\n\\]\nIn Python, the same can be written as:\n    L = np.linalg.cholesky(K)\n    alpha = np.linalg.solve(L.T, np.linalg.solve(L, x-mu))\nThus, we can find the posterior mean as: \\[\n\\mu' = \\mu_* + K_*^T \\alpha \\\\\n\\]\nWe also know that \\[\n\\Sigma' = K_{**} - K_*^TK^{-1}K_*\n\\]\nLet us now define \\[\nv = L \\setminus K_{*}\\\\\nor, v = L^{-1}K_{*}\\\\\nThus, v^{T} = K_{*}^TL^{-T}\\\\\nThus, v^{T}v = K_{*}^TL^{-T}L^{-1}K_{*}\\\\\nThus, v^{T}v = K_*^TK^{-1}K_* = K_{**} - \\Sigma'\n\\]\n\\[\n\\Sigma' = K_{**} - v^{T}v\n\\]\nLet us know rewrite the code with Cholesky decomposition.\n\nclass NoiselessGP_Cholesky:\n    def __init__(self, l=0.1, s=1, prior_mean=0):\n        self.l = l\n        self.s = s     \n        self.prior_mean = prior_mean\n        \n    def prior_sample(self, x, n):\n        \"\"\"\n        Sample GP on x\n        \"\"\"\n        self.sample_k = self.create_cov_matrix(x, x, self.l, self.s)\n        for i in range(n):\n            pass\n      \n    \n    def kernel(self, a, b, l, s):\n        \"\"\"\n        Borrowed from Nando De Freita's lecture code\n        https://www.cs.ubc.ca/~nando/540-2013/lectures/gp.py\n        \"\"\"\n        sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T)\n        return s**2*np.exp(-.5 * (1/l) * sqdist)\n    \n    def fit(self, train_x, train_y):\n        self.train_x = train_x\n        self.train_y = train_y\n        self.N = len(train_x)\n        self.K = self.kernel(train_x, train_x, self.l, self.s)\n        self.L = np.linalg.cholesky(self.K)\n        \n                \n    def predict(self, test_x):\n        self.N_star = len(test_x)\n        self.K_star = self.kernel(self.train_x, test_x, self.l, self.s)\n        self.K_star_star = self.kernel(test_x, test_x, self.l, self.s)\n        self.alpha = np.linalg.solve(self.L.T, np.linalg.solve(self.L, self.train_y-self.prior_mean))\n        self.v = np.linalg.solve(self.L, self.K_star)\n        self.posterior_mu = self.prior_mean + self.K_star.T@self.alpha\n        self.posterior_sigma = self.K_star_star - self.v.T@self.v\n        return self.posterior_mu, self.posterior_sigma\n\n\nclf = NoiselessGP_Cholesky()\nclf.fit(train_x, train_y)\nposterior_mu_cholesky, posterior_var_cholesky = clf.predict(test_x)\n\nWe will now compare our Cholesky decomposition based decompostion with the earlier one.\n\nnp.allclose(posterior_mu_cholesky, posterior_mu)\n\nTrue\n\n\n\nnp.allclose(posterior_var_cholesky, posterior_var)\n\nTrue\n\n\nOk, all looks good till now! Let us now move on to the case for Noisy GPs.\n\n\nNoisy GPs\nPreviously, we had assumed a noiseless model, which is to say, for the observed data, we had: \\[y_i = f(x_i)\\]\nWe now make the model more flexible by saying that there can be noise in the observed data as well, thus: \\[\ny_i = f(x_i) + \\epsilon \\\\\n\\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2)\n\\]\nOne of the main difference compared to the noiseless model would be that in the noisy model, we will have some uncertainty even about the training points.\nEverything about our model remains the same, except for the change in the covariance matrix \\(K\\) for the training points, which is now given as:\n\\[K_y = \\sigma_y^2\\mathbf{I_n} + K\n\\]\nWe can now rewrite the function as follows:\n\nclass NoisyGP:\n    def __init__(self, l = 0.1, s = 1, prior_mean = 0, sigma_y = 1):\n        self.l = l\n        self.s = s     \n        self.prior_mean = prior_mean\n        self.sigma_y = sigma_y\n        \n    def prior_sample(self, x, n):\n        \"\"\"\n        Sample GP on x\n        \"\"\"\n        self.sample_k = self.create_cov_matrix(x, x, self.l, self.s)\n        for i in range(n):\n            pass\n      \n    \n    def kernel(self, a, b, l, s):\n        \"\"\"\n        Borrowed from Nando De Freita's lecture code\n        https://www.cs.ubc.ca/~nando/540-2013/lectures/gp.py\n        \"\"\"\n        sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T)\n        return s**2*np.exp(-.5 * (1/l) * sqdist)\n    \n    def fit(self, train_x, train_y):\n        self.train_x = train_x\n        self.train_y = train_y\n        self.N = len(train_x)\n        self.K = self.kernel(train_x, train_x, self.l, self.s) + self.sigma_y*np.eye(len(train_x))\n        self.L = np.linalg.cholesky(self.K)\n        \n                \n    def predict(self, test_x):\n        self.N_star = len(test_x)\n        self.K_star = self.kernel(self.train_x, test_x, self.l, self.s)\n        self.K_star_star = self.kernel(test_x, test_x, self.l, self.s)\n        self.alpha = np.linalg.solve(self.L.T, np.linalg.solve(self.L, self.train_y-self.prior_mean))\n        self.v = np.linalg.solve(self.L, self.K_star)\n        self.posterior_mu = self.prior_mean + self.K_star.T@self.alpha\n        self.posterior_sigma = self.K_star_star - self.v.T@self.v\n        return self.posterior_mu, self.posterior_sigma\n\n\nclf = NoisyGP(sigma_y=0.2)\nclf.fit(train_x, train_y)\nposterior_mu_noisy, posterior_var_noisy = clf.predict(test_x)\n\n\nplt.plot(test_x, clf.posterior_mu,'k',label='Predicted',lw=1)\nplt.plot(test_x, test_y, 'purple',label='GT',lw=2)\nplt.plot(train_x, train_y, 'ko',label='Training Data')\n\nplt.fill_between(test_x.flatten(), \n                 (clf.posterior_mu.flatten() - clf.posterior_sigma.diagonal().flatten()),\n                 (clf.posterior_mu.flatten() + clf.posterior_sigma.diagonal().flatten()),\n                 color='gray', alpha=0.3\n                )\nplt.legend()\nformat_axes(plt.gca())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f9d11584550>\n\n\n\n\n\nWe can now see that our model has some uncertainty even on the train points!"
  },
  {
    "objectID": "posts/2022-02-09-autograd-pytorch-jax.html",
    "href": "posts/2022-02-09-autograd-pytorch-jax.html",
    "title": "Autograd in JAX and PyTorch",
    "section": "",
    "text": "Creating scalar variables in PyTorch\n\nx_torch = torch.autograd.Variable(torch.tensor(1.), requires_grad=True)\ny_torch = torch.autograd.Variable(torch.tensor(1.), requires_grad=True)\n\n\n\nCreating scalar variables in JAX\n\nx_jax = jnp.array(1.)\ny_jax = jnp.array(1.)\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n\nDefining a loss on scalar inputs\n\ndef loss(x, y):\n    return x*x + y*y\n\n\n\nComputing the loss on PyTorch input\n\nl_torch  = loss(x_torch, y_torch)\nl_torch\n\ntensor(2., grad_fn=<AddBackward0>)\n\n\n\n\nComputing the loss on JAX input\n\nl_jax = loss(x_jax, y_jax)\n\n\n\nComputing the gradient on PyTorch input\n\nl_torch.backward()\nx_torch.grad, y_torch.grad\n\n(tensor(2.), tensor(2.))\n\n\n\n\nComputing the gradient on JAX input\n\ngrad_loss = grad(loss, argnums=[0, 1])\ngrad_loss(x_jax, y_jax)\n\n(DeviceArray(2., dtype=float32, weak_type=True),\n DeviceArray(2., dtype=float32, weak_type=True))\n\n\n\n\nRepeating the same procedure as above for both libraries but instead using vector function\n\ndef loss(theta):\n    return theta.T@theta\n\n\ntheta_torch = torch.autograd.Variable(torch.tensor([1., 1.]), requires_grad=True)\n\n\ntheta_torch\n\ntensor([1., 1.], requires_grad=True)\n\n\n\nl = loss(theta_torch)\nl\n\ntensor(2., grad_fn=<DotBackward0>)\n\n\n\nl.backward()\ntheta_torch.grad\n\ntensor([2., 2.])\n\n\n\ntheta_jax = jnp.array([1., 1.])\n\n\nloss(theta_jax)\n\nDeviceArray(2., dtype=float32)\n\n\n\ngrad_loss = grad(loss, argnums=[0])\n\n\ngrad_loss(theta_jax)\n\n(DeviceArray([2., 2.], dtype=float32),)"
  },
  {
    "objectID": "posts/2022-02-15-draw-graphical-models.html",
    "href": "posts/2022-02-15-draw-graphical-models.html",
    "title": "Drawing graphical models",
    "section": "",
    "text": "%cat ../pgm/coin-toss.tex\n\n\\documentclass[a4paper]{article}\n\\usepackage{caption}\n\\usepackage{subcaption}\n\\usepackage{tikz}\n\\usetikzlibrary{bayesnet}\n\\usepackage{booktabs}\n\n\n\\setlength{\\tabcolsep}{12pt}\n\\begin{document}\n    \n    \\begin{figure}[ht]\n        \\begin{center}\n            \\begin{tabular}{@{}cccc@{}}\n                \\toprule\n                \n                $x_N$ explicit & Plate Notation & Hyperparameters on $\\mu$ & Factor\\\\  \\midrule\n                &                &            &              \\\\\n                \\begin{tikzpicture}\n                    \n                    \n                    \\node[obs]                               (x1) {$x_1$};\n                    \\node[const, right=0.5cm of x1]                               (dots) {$\\cdots$};\n                    \\node[obs, right=0.5cm of dots]                               (xn) {$x_N$};\n                    \\node[latent, above=of dots] (mu) {$\\mathbf{\\mu}$};\n                    \n                    \n                    \\edge {mu} {x1,dots,xn} ; %\n                    \n                \\end{tikzpicture}&\n                \\begin{tikzpicture}\n                    \n                    \n                    \\node[obs]                               (xn) {$x_n$};\n                    \\node[latent, above=of xn] (mu) {$\\mathbf{\\mu}$};\n                    \n                    \\plate{}{(xn)}{$n = 1, \\cdots, N$};\n                    \n                    \n                    \\edge {mu} {xn} ; %\n                    \n                \\end{tikzpicture} &\n                \n                \\begin{tikzpicture}\n                    \n                    \n                    \\node[obs]                               (xn) {$x_n$};\n                    \\node[latent, above=of xn] (mu) {$\\mathbf{\\mu}$};\n                    \\node[const, right=0.5cm of mu] (beta) {$\\mathbf{\\beta}$};\n                    \\node[const, left=0.5cm of mu] (alpha) {$\\mathbf{\\alpha}$};\n                    \n                    \\plate{}{(xn)}{$n = 1, \\cdots, N$};\n                    \n                    \n                    \\edge {mu} {xn} ; %\n                    \\edge {alpha,beta} {mu} ; %\n                    \n                    \n                \\end{tikzpicture}\n            &   \n            \\begin{tikzpicture}\n                \n                \n                \\node[obs]                               (xn) {$x_n$};\n                \\node[latent, above=of xn] (mu) {$\\mathbf{\\mu}$};\n                \\factor[above=of xn] {y-f} {left:${Ber}$} {} {} ; %\n                \\node[const, above=1 of mu, xshift=0.5cm] (beta) {$\\mathbf{\\beta}$};\n                \\node[const, above=1 of mu, xshift=-0.5cm] (alpha) {$\\mathbf{\\alpha}$};\n                \\factor[above=of mu] {mu-f} {left:${Beta}$} {} {} ; %\n                \\plate{}{(xn)}{$n = 1, \\cdots, N$};\n                \n                \n                \n                \\edge {mu} {xn} ; %\n                \\edge {alpha,beta} {mu-f} ; %\n                \\edge  {mu-f}{mu} ; %\n                \n                \n            \\end{tikzpicture}\n            \n                \n            \\end{tabular}\n            \n        \\end{center}\n        \\caption{Graphical models for a repeated Bernoulli experiment.}\n    \\end{figure}\n\n    \n\\end{document}\n\n\nReferences\n\nhttps://mml-book.github.io/book/mml-book.pdf Figure 8.10"
  },
  {
    "objectID": "posts/2020-04-16-inverse-transform.html",
    "href": "posts/2020-04-16-inverse-transform.html",
    "title": "Sampling from common distributions",
    "section": "",
    "text": "PRNG\n\nUniform distribution\nI am pasting the code from vega-vis\nexport default function(seed) {\n  // Random numbers using a Linear Congruential Generator with seed value\n  // Uses glibc values from https://en.wikipedia.org/wiki/Linear_congruential_generator\n  return function() {\n    seed = (1103515245 * seed + 12345) % 2147483647;\n    return seed / 2147483647;\n  };\n}\n\ndef random_gen(seed, num):\n    out = np.zeros(num)\n    out[0] =  (1103515245 * seed + 12345) % 2147483647\n    for i in range(1, num):\n        out[i] = (1103515245 * out[i-1] + 12345) % 2147483647\n    return out/2147483647\n        \n        \n\n\nplt.hist(random_gen(0, 5000), density=True, alpha=0.5, label='Our implementation');\n\n\n\n\n\nplt.hist(random_gen(0, 5000), density=True, alpha=0.5, label='Our implementation');\nplt.hist(np.random.random(5000), density=True, alpha=0.4, label='Numpy implementation');\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f51d3cbd290>\n\n\n\n\n\n\n\n\nInverse transform sampling\n\nExponential distribution\nBorrowing from Wikipedia.\nThe probability density function (pdf) of an exponential distribution is \\[\nf(x ; \\lambda)=\\left\\{\\begin{array}{ll}\n\\lambda e^{-\\lambda x} & x \\geq 0 \\\\\n0 & x \\leq 0\n\\end{array}\\right.\n\\]\nThe exponential distribution is sometimes parametrized in terms of the scale parameter \\(\\beta=1 / \\lambda:\\) \\[\nf(x ; \\beta)=\\left\\{\\begin{array}{ll}\n\\frac{1}{\\beta} e^{-x / \\beta} & x \\geq 0 \\\\\n0 & x<0\n\\end{array}\\right.\n\\]\nThe cumulative distribution function is given by \\[\nF(x ; \\lambda)=\\left\\{\\begin{array}{ll}\n1-e^{-\\lambda x} & x \\geq 0 \\\\\n0 & x<0\n\\end{array}\\right.\n\\]\n\nfrom scipy.stats import expon\nrvs = [expon(scale=s) for s in [1/1., 1/2., 1/3.]]\n\n\nx = np.arange(0, 10, 0.1)\nfor i, lambda_val in enumerate([1, 2, 3]):\n    plt.plot(x, rvs[i].pdf(x), lw=2, label=r'$\\lambda=%s$' %lambda_val)\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f51da6d4e10>\n\n\n\n\n\nFor the purposes of this notebook, I will be looking only at the standard exponential or set the scale to 1.\nLet us now view the CDF of the standard exponential.\n\nfig, ax = plt.subplots(nrows=2, sharex=True)\nax[0].plot(x, expon().pdf(x), lw=2)\nax[0].set_title(\"PDF\")\nax[1].set_title(\"CDF\")\nax[1].plot(x, expon().cdf(x), lw=2,)\n\n\n\n\n\nr = expon.rvs(size=1000)\n\n\nplt.hist(r, normed=True, bins=100)\nplt.plot(x, expon().pdf(x), lw=2)\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: MatplotlibDeprecationWarning: \nThe 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n\n\n\nInverse of the CDF of exponential\nThe cumulative distribution function is given by \\[\nF(x ; \\lambda)=\\left\\{\\begin{array}{ll}\n1-e^{-\\lambda x} & x \\geq 0 \\\\\n0 & x<0\n\\end{array}\\right.\n\\]\nLet us consider only \\(x \\geq 0\\).\nLet \\(u = F^{-1}\\) be the inverse of the CDF of \\(F\\).\n\\[\nu = 1-e^{-\\lambda x} \\\\\n1- u = e^{-\\lambda x} \\\\\n\\log(1-u) = -\\lambda x\\\\\nx = -\\frac{\\log(1-u)}{\\lambda}\n\\]\n\ndef inverse_transform(lambda_val, num_samples):\n    u = np.random.random(num_samples)\n    x = -np.log(1-u)/lambda_val\n    return x\n\n\nplt.hist(inverse_transform(1, 5000), bins=100, normed=True,label='Generated using our function');\nplt.plot(x, expon().pdf(x), lw=4, label='Generated using scipy')\nplt.legend()\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: MatplotlibDeprecationWarning: \nThe 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n<matplotlib.legend.Legend at 0x7f51d5f95410>\n\n\n\n\n\n\n\nDrawing samples from Laplace distribution\nA random variable has a Laplace \\((\\mu, b)\\) distribution if its probability density function is \\[\nf(x | \\mu, b)=\\frac{1}{2 b} \\exp \\left(-\\frac{|x-\\mu|}{b}\\right)\n\\]\n\\[F^{-1}(u)=\\mu-b \\operatorname{sgn}(u-0.5) \\ln (1-2|u-0.5|)\\]\n\ndef inverse_transform_laplace(b, mu, num_samples):\n    u = np.random.random(num_samples)\n    x = mu-b*np.sign(u-0.5)*np.log(1-2*np.abs(u-0.5))\n    return x\n\n\nfrom scipy.stats import laplace\nplt.hist(inverse_transform_laplace(1, 0, 5000),bins=100, density=True, label='Generated using \\nour function');\nx_n = np.linspace(-10, 10, 100)\nplt.plot(x_n, laplace().pdf(x_n), lw=4, label='Generated from\\n scipy')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f51d4a99750>\n\n\n\n\n\n\n\n\nBox-Muller transform"
  },
  {
    "objectID": "posts/2021-06-18-audio-filters.html",
    "href": "posts/2021-06-18-audio-filters.html",
    "title": "Audio Filtering on the command line and Python",
    "section": "",
    "text": "In this post I will look into some filters for audio processing in ffmpeg, sox, and Python. I have recorded a small 6 second audio clip where for the first couple of seconds I was not speaking, but background noise is present.\nI had recorded the audio on my Apple device and it was default recorded in .m4a format. I convert it to the wav format. I use ffmpeg for the same. In addition, I am using two flags: -v quiet to reduce the amount of information printed on the console. Second, I am using -y to overwrite an existing file with the same name.\n\n!ffmpeg -i Test.m4a Test.wav -v quiet -y\n\n\nfrom IPython.display import Audio\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nAudio(\"Test.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n!ffmpeg -i Test.wav -lavfi showspectrumpic=s=720x540:color='magma' ../images/input-spectogram.png -y -v quiet\n\n\nAs can be seen in the above image, I am speaking somewhere close to 3.70 seconds onwards. However, the audio is pretty noisy before this even though I am not speaking. This is due to the background noise coming in from the fans and the air conditioning system.\n\n!sox Test.wav -n spectrogram -o ../images/sox-sg.png\n\n\n\n!sox Test.wav -n rate 32k spectrogram  -o ../images/sox-sg-trimmed.png \n\n\nI’ll now get some attributes of the post that are required for processing, such as the recording rate. ## Getting attributes of the recorded file\n\n!ffmpeg -i Test.wav\n\nffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers\n  built with Apple clang version 12.0.5 (clang-1205.0.22.9)\n  configuration: --prefix=/usr/local/Cellar/ffmpeg/4.4_2 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags= --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libbluray --enable-libdav1d --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-avresample --enable-videotoolbox\n  libavutil      56. 70.100 / 56. 70.100\n  libavcodec     58.134.100 / 58.134.100\n  libavformat    58. 76.100 / 58. 76.100\n  libavdevice    58. 13.100 / 58. 13.100\n  libavfilter     7.110.100 /  7.110.100\n  libavresample   4.  0.  0 /  4.  0.  0\n  libswscale      5.  9.100 /  5.  9.100\n  libswresample   3.  9.100 /  3.  9.100\n  libpostproc    55.  9.100 / 55.  9.100\nGuessed Channel Layout for Input Stream #0.0 : mono\nInput #0, wav, from 'Test.wav':\n  Metadata:\n    title           : Test\n    encoder         : Lavf58.76.100\n  Duration: 00:00:06.63, bitrate: 768 kb/s\n  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, mono, s16, 768 kb/s\nAt least one output file must be specified\n\n\nAs can be seen from the cell above, the recording rate is 48 kHz. We will need this when we do some processing in Python.\nBuilding a noise profile from first 3 second\n\n!ffmpeg -i Test.wav -ss 0 -to 3.5 -c copy Noise-Test.wav -v quiet -y\n\n\nAudio('Noise-Test.wav')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n!sox Noise-Test.wav -n rate 32k spectrogram  -o ../images/sox-noise.png \n\n\n\n!sox Noise-Test.wav -n noiseprof noise.prof\n\n\n!sox Noise-Test.wav Noise-Test-cleaned.wav noisered noise.prof 0.21\n\n\nAudio(\"Noise-Test-cleaned.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n!sox Test.wav Test-cleaned-05.wav noisered noise.prof 0.05\n\n\n!sox Test.wav Test-cleaned-18.wav noisered noise.prof 0.18\n!sox Test.wav Test-cleaned-21.wav noisered noise.prof 0.21\n\n\nAudio(\"Test-cleaned-05.wav\")\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nAudio(\"Test-cleaned-18.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nAudio(\"Test-cleaned-21.wav\")\n\n\n!sox Test-cleaned-21.wav -n rate 32k spectrogram  -o ../images/sox-cleaned-21.png \n\n\n\n!sox Test-cleaned-05.wav -n rate 32k spectrogram  -o ../images/sox-cleaned-05.png \n\n\n\nAudio(\"Test-audacity.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n!sox Test-audacity.wav -n rate 32k spectrogram  -o ../images/sg-audacity.png \n\n\n\n!ffmpeg -i Test.wav -filter:a \"highpass=f=300\" high-passed.wav -y -v quiet\n\n\n\nAudio(\"high-passed.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n!sox high-passed.wav -n rate 32k spectrogram  -o ../images/highpass.png \n\n\n\nAudio(\"test-imovie.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n!sox test-imovie.wav -n remix 1 rate 32k spectrogram  -o ../images/imovie.png \n\n\nimport mediapy\n\norig  = mediapy.read_image('../images/sox-sg-trimmed.png')\naudacity = mediapy.read_image('../images/sg-audacity.png')\nsox_21 = mediapy.read_image('../images/sox-cleaned-21.png')\nsox_05 = mediapy.read_image('../images/sox-cleaned-05.png')\nhigh_pass_300 = mediapy.read_image('../images/highpass.png')\nimovie = mediapy.read_image('../images/imovie.png')\n\n\n\n\nmediapy.show_images({'Original':orig, \n                     'Audacity':audacity,\n                     'Sox:0.21':sox_21,\n                    'Sox:0.05':sox_05,\n                    'HPF:300': high_pass_300,\n                    'imovie':imovie},\n                    cmap='magma', columns=4, height=200 )\n\n\n\n      \n      Original\n      \n      Audacity\n      \n      Sox:0.21\n      \n      Sox:0.05\n      \n      HPF:300\n      \n      imovie\n\n\n\n!sox test-audacity.wav output.dat\n\n\nimport pandas as pd\ndf = pd.read_csv(\"output.dat\", skiprows=2, index_col=0, names=['values'],delim_whitespace=True)\ndf = df.astype('float64')\n\n\ndf.plot()\n\n<AxesSubplot:>"
  },
  {
    "objectID": "posts/2022-02-17-pyro-linreg.html",
    "href": "posts/2022-02-17-pyro-linreg.html",
    "title": "Linear Regression using Pyro",
    "section": "",
    "text": "Creating dataset\n\nx = torch.linspace(-5, 5, 200)\ntrue_y = 3*x + 4 \nobserved_y = true_y+ 2*torch.randn(200)\n\nplt.scatter(x, observed_y, s = 30, alpha=0.5)\nplt.plot(x, true_y, color = 'k')\n\n\nsns.despine()\n\n\n\n\n\n\nMLE\n\nimport pyro\nimport pyro.distributions as dist\nimport pyro.distributions.constraints as constraints\n\npyro.clear_param_store()\n\n\ndef mle_model(x, y=None):\n    theta_0 = pyro.param(\"theta_0\", torch.randn(1))\n    theta_1 = pyro.param(\"theta_1\", torch.randn(1))\n    y_hat_mean = theta_0 + theta_1*x\n\n    with pyro.plate(\"data\", len(x)):\n        return pyro.sample(\"obs\", dist.Normal(y_hat_mean, 1), obs=y)\n\n#pyro.render_model(mle_model, model_args=(x, observed_y))\n\n\nm = mle_model(x)\npyro.param(\"theta_0\").item(), pyro.param(\"theta_1\").item()\n\n(-0.100727379322052, 1.5172470808029175)\n\n\n\nfor i in range(5):\n    plt.scatter(x, mle_model(x).detach(), s = 5, alpha = 0.4)\n    plt.plot(x, )\n\n\n\n\n\ndef guide(x, y):\n    # register the two variational parameters with Pyro.\n    pyro.sample(\"theta_0\", dist.Normal(0., 1.))\n    pyro.sample(\"theta_1\", dist.Normal(0., 1.))\n\n\npyro.render_model(guide, model_args=(x, observed_y))\n\n\n\n\n\nfrom pyro.optim import Adam\nfrom pyro.infer import SVI, Trace_ELBO\nadam_params = {\"lr\": 0.005, \"betas\": (0.90, 0.999)}\noptimizer = Adam(adam_params)\n\n# setup the inference algorithm\nsvi = SVI(mle_model, guide, optimizer, loss=Trace_ELBO())\n\nn_steps = 5000\n# do gradient steps\nfor step in range(n_steps):\n    svi.step(x, y)\n    if step%110==0:\n        print(pyro.param(\"theta_0\").item(), pyro.param(\"theta_1\").item())\n\n/Users/nipun/miniforge3/lib/python3.9/site-packages/pyro/util.py:288: UserWarning: Found non-auxiliary vars in guide but not model, consider marking these infer={'is_auxiliary': True}:\n{'theta_1', 'theta_0'}\n  warnings.warn(\n\n\nValueError: Error while computing log_prob at site 'obs':\nValue is not broadcastable with batch_shape+event_shape: torch.Size([100]) vs torch.Size([200]).\nTrace Shapes:      \n Param Sites:      \n      theta_0     1\n      theta_1     1\nSample Sites:      \n     obs dist 200 |\n        value 100 |\n\n\n\nx = torch.linspace(-5, 5, 100)\npredicted_y = pyro.param(\"theta_1\").item()*x + pyro.param(\"theta_0\").item()\n#observed_y = true_y+ 2*torch.randn(100)\n\nplt.scatter(x, observed_y, s = 30, alpha=0.5)\nplt.plot(x, predicted_y, color = 'k')\nplt.plot(x, true_y, color = 'k')\n\n\n\nsns.despine()\n\n\n\n\n\ndata_dim = 2\nlatent_dim = 1\nnum_datapoints = 100\nz = dist.Normal(\n    loc=torch.zeros([latent_dim, num_datapoints]),\n    scale=torch.ones([latent_dim, num_datapoints]),)\n\nw = dist.Normal(\n    loc=torch.zeros([data_dim, latent_dim]),\n    scale=5.0 * torch.ones([data_dim, latent_dim]),\n)\n\n\nw_sample= w.sample()\nz_sample = z.sample()\n\n\nx = dist.Normal(loc = w_sample@z_sample, scale=2)\nx_sample = x.sample([100])\nplt.scatter(x_sample[:, 0], x_sample[:, 1], alpha=0.2, s=30)\n\n<matplotlib.collections.PathCollection at 0x135cdf700>\n\n\n\n\n\n\n\nGenerative model for PPCA in Pyro\n\nimport pyro.distributions as dist\nimport pyro.distributions.constraints as constraints\nimport pyro\npyro.clear_param_store()\n\ndef ppca_model(data, latent_dim):\n    N, data_dim = data.shape\n    W = pyro.param(\"W\", torch.zeros((data_dim, latent_dim)))\n    #print(W.shape, data_dim, (data_dim, latent_dim))\n    \n    for i in range(N):\n        z_vec = pyro.sample(\"z_{}\".format(i), dist.Normal(loc = torch.zeros(latent_dim), scale = 1.))\n        #print(W.shape, z.shape, W@z)\n        pyro.sample(fr\"\\$x_{i}\\$\", dist.Normal(W@z_vec, 2.), obs=data[i])\n\n\npyro.render_model(ppca_model, model_args=(torch.randn(150, 3), 1))\n\n\n\n\n\npyro.clear_param_store()\n\ndef ppca_model2(data, latent_dim):\n    N, data_dim = data.shape\n    W = pyro.param(\"W\", torch.zeros((data_dim, latent_dim)))\n    #print(W.shape, data_dim, (data_dim, latent_dim))\n    z_vec = pyro.sample(\"z\", dist.Normal(loc = torch.zeros([latent_dim, N]), scale = 1.))\n    \n    print(W.shape, z_vec.shape, (W@z_vec).t().shape, data.shape)\n    return pyro.sample(\"obs\", (W@z_vec).t(), obs=data)\n    \npyro.render_model(ppca_model2, model_args=(torch.randn(150, 3), 1))\n\ntorch.Size([3, 1]) torch.Size([1, 150]) torch.Size([150, 3]) torch.Size([150, 3])\n\n\nAttributeError: 'ProvenanceTensor' object has no attribute 'log_prob'\n\n\n\ndist.Normal(loc = torch.tensor([0.]), scale = 1.).sample()\n\ntensor([-1.2529])\n\n\n\npyro.clear_param_store()\n\nD = 2\nd = 1\n\ndata = torch.zeros(100, D)\n\ndef ppca(data):\n    A = pyro.param(\"A\", torch.zeros((D, d)))\n    mu = pyro.param(\"mu\", torch.zeros(D))\n\n    for i in pyro.plate(\"data\", len(data)):\n        z = pyro.sample(\"latent_{}\".format(i), dist.Normal(torch.zeros(d), 1.0).to_event(1))\n        pyro.sample(\"observed_{}\".format(i), dist.Normal(A @ z + mu, 1.0).to_event(1), obs=data[i])\n\npyro.render_model(ppca, model_kwargs={'data':data})\n\n\n\n\n\nppca(data)\n\n\ndata.shape\n\ntorch.Size([100, 2])\n\n\n\nN = 1000\nx = np.random.normal(loc = 5, scale = 1., size = N)\n\n\no = {}\nfor i in range(N-2):\n    o[i] = x[:i+2].std()\n\n\npd.Series(o).plot()\nplt.axhline(y=1)\nsns.despine()\n\n\n\n\n\nx2 = np.array(list(o.values()))\n\n\no2 = {}\nfor i in range(N-3):\n    o2[i] = x2[:i+2].std()\n\n\npd.Series(o2).plot()\n\n<AxesSubplot:>"
  },
  {
    "objectID": "posts/2021-06-12-setup-mac.html",
    "href": "posts/2021-06-12-setup-mac.html",
    "title": "My Mac Setup",
    "section": "",
    "text": "Here is a screenshot.\n\nI will now discuss how I setup a new Mac. I use homebrew. It makes it very easy to maintain all the packages up to date.\n\n\nxcode-select --install\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nInstall git\nbrew install git\nInstall Git-Credential-Manager\nbrew tap microsoft/git\nbrew install --cask git-credential-manager-core\nInstall powerline fonts\ngit clone https://github.com/powerline/fonts\ncd fonts\n./install.sh\nUse powerline fonts in shell\nGo to Terminal -> Preferences Change font to powerline I am using Roboto Mono Light Powerline 18pt\nUse Argonaut theme for terminal\ngit clone https://github.com/pwaleczek/Argonaut-theme\n\ncd os-x-terminal\n\nopen Argonaut.terminal\n\n(set this theme as default)\nOhMyZSH customization\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting\n\ngit clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions\nIn ~/.zshrc set plugins=(git zsh-autosuggestions zsh-syntax-highlighting) and ZSH_THEME=\"agnoster\"\nInstall Zoom\nbrew install --cask zoom\nInstall Firefox\nbrew install --cask firefox\nInstall VSCode\nbrew install --cask visual-studio-code\nInstall OBSStudio\nbrew install --cask obs\nVLC\nbrew install --cask vlc\nwget\nbrew install wget\nDownload miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh sh Miniconda3-latest-MacOSX-x86_64.sh\nAnaconda path setup in zsh\nsource ~/miniconda3/bin/activate conda init zsh\nInstall mamba (faster than conda) conda install mamba -n base -c conda-forge\nDownload oh-my-zsh sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\nVSCode configure Python Cmd + Shift + p -> Select Python Interpreter -> Point to the miniconda one \nInstall MacTex (slow!)\nbrew install --cask mactex\nInstalling TexStudio brew install --cask texstudio\nFFMPeg brew install ffmpeg\nImagemagick brew install imagemagick\nGhostscript brew install ghostscript\nInstall pandoc\nbrew install pandoc\n\n\n\nbrew leaves > brew.txt\nThe content of brew.txt is:\nboost\ncmake\nffmpeg\nfish\ngit\ngraphviz\nilmbase\nimagemagick\npandoc\nr\nrtmpdump\nswig\nvim\nwget\nbrew list --cask > casks.txt\nThe content of casks.txt is:\nanydesk\narduino\naudacity\nfirefox\ngoogle-chrome\ninkscape\nkeycastr\nmactex\nnotion\nobs\npdf-expert\npycharm\nrstudio\nsimplenote\ntexstudio\nvisual-studio-code\nvlc\nzoom\n\n\n\nmamba install numpy scipy matplotlib pandas jupyter ipython seaborn rich -c conda-forge\nPeriodically updating all packages mamba update --all -c conda-forge"
  },
  {
    "objectID": "posts/2022-01-26-tfp-distributions.html",
    "href": "posts/2022-01-26-tfp-distributions.html",
    "title": "Testing out some distributions in Tensorflow Probability",
    "section": "",
    "text": "Univariate normal\n\nuv_normal = tfd.Normal(loc=0., scale=1.)\n\n\nuv_normal\n\n<tfp.distributions.Normal 'Normal' batch_shape=[] event_shape=[] dtype=float32>\n\n\n\nsamples = uv_normal.sample(1000)\n\n\nsns.histplot(samples.numpy())\nsns.despine()\n\n\n\n\n\nsns.displot(samples.numpy(), kind='kde')\n\n<seaborn.axisgrid.FacetGrid at 0x7f94f31ecc70>\n\n\n\n\n\n\nuv_normal_dict_mean = {x: tfd.Normal(loc=x, scale=1.) for x in [-2, -1, 0, 1, 2]}\n\n\nuv_normal_dict_mean_samples = pd.DataFrame({x:uv_normal_dict_mean[x].sample(10000).numpy() \n                                            for x in uv_normal_dict_mean})\n\n\nsns.displot(uv_normal_dict_mean_samples, kind='kde', fill=True)\n\n<seaborn.axisgrid.FacetGrid at 0x7f94f6d43580>\n\n\n\n\n\n\nuv_normal_dict_var = {x: tfd.Normal(loc=0, scale=x) for x in [1, 2, 5, 10]}\nuv_normal_dict_var_samples = pd.DataFrame({x:uv_normal_dict_var[x].sample(10000).numpy() \n                                            for x in uv_normal_dict_var})\n\n\nsns.displot(uv_normal_dict_var_samples, kind='kde', fill=True)\n\n<seaborn.axisgrid.FacetGrid at 0x7f94f3643f40>\n\n\n\n\n\n\n\nUsing batches\n\nvar_dfs = pd.DataFrame(\n    tfd.Normal(loc=[0., 0., 0., 0.],\n               scale=[1., 2., 5., 10.]).sample(10000).numpy())\nvar_dfs.columns = [1, 2, 5, 10]\nsns.displot(var_dfs, kind='kde', fill=True)\n\n<seaborn.axisgrid.FacetGrid at 0x7f94db7bfeb0>\n\n\n\n\n\n\ntfd.Normal(loc=[0., 0., 0., 0.],\n               scale=[1., 2., 5., 10.])\n\n<tfp.distributions.Normal 'Normal' batch_shape=[4] event_shape=[] dtype=float32>\n\n\n\nsamples = uv_normal.sample(10000)\nsns.displot(samples.numpy(), kind='kde')\nplt.axvline(0.5, color='k', linestyle='--')\npdf_05 = uv_normal.prob(0.5).numpy()\nlog_pdf_05 = uv_normal.log_prob(0.5).numpy()\n\n\nplt.title(\"Density at x = 0.5 is {:.2f}\\n Logprob at x = 0.5 is {:.2f}\".format(pdf_05, log_pdf_05))\n\nText(0.5, 1.0, 'Density at x = 0.5 is 0.35\\n Logprob at x = 0.5 is -1.04')\n\n\n\n\n\n\n\nLearning parameters\nLet us generate some normally distributed data and see if we can learn the mean.\n\ntrain_data = uv_normal.sample(10000)\n\n\nuv_normal.loc, uv_normal.scale\n\n(<tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)\n\n\nLet us create a new TFP trainable distribution where we wish to learn the mean.\n\nto_train = tfd.Normal(loc = tf.Variable(-1., name='loc'), scale = 1.)\n\n\nto_train\n\n<tfp.distributions.Normal 'Normal' batch_shape=[] event_shape=[] dtype=float32>\n\n\n\nto_train.trainable_variables\n\n(<tf.Variable 'loc:0' shape=() dtype=float32, numpy=-1.0>,)\n\n\n\ntf.reduce_mean(train_data), tf.math.reduce_variance(train_data)\n\n(<tf.Tensor: shape=(), dtype=float32, numpy=-0.024403999>,\n <tf.Tensor: shape=(), dtype=float32, numpy=0.9995617>)\n\n\n\ndef nll(train):\n    return -tf.reduce_mean(to_train.log_prob(train))\n\n\nnll(train_data)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=1.8946133>\n\n\n\ndef get_loss_and_grads(train):\n    with tf.GradientTape() as tape:\n        tape.watch(to_train.trainable_variables)\n        loss = nll(train)\n    grads = tape.gradient(loss, to_train.trainable_variables)\n    return loss, grads\n\n\nget_loss_and_grads(train_data)\n\n(<tf.Tensor: shape=(), dtype=float32, numpy=1.8946133>,\n (<tf.Tensor: shape=(), dtype=float32, numpy=-0.97559595>,))\n\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n\noptimizer\n\n<keras.optimizer_v2.adam.Adam at 0x7f94c97ae490>\n\n\n\niterations = 500\nlosses = np.empty(iterations)\nvals = np.empty(iterations)\nfor i in range(iterations):\n    loss, grads = get_loss_and_grads(train_data)\n    losses[i] = loss\n    vals[i] = to_train.trainable_variables[0].numpy()\n    optimizer.apply_gradients(zip(grads, to_train.trainable_variables))\n    if i%50 == 0:\n        print(i, loss.numpy())\n\n0 1.8946133\n50 1.5505791\n100 1.4401271\n150 1.4205703\n200 1.4187955\n250 1.4187206\n300 1.4187194\n350 1.4187193\n400 1.4187193\n450 1.4187194\n\n\n\nplt.plot(losses)\nsns.despine()\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\nplt.plot(vals)\nsns.despine()\nplt.xlabel(\"Iterations\")\nplt.ylabel(r\"Value of $\\hat{\\mu}$\")\n\nText(0, 0.5, 'Value of $\\\\hat{\\\\mu}$')\n\n\n\n\n\n\nto_train_mean_var = tfd.Normal(loc = tf.Variable(-1., name='loc'), scale = tf.Variable(10., name='scale'))\n\ndef nll(train):\n    return -tf.reduce_mean(to_train_mean_var.log_prob(train))\n\ndef get_loss_and_grads(train):\n    with tf.GradientTape() as tape:\n        tape.watch(to_train_mean_var.trainable_variables)\n        loss = nll(train)\n    grads = tape.gradient(loss, to_train_mean_var.trainable_variables)\n    return loss, grads\n\nto_train_mean_var.trainable_variables\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\niterations = 1000\nlosses = np.empty(iterations)\nvals_scale = np.empty(iterations)\nvals_means = np.empty(iterations)\nfor i in range(iterations):\n    loss, grads = get_loss_and_grads(train_data)\n    losses[i] = loss\n    vals_means[i] = to_train_mean_var.trainable_variables[0].numpy()\n    vals_scale[i] = to_train_mean_var.trainable_variables[1].numpy()\n\n\n    optimizer.apply_gradients(zip(grads, to_train_mean_var.trainable_variables))\n    if i%50 == 0:\n        print(i, loss.numpy())\n\n0 3.2312806\n50 3.1768403\n100 3.1204312\n150 3.0602157\n200 2.9945102\n250 2.9219644\n300 2.8410006\n350 2.749461\n400 2.6442661\n450 2.5208094\n500 2.3718355\n550 2.1852348\n600 1.9403238\n650 1.6161448\n700 1.4188237\n750 1.4187355\n800 1.4187193\n850 1.4187193\n900 1.4187193\n950 1.4187193\n\n\n\nplt.plot(losses)\nsns.despine()\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\n\nText(0, 0.5, 'Loss')\n\n\n\n\n\n\ndf = pd.DataFrame({\"Mean\":vals_means, \"Scale\":vals_scale}, index=range(iterations))\ndf.index.name = 'Iteration'\n\n\ndf.plot(alpha=1)\nsns.despine()\nplt.axhline(0, linestyle='--', lw = 4, label = 'True mean', alpha=0.5, color='purple')\nplt.axhline(1, linestyle='--', lw = 4, label = 'True scale', alpha=0.5, color='red')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f94cbee0c10>\n\n\n\n\n\n\n\nMultivariate Normal\n\nmv_normal = tfd.MultivariateNormalFullCovariance(loc=[0, 0], covariance_matrix=[[1, 0.5], [0.5, 2]])\n\n\nmv_data = pd.DataFrame(mv_normal.sample(10000).numpy())\nmv_data.columns = [r'$x_1$', r'$x_2$']\n\n\nmv_normal.prob([0, 0])\n\n<tf.Tensor: shape=(), dtype=float32, numpy=0.120309845>\n\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n\n\ndef make_pdf_2d_gaussian(mu, sigma):\n    N = 60\n    X = np.linspace(-3, 3, N)\n    Y = np.linspace(-3, 4, N)\n    X, Y = np.meshgrid(X, Y)\n\n    # Pack X and Y into a single 3-dimensional array\n    pos = np.empty(X.shape + (2,))\n    pos[:, :, 0] = X\n    pos[:, :, 1] = Y\n\n    F = tfd.MultivariateNormalFullCovariance(loc=mu, covariance_matrix=sigma)\n    Z = F.prob(pos)\n\n    plt.contourf(X, Y, Z, cmap=cm.Purples)\n    sns.despine() \n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\")\n    plt.gca().set_aspect('equal')\n    plt.title(f'$\\mu$ = {mu}\\n $\\Sigma$ = {np.array(sigma)}')\n\n\nmake_pdf_2d_gaussian([0, 0,], [[1, 0.5,], [0.5, 1]])\n\n\n\n\n\nmake_pdf_2d_gaussian([0, 0,], [[3, 0.,], [0., 1]])\n\n\n\n\n\nsns.jointplot(data=mv_data,\n              x=r'$x_1$',y=r'$x_2$',\n              alpha=0.1)\n\n<seaborn.axisgrid.JointGrid at 0x7f94d0ce9e50>\n\n\n\n\n\n\nmv_data\n\n\n\n\n\n  \n    \n      \n      $x_1$\n      $x_2$\n    \n  \n  \n    \n      0\n      2.155621\n      -0.343866\n    \n    \n      1\n      -0.731184\n      0.378393\n    \n    \n      2\n      0.832593\n      -0.459740\n    \n    \n      3\n      -0.701200\n      -0.249675\n    \n    \n      4\n      -0.430790\n      -1.694002\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      9995\n      -0.165910\n      -0.171243\n    \n    \n      9996\n      0.208389\n      -1.698432\n    \n    \n      9997\n      -0.030418\n      0.353905\n    \n    \n      9998\n      1.342328\n      1.127457\n    \n    \n      9999\n      -0.145741\n      0.830713\n    \n  \n\n10000 rows × 2 columns"
  },
  {
    "objectID": "posts/2017-12-18-recommend-keras.html",
    "href": "posts/2017-12-18-recommend-keras.html",
    "title": "Recommender Systems in Keras",
    "section": "",
    "text": "Specifically, in this post, I’ll talk about:\n\nMatrix Factorisation in Keras\nAdding non-negativitiy constraints to solve non-negative matrix factorisation (NNMF)\nUsing neural networks for recommendations\n\nI’ll be using the Movielens-100k dataset for illustration. There are 943 users and 1682 movies. In total there are a 100k ratings in the dataset. It should be noted that the max. total number of rating for the <users, movies> would be 943*1682, which means that we have about 7% of the total ratings! All rating are on a scale of 1-5.\n\nTask\nGiven this set of ratings, can we recommend the next set of movies to a user? This would translate to: for every user, estimating the ratings for all the movies that (s)he hasn’t watched and maybe recommend the top-k movies by the esimtated ratings!\n\n\nPeak into the dataset\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\n\ndataset = pd.read_csv(\"/Users/nipun/Downloads/ml-100k/u.data\",sep='\\t',names=\"user_id,item_id,rating,timestamp\".split(\",\"))\n\n\ndataset.head()\n\n\n\n\n\n  \n    \n      \n      user_id\n      item_id\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n\n\nSo, each record (row) shows the rating for a user, item (movie) pair. It should be noted that I use item and movie interchangeably in this post.\n\nlen(dataset.user_id.unique()), len(dataset.item_id.unique())\n\n(943, 1682)\n\n\nWe assign a unique number between (0, #users) to each user and do the same for movies.\n\ndataset.user_id = dataset.user_id.astype('category').cat.codes.values\ndataset.item_id = dataset.item_id.astype('category').cat.codes.values\n\n\ndataset.head()\n\n\n\n\n\n  \n    \n      \n      user_id\n      item_id\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      195\n      241\n      3\n      881250949\n    \n    \n      1\n      185\n      301\n      3\n      891717742\n    \n    \n      2\n      21\n      376\n      1\n      878887116\n    \n    \n      3\n      243\n      50\n      2\n      880606923\n    \n    \n      4\n      165\n      345\n      1\n      886397596\n    \n  \n\n\n\n\n\n\nTrain test split\nWe’ll now split our dataset of 100k ratings into train (containing 80k ratings) and test (containing 20k ratings). Given the train set, we’d like to accurately estimate the ratings in the test set.\n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(dataset, test_size=0.2)\n\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      user_id\n      item_id\n      rating\n      timestamp\n    \n  \n  \n    \n      90092\n      832\n      12\n      2\n      875036139\n    \n    \n      50879\n      94\n      132\n      3\n      888954341\n    \n    \n      67994\n      436\n      12\n      4\n      880141129\n    \n    \n      49769\n      710\n      344\n      4\n      884485683\n    \n    \n      11032\n      121\n      736\n      4\n      879270874\n    \n  \n\n\n\n\n\ntest.head()\n\n\n\n\n\n  \n    \n      \n      user_id\n      item_id\n      rating\n      timestamp\n    \n  \n  \n    \n      89284\n      907\n      493\n      3\n      879723046\n    \n    \n      60499\n      550\n      25\n      4\n      892785056\n    \n    \n      11090\n      373\n      222\n      5\n      880394520\n    \n    \n      36096\n      199\n      140\n      4\n      884129346\n    \n    \n      21633\n      71\n      317\n      5\n      880037702\n    \n  \n\n\n\n\n\n\nMatrix factorisation\nOne popular recommender systems approach is called Matrix Factorisation. It works on the principle that we can learn a low-dimensional representation (embedding) of user and movie. For example, for each movie, we can have how much action it has, how long it is, and so on. For each user, we can encode how much they like action, or how much they like long movies, etc. Thus, we can combine the user and the movie embeddings to estimate the ratings on unseen movies. This approach can also be viewed as: given a matrix (A [M X N]) containing users and movies, we want to estimate low dimensional matrices (W [M X k] and H [M X k]), such that: \\(A \\approx W.H^T\\)\n\n\nMatrix factorisation in Keras\nWe’ll now write some code to solve the recommendation problem by matrix factorisation in Keras. We’re trying to learn two low-dimensional embeddings of users and items.\n\nimport keras\nfrom IPython.display import SVG\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import model_to_dot\nn_users, n_movies = len(dataset.user_id.unique()), len(dataset.item_id.unique())\nn_latent_factors = 3\n\nUsing TensorFlow backend.\n\n\nThe key thing is to learn an embedding for movies and users, and then combine them using the dot product! For estimating the rating, for each user, movie pair of interest, we’d take the dot product of the respective user and item embedding. As an example, if we have 2 dimensions in our user and item embedding, which say correspond to [how much user likes action, how much user likes long movies], and the item embedding is [how much action is in the movie, how long is the movie]. Then, we can predict for a user u, and movie m as how much u likes action \\(\\times\\) how much action is there in m \\(+\\) how much u likes long movies \\(\\times\\) how long is m.\nOur model would optimise the emebedding such that we minimise the mean squared error on the ratings from the train set.\n\nmovie_input = keras.layers.Input(shape=[1],name='Item')\nmovie_embedding = keras.layers.Embedding(n_movies + 1, n_latent_factors, name='Movie-Embedding')(movie_input)\nmovie_vec = keras.layers.Flatten(name='FlattenMovies')(movie_embedding)\n\nuser_input = keras.layers.Input(shape=[1],name='User')\nuser_vec = keras.layers.Flatten(name='FlattenUsers')(keras.layers.Embedding(n_users + 1, n_latent_factors,name='User-Embedding')(user_input))\n\nprod = keras.layers.merge([movie_vec, user_vec], mode='dot',name='DotProduct')\nmodel = keras.Model([user_input, movie_input], prod)\nmodel.compile('adam', 'mean_squared_error')\n\nHere’s a visualisation of our model for a better understanding.\n\nSVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))\n\n\n\n\nWe can see that in the Merge layer, we take the dot product of the user and the item embeddings to obtain the rating.\nWe can also summarise our model as follows:\n\nmodel.summary()\n\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nItem (InputLayer)               (None, 1)            0                                            \n__________________________________________________________________________________________________\nUser (InputLayer)               (None, 1)            0                                            \n__________________________________________________________________________________________________\nMovie-Embedding (Embedding)     (None, 1, 3)         5049        Item[0][0]                       \n__________________________________________________________________________________________________\nUser-Embedding (Embedding)      (None, 1, 3)         2832        User[0][0]                       \n__________________________________________________________________________________________________\nFlattenMovies (Flatten)         (None, 3)            0           Movie-Embedding[0][0]            \n__________________________________________________________________________________________________\nFlattenUsers (Flatten)          (None, 3)            0           User-Embedding[0][0]             \n__________________________________________________________________________________________________\nDotProduct (Merge)              (None, 1)            0           FlattenMovies[0][0]              \n                                                                 FlattenUsers[0][0]               \n==================================================================================================\nTotal params: 7,881\nTrainable params: 7,881\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\nSo, we have 7881 parameters to learn! Let’s train our model now!\n\nhistory = model.fit([train.user_id, train.item_id], train.rating, epochs=100, verbose=0)\n\n\nTrain error v/s epoch number\nBefore we test how well our model does in the test setting, we can visualise the train loss with epoch number.\n\npd.Series(history.history['loss']).plot(logy=True)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Train Error\")\n\n<matplotlib.text.Text at 0x1155a07b8>\n\n\n\n\n\n\n\nPrediction error\nLet’s now see how our model does! I’ll do a small post-processing step to round off our prediction to the nearest integer. This is usually not done, and thus just a whimsical step, since the training ratings are all integers! There are better ways to encode this intger requirement (one-hot encoding!), but we won’t discuss them in this post.\n\ny_hat = np.round(model.predict([test.user_id, test.item_id]),0)\ny_true = test.rating\n\n\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_true, y_hat)\n\n0.6915\n\n\nNot bad! We’re able to get a \\(MAE\\) of 0.69! I’m sure with a bit of parameter/hyper-parameter optimisation, we may be able to improve the results. However, I won’t talk about these optimisations in this post.\n\n\nExtracting the learnt embeddings\nWe can extract the learnt movie and item embeddings as follows:\n\nmovie_embedding_learnt = model.get_layer(name='Movie-Embedding').get_weights()[0]\npd.DataFrame(movie_embedding_learnt).describe()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      count\n      1683.000000\n      1683.000000\n      1683.000000\n    \n    \n      mean\n      -0.935420\n      0.857862\n      0.954169\n    \n    \n      std\n      0.517458\n      0.447439\n      0.458095\n    \n    \n      min\n      -2.524487\n      -0.459752\n      -0.989537\n    \n    \n      25%\n      -1.323431\n      0.546364\n      0.642444\n    \n    \n      50%\n      -0.949188\n      0.851243\n      0.993619\n    \n    \n      75%\n      -0.550862\n      1.159588\n      1.283555\n    \n    \n      max\n      0.500618\n      2.140607\n      2.683658\n    \n  \n\n\n\n\n\nuser_embedding_learnt = model.get_layer(name='User-Embedding').get_weights()[0]\npd.DataFrame(user_embedding_learnt).describe()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      count\n      944.000000\n      944.000000\n      944.000000\n    \n    \n      mean\n      -1.126231\n      1.171609\n      1.109131\n    \n    \n      std\n      0.517478\n      0.409016\n      0.548384\n    \n    \n      min\n      -2.883226\n      -0.500010\n      -0.415373\n    \n    \n      25%\n      -1.458197\n      0.903574\n      0.735729\n    \n    \n      50%\n      -1.159480\n      1.199517\n      1.084089\n    \n    \n      75%\n      -0.836746\n      1.456610\n      1.468611\n    \n    \n      max\n      0.899436\n      2.605330\n      2.826109\n    \n  \n\n\n\n\nWe can see that both the user and the item embeddings have negative elements. There are some applications which require that the learnt embeddings be non-negative. This approach is also called non-negative matrix factorisation, which we’ll workout now.\n\n\n\nNon-negative Matrix factorisation (NNMF) in Keras\nThe code for NNMF remains exactly the same as the code for matrix factorisation. The only change is that we add non-negativity constraints on the learnt embeddings. This is done as follows:\n\nfrom keras.constraints import non_neg\nmovie_input = keras.layers.Input(shape=[1],name='Item')\nmovie_embedding = keras.layers.Embedding(n_movies + 1, n_latent_factors, name='NonNegMovie-Embedding', embeddings_constraint=non_neg())(movie_input)\nmovie_vec = keras.layers.Flatten(name='FlattenMovies')(movie_embedding)\n\nuser_input = keras.layers.Input(shape=[1],name='User')\nuser_vec = keras.layers.Flatten(name='FlattenUsers')(keras.layers.Embedding(n_users + 1, n_latent_factors,name='NonNegUser-Embedding',embeddings_constraint=non_neg())(user_input))\n\nprod = keras.layers.merge([movie_vec, user_vec], mode='dot',name='DotProduct')\nmodel = keras.Model([user_input, movie_input], prod)\nmodel.compile('adam', 'mean_squared_error')\n\nWe now verify if we are indeed able to learn non-negative embeddings. I’ll not compare the performance of NNMF on the test set, in the interest of space.\n\nhistory_nonneg = model.fit([train.user_id, train.item_id], train.rating, epochs=10, verbose=0)\n\n\nmovie_embedding_learnt = model.get_layer(name='NonNegMovie-Embedding').get_weights()[0]\npd.DataFrame(movie_embedding_learnt).describe()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      count\n      1683.000000\n      1683.000000\n      1683.000000\n    \n    \n      mean\n      0.838450\n      0.840330\n      0.838066\n    \n    \n      std\n      0.301618\n      0.301529\n      0.301040\n    \n    \n      min\n      -0.000000\n      -0.000000\n      -0.000000\n    \n    \n      25%\n      0.657749\n      0.663951\n      0.656453\n    \n    \n      50%\n      0.901495\n      0.904192\n      0.895934\n    \n    \n      75%\n      1.072706\n      1.073591\n      1.072926\n    \n    \n      max\n      1.365719\n      1.379006\n      1.373672\n    \n  \n\n\n\n\nLooks good!\n\n\nNeural networks for recommendation\nWe’ll now create a simple neural network for recommendation, or for estimating rating! This model is very similar to the earlier matrix factorisation models, but differs in the following ways:\n\nInstead of taking a dot product of the user and the item embedding, we concatenate them and use them as features for our neural network. Thus, we are not constrained to the dot product way of combining the embeddings, and can learn complex non-linear relationships.\nDue to #1, we can now have a different dimension of user and item embeddings. This can be useful if one dimension is larger than the other.\n\n\nn_latent_factors_user = 5\nn_latent_factors_movie = 8\n\nmovie_input = keras.layers.Input(shape=[1],name='Item')\nmovie_embedding = keras.layers.Embedding(n_movies + 1, n_latent_factors_movie, name='Movie-Embedding')(movie_input)\nmovie_vec = keras.layers.Flatten(name='FlattenMovies')(movie_embedding)\nmovie_vec = keras.layers.Dropout(0.2)(movie_vec)\n\n\nuser_input = keras.layers.Input(shape=[1],name='User')\nuser_vec = keras.layers.Flatten(name='FlattenUsers')(keras.layers.Embedding(n_users + 1, n_latent_factors_user,name='User-Embedding')(user_input))\nuser_vec = keras.layers.Dropout(0.2)(user_vec)\n\n\nconcat = keras.layers.merge([movie_vec, user_vec], mode='concat',name='Concat')\nconcat_dropout = keras.layers.Dropout(0.2)(concat)\ndense = keras.layers.Dense(200,name='FullyConnected')(concat)\ndropout_1 = keras.layers.Dropout(0.2,name='Dropout')(dense)\ndense_2 = keras.layers.Dense(100,name='FullyConnected-1')(concat)\ndropout_2 = keras.layers.Dropout(0.2,name='Dropout')(dense_2)\ndense_3 = keras.layers.Dense(50,name='FullyConnected-2')(dense_2)\ndropout_3 = keras.layers.Dropout(0.2,name='Dropout')(dense_3)\ndense_4 = keras.layers.Dense(20,name='FullyConnected-3', activation='relu')(dense_3)\n\n\nresult = keras.layers.Dense(1, activation='relu',name='Activation')(dense_4)\nadam = Adam(lr=0.005)\nmodel = keras.Model([user_input, movie_input], result)\nmodel.compile(optimizer=adam,loss= 'mean_absolute_error')\n\nLet’s now see how our model looks like:\n\nSVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))\n\n\n\n\nIt should be noted that we use a different number of embeddings for user (3) and items (5)! These combine to form a vector of length (5+3 = 8), which is then fed into the neural network. We also add a dropout layer to prevent overfitting!\n\nmodel.summary()\n\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nItem (InputLayer)               (None, 1)            0                                            \n__________________________________________________________________________________________________\nUser (InputLayer)               (None, 1)            0                                            \n__________________________________________________________________________________________________\nMovie-Embedding (Embedding)     (None, 1, 8)         13464       Item[0][0]                       \n__________________________________________________________________________________________________\nUser-Embedding (Embedding)      (None, 1, 5)         4720        User[0][0]                       \n__________________________________________________________________________________________________\nFlattenMovies (Flatten)         (None, 8)            0           Movie-Embedding[0][0]            \n__________________________________________________________________________________________________\nFlattenUsers (Flatten)          (None, 5)            0           User-Embedding[0][0]             \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 8)            0           FlattenMovies[0][0]              \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 5)            0           FlattenUsers[0][0]               \n__________________________________________________________________________________________________\nConcat (Merge)                  (None, 13)           0           dropout_1[0][0]                  \n                                                                 dropout_2[0][0]                  \n__________________________________________________________________________________________________\nFullyConnected-1 (Dense)        (None, 100)          1400        Concat[0][0]                     \n__________________________________________________________________________________________________\nFullyConnected-2 (Dense)        (None, 50)           5050        FullyConnected-1[0][0]           \n__________________________________________________________________________________________________\nFullyConnected-3 (Dense)        (None, 20)           1020        FullyConnected-2[0][0]           \n__________________________________________________________________________________________________\nActivation (Dense)              (None, 1)            21          FullyConnected-3[0][0]           \n==================================================================================================\nTotal params: 25,675\nTrainable params: 25,675\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\nWe can see that the number of parameters is more than what we had in the Matrix Factorisation case. Let’s see how this model works. I’ll run it for more epochs given that we have more parameters.\n\nhistory = model.fit([train.user_id, train.item_id], train.rating, epochs=250, verbose=0)\n\n\nPrediction performance of Neural Network based recommender system\n\ny_hat_2 = np.round(model.predict([test.user_id, test.item_id]),0)\nprint(mean_absolute_error(y_true, y_hat_2))\n\nprint(mean_absolute_error(y_true, model.predict([test.user_id, test.item_id])))\n\n\n0.6957\n0.708807692927\n\n\nPretty similar to the result we got using matrix factorisation. Maybe, we need to tweak around a lot more with the neural network to get better results?\nThanks for reading. This post has been a good learning experience for me. Hope you enjoyed too!"
  },
  {
    "objectID": "posts/2017-08-02-fifty-ggplot-python-1.html",
    "href": "posts/2017-08-02-fifty-ggplot-python-1.html",
    "title": "Top 50 ggplot2 Visualizations in Python - Part 1",
    "section": "",
    "text": "Here’s how the end result should look like.\n\nHow the final plot should look like\n\n\n\nAttributes of above plot\n\nX-Y scatter for area vs population\nColor by state\nMarker-size by population\n\nI’ll first use Pandas to create the plot. Pandas plotting capabilites are almost the first thing I use to create plots. Next, I’ll show how to use Seaborn to reduce some complexity. Lastly, I’ll use Altair, ggplot and Plotnine to show how it focuses on getting directly to the point, i.e. expressing the 3 required attributes!\n\n\nTLDR: Declarative visualisatio) is super useful!\n\n\nOriginal R code\n# install.packages(\"ggplot2\")\n# load package and data\noptions(scipen=999)  # turn-off scientific notation like 1e+48\nlibrary(ggplot2)\ntheme_set(theme_bw())  # pre-set the bw theme.\ndata(\"midwest\", package = \"ggplot2\")\n# midwest <- read.csv(\"http://goo.gl/G1K41K\")  # bkup data source\n\n# Scatterplot\ngg <- ggplot(midwest, aes(x=area, y=poptotal)) + \n  geom_point(aes(col=state, size=popdensity)) + \n  geom_smooth(method=\"loess\", se=F) + \n  xlim(c(0, 0.1)) + \n  ylim(c(0, 500000)) + \n  labs(subtitle=\"Area Vs Population\", \n       y=\"Population\", \n       x=\"Area\", \n       title=\"Scatterplot\", \n       caption = \"Source: midwest\")\n\nplot(gg)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n\nColor scheme (borrowed from Randy Olson’s website)\n\n# Tableau 20 Colors\ntableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),  \n             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),  \n             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),  \n             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),  \n             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]\n             \n\n# Rescale to values between 0 and 1 \nfor i in range(len(tableau20)):  \n    r, g, b = tableau20[i]  \n    tableau20[i] = (r / 255., g / 255., b / 255.)\n\n\n\nGetting the data\n\nmidwest= pd.read_csv(\"http://goo.gl/G1K41K\") \n# Filtering\nmidwest= midwest[midwest.poptotal<50000]\n\n\nmidwest.head().loc[:, ['area'] ]\n\n\n\n\n  \n    \n      \n      area\n    \n  \n  \n    \n      1\n      0.014\n    \n    \n      2\n      0.022\n    \n    \n      3\n      0.017\n    \n    \n      4\n      0.018\n    \n    \n      5\n      0.050\n    \n  \n\n\n\n\n\n\nDefault Pandas scatter plot with marker size by population density\n\nmidwest.plot(kind='scatter', x='area', y='poptotal', ylim=((0, 50000)), xlim=((0., 0.1)), s=midwest['popdensity']*0.1)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x10e5714d0>\n\n\n\n\n\nIf we just use the default Pandas scatter, we won’t get the colour by state. For that we wil group the dataframe by states and then scatter plot each group individually.\n\n\nComplete Pandas’ solution (hand-wavy at times!)\n\nfig, ax = plt.subplots()\ngroups = midwest.groupby('state')\ncolors = tableau20[::2]\n\n# Plotting each group \nfor i, (name, group) in enumerate(groups):\n    group.plot(kind='scatter', x='area', y='poptotal', ylim=((0, 50000)), xlim=((0., 0.1)),\n               s=10+group['popdensity']*0.1, # hand-wavy :(\n               label=name, ax=ax, color=colors[i])\n\n# Legend for State colours\nlgd = ax.legend(numpoints=1, loc=1, borderpad=1, \n            frameon=True, framealpha=0.9, title=\"state\")\nfor handle in lgd.legendHandles:\n    handle.set_sizes([100.0])\n\n# Make a legend for popdensity. Hand-wavy. Error prone!\npws = (pd.cut(midwest['popdensity'], bins=4, retbins=True)[1]).round(0)\nfor pw in pws:\n    plt.scatter([], [], s=(pw**2)/2e4, c=\"k\",label=str(pw))\n\nh, l = plt.gca().get_legend_handles_labels()\nplt.legend(h[5:], l[5:], labelspacing=1.2, title=\"popdensity\", borderpad=1, \n            frameon=True, framealpha=0.9, loc=4, numpoints=1)\n\nplt.gca().add_artist(lgd)\n\n<matplotlib.legend.Legend at 0x110a3e790>\n\n\n\n\n\n\n\nUsing Seaborn\nThe solution using Seaborn is slightly less complicated as we won’t need to write the code for plotting different states on different colours. However, the legend jugglery for markersize would still be required!\n\nsizes = [10, 40, 70, 100] \nmarker_size = pd.cut(midwest['popdensity'], range(0, 2500, 500), labels=sizes) \nsns.lmplot('area', 'poptotal', data=midwest, hue='state', fit_reg=False, scatter_kws={'s':marker_size})\nplt.ylim((0, 50000))\n\n(0, 50000)\n\n\n\n\n\n\n\nAltair (could not get simpler!)\n\nfrom altair import Chart\n\nchart = Chart(midwest)\nchart.mark_circle().encode(\n    x='area',\n    y='poptotal',\n    color='state',\n    size='popdensity',\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot\n\nfrom ggplot import *\n\nggplot(aes(x='area', y='poptotal', color='state', size='popdensity'), data=midwest) +\\\n    geom_point() +\\\n    theme_bw() +\\\n    xlab(\"Area\") +\\\n    ylab(\"Population\") +\\\n    ggtitle(\"Area vs Population\")\n\n\n\n\n<ggplot: (295628405)>\n\n\nIt was great fun (and frustration) trying to make this plot. Still some bits like LOESS are not included in the visualisation I made. The best thing about this exercise was discovering Altair! Declarative visualisation looks so natural. Way to go declarative visualisation!"
  },
  {
    "objectID": "posts/2017-12-29-neural-collaborative-filtering.html",
    "href": "posts/2017-12-29-neural-collaborative-filtering.html",
    "title": "Neural Networks for Collaborative Filtering",
    "section": "",
    "text": "In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation — collaborative filtering — on the basis of implicit feedback.\n\nI’d recently written a blog post on using Keras (deep learning library) for implementing traditional matrix factorization based collaborative filtering. So, I thought to get my hands dirty with building a prototype for the paper mentioned above. The authors have already provided their code on Github, which should serve as a reference for the paper and not my post, whose purpose is merely educational!\nHere’s how the proposed network architecture looks in the paper:\n\nThere are a few terms that we need to understand:\n\nUser (u) and Item (i) are used to create embeddings (low-dimensional) for user and item\nGeneralized Matrix Factorisation (GMF) combines the two embeddings using the dot product. This is our regular matrix factorisation.\nMulti-layer perceptron can also create embeddings for user and items. However, instead of taking a dot product of these to obtain the rating, we can concatenate them to create a feature vector which can be passed on to the further layers.\nNeural MF can then combine the predictions from MLP and GMF to obtain the following prediction.\n\nAs done in my previous post, I’ll use the MovieLens-100k dataset for illustration. Please refer to my previous post for more details.\n\nPeak into the dataset\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\n\ndataset = pd.read_csv(\"/Users/nipun/Downloads/ml-100k/u.data\",sep='\\t',names=\"user_id,item_id,rating,timestamp\".split(\",\"))\n\n\ndataset.head()\n\n\n\n\n\n  \n    \n      \n      user_id\n      item_id\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      196\n      242\n      3\n      881250949\n    \n    \n      1\n      186\n      302\n      3\n      891717742\n    \n    \n      2\n      22\n      377\n      1\n      878887116\n    \n    \n      3\n      244\n      51\n      2\n      880606923\n    \n    \n      4\n      166\n      346\n      1\n      886397596\n    \n  \n\n\n\n\nSo, each record (row) shows the rating for a user, item (movie) pair. It should be noted that I use item and movie interchangeably in this post.\n\nlen(dataset.user_id.unique()), len(dataset.item_id.unique())\n\n(943, 1682)\n\n\nWe assign a unique number between (0, #users) to each user and do the same for movies.\n\ndataset.user_id = dataset.user_id.astype('category').cat.codes.values\ndataset.item_id = dataset.item_id.astype('category').cat.codes.values\n\n\ndataset.head()\n\n\n\n\n\n  \n    \n      \n      user_id\n      item_id\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      195\n      241\n      3\n      881250949\n    \n    \n      1\n      185\n      301\n      3\n      891717742\n    \n    \n      2\n      21\n      376\n      1\n      878887116\n    \n    \n      3\n      243\n      50\n      2\n      880606923\n    \n    \n      4\n      165\n      345\n      1\n      886397596\n    \n  \n\n\n\n\n\n\nTrain test split\nWe’ll now split our dataset of 100k ratings into train (containing 80k ratings) and test (containing 20k ratings). Given the train set, we’d like to accurately estimate the ratings in the test set.\n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(dataset, test_size=0.2)\n\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      user_id\n      item_id\n      rating\n      timestamp\n    \n  \n  \n    \n      13185\n      71\n      95\n      5\n      880037203\n    \n    \n      23391\n      144\n      509\n      4\n      882181859\n    \n    \n      90744\n      895\n      50\n      2\n      887159951\n    \n    \n      3043\n      255\n      279\n      5\n      882151167\n    \n    \n      8932\n      55\n      94\n      4\n      892683274\n    \n  \n\n\n\n\n\ntest.head()\ny_true = test.rating\n\n\n\nCreating the model\n\nimport keras\nn_latent_factors_user = 8\nn_latent_factors_movie = 10\nn_latent_factors_mf = 3\nn_users, n_movies = len(dataset.user_id.unique()), len(dataset.item_id.unique())\n\nmovie_input = keras.layers.Input(shape=[1],name='Item')\nmovie_embedding_mlp = keras.layers.Embedding(n_movies + 1, n_latent_factors_movie, name='Movie-Embedding-MLP')(movie_input)\nmovie_vec_mlp = keras.layers.Flatten(name='FlattenMovies-MLP')(movie_embedding_mlp)\nmovie_vec_mlp = keras.layers.Dropout(0.2)(movie_vec_mlp)\n\nmovie_embedding_mf = keras.layers.Embedding(n_movies + 1, n_latent_factors_mf, name='Movie-Embedding-MF')(movie_input)\nmovie_vec_mf = keras.layers.Flatten(name='FlattenMovies-MF')(movie_embedding_mf)\nmovie_vec_mf = keras.layers.Dropout(0.2)(movie_vec_mf)\n\n\nuser_input = keras.layers.Input(shape=[1],name='User')\nuser_vec_mlp = keras.layers.Flatten(name='FlattenUsers-MLP')(keras.layers.Embedding(n_users + 1, n_latent_factors_user,name='User-Embedding-MLP')(user_input))\nuser_vec_mlp = keras.layers.Dropout(0.2)(user_vec_mlp)\n\nuser_vec_mf = keras.layers.Flatten(name='FlattenUsers-MF')(keras.layers.Embedding(n_users + 1, n_latent_factors_mf,name='User-Embedding-MF')(user_input))\nuser_vec_mf = keras.layers.Dropout(0.2)(user_vec_mf)\n\n\nconcat = keras.layers.merge([movie_vec_mlp, user_vec_mlp], mode='concat',name='Concat')\nconcat_dropout = keras.layers.Dropout(0.2)(concat)\ndense = keras.layers.Dense(200,name='FullyConnected')(concat_dropout)\ndense_batch = keras.layers.BatchNormalization(name='Batch')(dense)\ndropout_1 = keras.layers.Dropout(0.2,name='Dropout-1')(dense_batch)\ndense_2 = keras.layers.Dense(100,name='FullyConnected-1')(dropout_1)\ndense_batch_2 = keras.layers.BatchNormalization(name='Batch-2')(dense_2)\n\n\ndropout_2 = keras.layers.Dropout(0.2,name='Dropout-2')(dense_batch_2)\ndense_3 = keras.layers.Dense(50,name='FullyConnected-2')(dropout_2)\ndense_4 = keras.layers.Dense(20,name='FullyConnected-3', activation='relu')(dense_3)\n\npred_mf = keras.layers.merge([movie_vec_mf, user_vec_mf], mode='dot',name='Dot')\n\n\npred_mlp = keras.layers.Dense(1, activation='relu',name='Activation')(dense_4)\n\ncombine_mlp_mf = keras.layers.merge([pred_mf, pred_mlp], mode='concat',name='Concat-MF-MLP')\nresult_combine = keras.layers.Dense(100,name='Combine-MF-MLP')(combine_mlp_mf)\ndeep_combine = keras.layers.Dense(100,name='FullyConnected-4')(result_combine)\n\n\nresult = keras.layers.Dense(1,name='Prediction')(deep_combine)\n\n\nmodel = keras.Model([user_input, movie_input], result)\nopt = keras.optimizers.Adam(lr =0.01)\nmodel.compile(optimizer='adam',loss= 'mean_absolute_error')\n\nUsing TensorFlow backend.\n\n\nLet’s now see how our model looks like:\n\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nSVG(model_to_dot(model,  show_shapes=False, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))\n\n\n\n\nSo, it wasn’t very complicated to set up. Courtesy Keras, we can do even more complex stuff!\n\nmodel.summary()\n\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nItem (InputLayer)               (None, 1)            0                                            \n__________________________________________________________________________________________________\nUser (InputLayer)               (None, 1)            0                                            \n__________________________________________________________________________________________________\nMovie-Embedding-MLP (Embedding) (None, 1, 10)        16830       Item[0][0]                       \n__________________________________________________________________________________________________\nUser-Embedding-MLP (Embedding)  (None, 1, 8)         7552        User[0][0]                       \n__________________________________________________________________________________________________\nFlattenMovies-MLP (Flatten)     (None, 10)           0           Movie-Embedding-MLP[0][0]        \n__________________________________________________________________________________________________\nFlattenUsers-MLP (Flatten)      (None, 8)            0           User-Embedding-MLP[0][0]         \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 10)           0           FlattenMovies-MLP[0][0]          \n__________________________________________________________________________________________________\ndropout_3 (Dropout)             (None, 8)            0           FlattenUsers-MLP[0][0]           \n__________________________________________________________________________________________________\nConcat (Merge)                  (None, 18)           0           dropout_1[0][0]                  \n                                                                 dropout_3[0][0]                  \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 18)           0           Concat[0][0]                     \n__________________________________________________________________________________________________\nFullyConnected (Dense)          (None, 200)          3800        dropout_5[0][0]                  \n__________________________________________________________________________________________________\nBatch (BatchNormalization)      (None, 200)          800         FullyConnected[0][0]             \n__________________________________________________________________________________________________\nDropout-1 (Dropout)             (None, 200)          0           Batch[0][0]                      \n__________________________________________________________________________________________________\nFullyConnected-1 (Dense)        (None, 100)          20100       Dropout-1[0][0]                  \n__________________________________________________________________________________________________\nBatch-2 (BatchNormalization)    (None, 100)          400         FullyConnected-1[0][0]           \n__________________________________________________________________________________________________\nMovie-Embedding-MF (Embedding)  (None, 1, 3)         5049        Item[0][0]                       \n__________________________________________________________________________________________________\nUser-Embedding-MF (Embedding)   (None, 1, 3)         2832        User[0][0]                       \n__________________________________________________________________________________________________\nDropout-2 (Dropout)             (None, 100)          0           Batch-2[0][0]                    \n__________________________________________________________________________________________________\nFlattenMovies-MF (Flatten)      (None, 3)            0           Movie-Embedding-MF[0][0]         \n__________________________________________________________________________________________________\nFlattenUsers-MF (Flatten)       (None, 3)            0           User-Embedding-MF[0][0]          \n__________________________________________________________________________________________________\nFullyConnected-2 (Dense)        (None, 50)           5050        Dropout-2[0][0]                  \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 3)            0           FlattenMovies-MF[0][0]           \n__________________________________________________________________________________________________\ndropout_4 (Dropout)             (None, 3)            0           FlattenUsers-MF[0][0]            \n__________________________________________________________________________________________________\nFullyConnected-3 (Dense)        (None, 20)           1020        FullyConnected-2[0][0]           \n__________________________________________________________________________________________________\nDot (Merge)                     (None, 1)            0           dropout_2[0][0]                  \n                                                                 dropout_4[0][0]                  \n__________________________________________________________________________________________________\nActivation (Dense)              (None, 1)            21          FullyConnected-3[0][0]           \n__________________________________________________________________________________________________\nConcat-MF-MLP (Merge)           (None, 2)            0           Dot[0][0]                        \n                                                                 Activation[0][0]                 \n__________________________________________________________________________________________________\nCombine-MF-MLP (Dense)          (None, 100)          300         Concat-MF-MLP[0][0]              \n__________________________________________________________________________________________________\nFullyConnected-4 (Dense)        (None, 100)          10100       Combine-MF-MLP[0][0]             \n__________________________________________________________________________________________________\nPrediction (Dense)              (None, 1)            101         FullyConnected-4[0][0]           \n==================================================================================================\nTotal params: 73,955\nTrainable params: 73,355\nNon-trainable params: 600\n__________________________________________________________________________________________________\n\n\nWe can see that the number of parameters is more than what we had in the Matrix Factorisation case. Let’s see how this model works. I’ll run it for more epochs given that we have more parameters.\n\nhistory = model.fit([train.user_id, train.item_id], train.rating, epochs=25, verbose=0, validation_split=0.1)\n\n\n\nPrediction performance of Neural Network based recommender system\n\nfrom sklearn.metrics import mean_absolute_error\ny_hat_2 = np.round(model.predict([test.user_id, test.item_id]),0)\nprint(mean_absolute_error(y_true, y_hat_2))\n\nprint(mean_absolute_error(y_true, model.predict([test.user_id, test.item_id])))\n\n\n0.716\n0.737380115688\n\n\nPretty similar to the result we got using matrix factorisation. This isn’t very optimised, and I am sure doing so, we can make this approach perform much better than GMF!\nThanks for reading. This post has been a good learning experience for me. Hope you enjoyed too!"
  },
  {
    "objectID": "posts/2013-05-01-aggregation-timeseries.html",
    "href": "posts/2013-05-01-aggregation-timeseries.html",
    "title": "Aggregation in Timeseries using Pandas",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\n\ndf = pd.read_csv(\"weather.csv\", index_col=0, parse_dates=True).tz_localize(\"UTC\").tz_convert(\"US/Central\")\n\n\ndf.head()\n\n\n\n\n  \n    \n      \n      humidity\n      temperature\n    \n  \n  \n    \n      2015-01-01 00:00:00-06:00\n      0.73\n      38.74\n    \n    \n      2015-01-01 01:00:00-06:00\n      0.74\n      38.56\n    \n    \n      2015-01-01 02:00:00-06:00\n      0.75\n      38.56\n    \n    \n      2015-01-01 03:00:00-06:00\n      0.79\n      37.97\n    \n    \n      2015-01-01 04:00:00-06:00\n      0.80\n      37.78\n    \n  \n\n\n\n\n\nQuestion 1: What is the mean temperature and humidity per hour of the day?\nWe’ll create a new column in the df containing the hour information from the index.\n\ndf[\"hour\"] = df.index.hour\n\n\ndf.head()\n\n\n\n\n  \n    \n      \n      humidity\n      temperature\n      hour\n    \n  \n  \n    \n      2015-01-01 00:00:00-06:00\n      0.73\n      38.74\n      0\n    \n    \n      2015-01-01 01:00:00-06:00\n      0.74\n      38.56\n      1\n    \n    \n      2015-01-01 02:00:00-06:00\n      0.75\n      38.56\n      2\n    \n    \n      2015-01-01 03:00:00-06:00\n      0.79\n      37.97\n      3\n    \n    \n      2015-01-01 04:00:00-06:00\n      0.80\n      37.78\n      4\n    \n  \n\n\n\n\n\nmean_temp_humidity = df.groupby(\"hour\").mean()\nmean_temp_humidity.head()\n\n\n\n\n  \n    \n      \n      humidity\n      temperature\n    \n    \n      hour\n      \n      \n    \n  \n  \n    \n      0\n      0.779322\n      45.976441\n    \n    \n      1\n      0.803898\n      44.859492\n    \n    \n      2\n      0.812203\n      44.244407\n    \n    \n      3\n      0.819153\n      43.724068\n    \n    \n      4\n      0.832712\n      43.105763\n    \n  \n\n\n\n\n\nmean_temp_humidity.plot(subplots=True);\n\n\n\n\nWe can use pivoting to achieve the same dataframe.\n\nmean_temp_humidity_pivoting = pd.pivot_table(df, index=[\"hour\"], values=[\"temperature\", \"humidity\"])\n\n\nmean_temp_humidity_pivoting.head()\n\n\n\n\n  \n    \n      \n      humidity\n      temperature\n    \n    \n      hour\n      \n      \n    \n  \n  \n    \n      0\n      0.779322\n      45.976441\n    \n    \n      1\n      0.803898\n      44.859492\n    \n    \n      2\n      0.812203\n      44.244407\n    \n    \n      3\n      0.819153\n      43.724068\n    \n    \n      4\n      0.832712\n      43.105763\n    \n  \n\n\n\n\nBy default the aggregation function used in pivoting is mean.\n\n\nQuestion 2: Can we plot the daily variation in temperature per hour of the day?\nFor this, we want to have a dataframe with hour of day as the index and the different days as the different columns.\n\ndf[\"day\"] = df.index.dayofyear\n\n\ndf.head()\n\n\n\n\n  \n    \n      \n      humidity\n      temperature\n      hour\n      day\n    \n  \n  \n    \n      2015-01-01 00:00:00-06:00\n      0.73\n      38.74\n      0\n      1\n    \n    \n      2015-01-01 01:00:00-06:00\n      0.74\n      38.56\n      1\n      1\n    \n    \n      2015-01-01 02:00:00-06:00\n      0.75\n      38.56\n      2\n      1\n    \n    \n      2015-01-01 03:00:00-06:00\n      0.79\n      37.97\n      3\n      1\n    \n    \n      2015-01-01 04:00:00-06:00\n      0.80\n      37.78\n      4\n      1\n    \n  \n\n\n\n\n\ndaily_temp = pd.pivot_table(df, index=[\"hour\"], columns=[\"day\"], values=[\"temperature\"])\n\n\ndaily_temp.head()\n\n\n\n\n  \n    \n      \n      temperature\n    \n    \n      day\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      50\n      51\n      52\n      53\n      54\n      55\n      56\n      57\n      58\n      59\n    \n    \n      hour\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      38.74\n      39.94\n      39.57\n      41.83\n      33.95\n      36.98\n      46.93\n      29.95\n      36.57\n      36.19\n      ...\n      46.17\n      54.01\n      66.57\n      55.49\n      37.68\n      30.34\n      34.97\n      39.93\n      36.19\n      32.25\n    \n    \n      1\n      38.56\n      39.76\n      39.75\n      40.85\n      32.29\n      35.89\n      45.33\n      28.55\n      37.31\n      36.40\n      ...\n      41.38\n      54.56\n      66.57\n      55.49\n      36.76\n      30.04\n      34.97\n      36.37\n      36.38\n      32.25\n    \n    \n      2\n      38.56\n      39.58\n      39.94\n      39.73\n      31.59\n      36.44\n      44.51\n      27.44\n      37.78\n      36.59\n      ...\n      39.99\n      55.81\n      66.57\n      55.34\n      35.56\n      30.57\n      34.75\n      34.74\n      36.20\n      32.25\n    \n    \n      3\n      37.97\n      38.83\n      40.16\n      38.78\n      30.48\n      36.85\n      43.92\n      25.97\n      37.97\n      36.38\n      ...\n      39.05\n      57.14\n      66.38\n      55.27\n      34.94\n      30.59\n      35.15\n      34.31\n      36.20\n      32.52\n    \n    \n      4\n      37.78\n      39.02\n      40.65\n      39.74\n      29.89\n      35.72\n      44.37\n      24.74\n      37.82\n      35.49\n      ...\n      37.99\n      57.51\n      66.57\n      55.49\n      34.04\n      30.38\n      35.15\n      33.02\n      34.49\n      32.52\n    \n  \n\n5 rows × 59 columns\n\n\n\n\ndaily_temp.plot(style='k-', alpha=0.3, legend=False)\nplt.ylabel(\"Temp\");\n\n\n\n\nSo, we can see some pattern up there! Around 15 hours, the temperature usually peaks.\nThere you go! Some recipes for aggregation and plotting of time series data."
  },
  {
    "objectID": "posts/2017-06-15-linear-regression-prior.html",
    "href": "posts/2017-06-15-linear-regression-prior.html",
    "title": "Linear regression with prior (using gradient descent)",
    "section": "",
    "text": "\\[W = \\alpha \\times W_{prior} + \\delta\\] \\[b = \\beta + b_{prior} + \\eta\\]\nOur task reduces to learn \\(\\alpha\\), \\(\\beta\\), \\(\\delta\\) and \\(\\eta\\). This can be solved as we would usually do using Gradient descent, the only difference being that we will compute the gradient wrt \\(\\alpha\\) , \\(\\beta\\), \\(\\delta\\), \\(\\eta\\). I will use autograd to compute the gradients.\nIn a typical model we might have 2 parameters (w and b). In our refined one, we have four- \\(\\alpha\\) , \\(\\beta\\), \\(\\delta\\), \\(\\eta\\).\n\nCustomary imports\n\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\nTrue model\n\\[Y = 10 X + 6\\]\n\n\nGenerating data\n\nnp.random.seed(0)\nn_samples = 50\nX = np.linspace(1, 50, n_samples)\nY = 10*X + 6 + 3*np.random.randn(n_samples)\n\n\nplt.plot(X, Y, 'k.')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\");\n\n\n\n\n\n\nDefining priors (bad ones!)\n\nw_prior = -2\nb_prior = -2\n\n\n\nDefining the cost function in terms of alpha and beta\n\ndef cost(alpha, beta, delta, eta):\n    pred = np.dot(X, alpha*w_prior+delta) + b_prior + beta + eta\n    return np.sqrt(((pred - Y) ** 2).mean(axis=None))\n\nfrom autograd import grad, multigrad\ngrad_cost= multigrad(cost, argnums=[0, 1, 2, 3])\n\n\n\nGradient descent\n\nalpha = np.random.randn()\nbeta = np.random.randn()\neta = np.random.randn()\ndelta = np.random.randn()\nlr = 0.001\n# We will also save the values for plotting later\nw_s = [alpha*w_prior+delta]\nb_s = [alpha*w_prior+delta]\nfor i in range(10001):\n    \n    del_alpha, del_beta, del_delta, del_eta = grad_cost(alpha, beta, delta, eta)\n    alpha = alpha - del_alpha*lr\n    beta = beta - del_beta*lr\n    delta = delta - del_delta*lr\n    eta = eta - del_eta*lr\n    w_s.append(alpha*w_prior+delta)\n    b_s.append(alpha*w_prior+delta)\n    if i%500==0:\n        print \"*\"*20\n        print i\n        print \"*\"*20\n    \n        print cost(alpha, beta, delta, eta), alpha*w_prior+delta, alpha*w_prior+delta\n\n********************\n0\n********************\n277.717926153 0.756766902473 0.756766902473\n********************\n500\n********************\n5.95005440573 10.218493676 10.218493676\n********************\n1000\n********************\n5.77702829051 10.2061390906 10.2061390906\n********************\n1500\n********************\n5.60823669668 10.1939366275 10.1939366275\n********************\n2000\n********************\n5.44395500928 10.1818982949 10.1818982949\n********************\n2500\n********************\n5.28446602486 10.1700368748 10.1700368748\n********************\n3000\n********************\n5.1300568557 10.158365894 10.158365894\n********************\n3500\n********************\n4.98101499128 10.1468995681 10.1468995681\n********************\n4000\n********************\n4.83762347034 10.1356527141 10.1356527141\n********************\n4500\n********************\n4.70015516667 10.1246406278 10.1246406278\n********************\n5000\n********************\n4.56886626032 10.1138789219 10.1138789219\n********************\n5500\n********************\n4.44398905185 10.1033833225 10.1033833225\n********************\n6000\n********************\n4.32572437603 10.0931694258 10.0931694258\n********************\n6500\n********************\n4.21423397192 10.0832524173 10.0832524173\n********************\n7000\n********************\n4.10963325557 10.0736467626 10.0736467626\n********************\n7500\n********************\n4.01198500112 10.0643658801 10.0643658801\n********************\n8000\n********************\n3.92129444852 10.0554218111 10.0554218111\n********************\n8500\n********************\n3.83750630808 10.046824905 10.046824905\n********************\n9000\n********************\n3.7605040187 10.0385835381 10.0385835381\n********************\n9500\n********************\n3.69011144573 10.0307038843 10.0307038843\n********************\n10000\n********************\n3.6260969956 10.023189752 10.023189752\n\n\nWe are able to learn a reasonably accurate W=10.07 and b=2.7.\n\n\nBonus: Animation\nMaking the plots look nicer.\n\ndef format_axes(ax):\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color('grey')\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color='grey')\n    return ax\n\n\n# Code courtesy: http://eli.thegreenplace.net/2016/drawing-animated-gifs-with-matplotlib/\nfrom matplotlib.animation import FuncAnimation\n\nfig, ax = plt.subplots(figsize=(4, 3))\nfig.set_tight_layout(True)\n\n# Query the figure's on-screen size and DPI. Note that when saving the figure to\n# a file, we need to provide a DPI for that separately.\nprint('fig size: {0} DPI, size in inches {1}'.format(\n    fig.get_dpi(), fig.get_size_inches()))\n\n# Plot a scatter that persists (isn't redrawn) and the initial line.\n\nax.scatter(X, Y, color='grey', alpha=0.8, s=1)\n# Initial line\n\nline, = ax.plot(X, X*w_prior+b_prior, 'r-', linewidth=1)\n\ndef update(i):\n    label = 'Iteration {0}'.format(i)\n    line.set_ydata(X*w_s[i]+b_s[i])\n    ax.set_xlabel(label)\n    format_axes(ax)\n    return line, ax\n\nanim = FuncAnimation(fig, update, frames=np.arange(0, 100), interval=1)\nanim.save('line_prior.gif', dpi=80, writer='imagemagick')\nplt.close()\n\nfig size: 72.0 DPI, size in inches [ 4.  3.]"
  },
  {
    "objectID": "posts/2021-06-17-python-ssh.html",
    "href": "posts/2021-06-17-python-ssh.html",
    "title": "Running Python scripts on server over ssh and getting back content",
    "section": "",
    "text": "Solution: Send video to server and run a Python script to convert to GIF and get the content back\nI use two scripts:\nFirst called convert-video-gif.py that uses moviepy (FFMPEG under the hood) to convert an MP4 to a GIF\nfrom moviepy.editor import *\nimport sys\n\nvid = sys.argv[1]\nvid_name = vid.split(\".\")[0]\nclip = (VideoFileClip(vid).speedx(10).resize(0.3))\nclip.write_gif(vid_name + \".gif\")\nSecond called convert_video.py that accepts: - a video file as a command line argument - transfers the video files to the server using SFTP - transfers the convert-video-gif.py file to the server using SFTP - runs the python file convert-video-gif.py on the server and create the GIF - retreive the GIF back to my local machine\n\nimport sys\nimport paramiko\n\n# The file you wish to convert to GIF\nf = sys.argv[1]\nf_name_without_ext = f.split(\".\")[0]\nf_name_gif = f_name_without_ext + \".gif\"\n\nssh_client =paramiko.SSHClient()\nssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\nssh_client.connect(hostname='MY-HOSTNAME', username='MY-USERNAME', password='MY-PASSWORD')\nftp_client=ssh_client.open_sftp()\n\n# Transferring convert-video-gif.py to server\nftp_client.put('convert-video-gif.py', 'convert-video-gif.py')\n\n# Transferring video to server\nftp_client.put(f, f)\n\n# Running Python script\nstdin, stdout, stderr = ssh_client.exec_command(f\"python convert-video-gif.py {f}\")\ne = stderr.readlines()\n\n# Retreiving the video back from server\nftp_client.get(f_name_gif, f_name_gif)\nIn another previous post, I had also mentioned about the amazing SSH/SFTP apps: Termius and ShellFish. I am again pasting the GIF showing a similar to above workflow initiated via ShellFish."
  },
  {
    "objectID": "posts/2018-06-26-map-electricity-access.html",
    "href": "posts/2018-06-26-map-electricity-access.html",
    "title": "Visualising Electricity Access Over Space and Time",
    "section": "",
    "text": "I’ll be using data from World Bank for electricity access. See the image below for the corresponding page.\n\n\nDownloading World Bank data\nNow, a Python package called wbdata provides a fairly easy way to access World Bank data. I’d be using it to get data in Pandas DataFrame.\n\n%matplotlib inline\nimport pandas as pd\nimport wbdata\nimport matplotlib.pyplot as plt\nimport datetime\ndata_date = (datetime.datetime(1990, 1, 1), datetime.datetime(2016, 1, 1))\ndf_elec = wbdata.get_data(\"EG.ELC.ACCS.ZS\", pandas=True, data_date=data_date)\n\n\ndf_elec.head()\n\ncountry     date\nArab World  2016    88.768654\n            2015    88.517967\n            2014    88.076774\n            2013    88.389705\n            2012    87.288244\nName: value, dtype: float64\n\n\n\n\nDownloading Geodata and Reading Using GeoPandas\nI’d now be downloading shapefile data for different countries. This will help us to spatially plot the data for the different countries.\n\n!wget http://naciscdn.org/naturalearth/10m/cultural/ne_10m_admin_0_countries_lakes.zip\n\n--2018-06-26 15:52:50--  http://naciscdn.org/naturalearth/10m/cultural/ne_10m_admin_0_countries_lakes.zip\nResolving naciscdn.org (naciscdn.org)... 146.201.97.163\nConnecting to naciscdn.org (naciscdn.org)|146.201.97.163|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5077755 (4.8M) [application/x-zip-compressed]\nSaving to: ‘ne_10m_admin_0_countries_lakes.zip’\n\nne_10m_admin_0_coun 100%[===================>]   4.84M   246KB/s    in 22s     \n\n2018-06-26 15:53:12 (228 KB/s) - ‘ne_10m_admin_0_countries_lakes.zip’ saved [5077755/5077755]\n\n\n\n\nExtracting shapefile\n\nimport zipfile\nzip_ref = zipfile.ZipFile('ne_10m_admin_0_countries_lakes.zip', 'r')\nzip_ref.extractall('.')\nzip_ref.close()\n\n\nimport geopandas as gpd\ngdf = gpd.read_file('ne_10m_admin_0_countries_lakes.shp')[['ADM0_A3', 'geometry']]\n\n\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      ADM0_A3\n      geometry\n    \n  \n  \n    \n      0\n      IDN\n      (POLYGON ((117.7036079039552 4.163414542001791...\n    \n    \n      1\n      MYS\n      (POLYGON ((117.7036079039552 4.163414542001791...\n    \n    \n      2\n      CHL\n      (POLYGON ((-69.51008875199994 -17.506588197999...\n    \n    \n      3\n      BOL\n      POLYGON ((-69.51008875199994 -17.5065881979999...\n    \n    \n      4\n      PER\n      (POLYGON ((-69.51008875199994 -17.506588197999...\n    \n  \n\n\n\n\n\n\n\nVisualising electricity access in 2016\n\nGetting electricity access data for 2016\n\ndf_2016 = df_elec.unstack()[['2016']].dropna()\n\n\ndf_2016.head()\n\n\n\n\n\n  \n    \n      date\n      2016\n    \n    \n      country\n      \n    \n  \n  \n    \n      Afghanistan\n      84.137138\n    \n    \n      Albania\n      100.000000\n    \n    \n      Algeria\n      99.439568\n    \n    \n      Andorra\n      100.000000\n    \n    \n      Angola\n      40.520607\n    \n  \n\n\n\n\nIn order to visualise electricity access data over the map, we would have to join the GeoPandas object gdf and df_elec\n\n\nJoining gdf and df_2016\nNow, gdf uses alpha_3 codes for country names like AFG, etc., whereas df_2016 uses country names. We will thus use pycountry package to get code names corresponding to countries in df_2016 as shown in this StackOverflow post.\n\nimport pycountry\ncountries = {}\nfor country in pycountry.countries:\n    countries[country.name] = country.alpha_3\ncodes = [countries.get(country, 'Unknown code') for country in df_2016.index]\ndf_2016['Code'] = codes\n\n\ndf_2016.head()\n\n\n\n\n\n  \n    \n      date\n      2016\n      Code\n    \n    \n      country\n      \n      \n    \n  \n  \n    \n      Afghanistan\n      84.137138\n      AFG\n    \n    \n      Albania\n      100.000000\n      ALB\n    \n    \n      Algeria\n      99.439568\n      DZA\n    \n    \n      Andorra\n      100.000000\n      AND\n    \n    \n      Angola\n      40.520607\n      AGO\n    \n  \n\n\n\n\nNow, we can join the two data sources\n\nmerged_df_2016 = gpd.GeoDataFrame(pd.merge(gdf, df_2016, left_on='ADM0_A3', right_on='Code'))\n\n\nmerged_df_2016.head()\n\n\n\n\n\n  \n    \n      \n      ADM0_A3\n      geometry\n      2016\n      Code\n    \n  \n  \n    \n      0\n      IDN\n      (POLYGON ((117.7036079039552 4.163414542001791...\n      97.620000\n      IDN\n    \n    \n      1\n      MYS\n      (POLYGON ((117.7036079039552 4.163414542001791...\n      100.000000\n      MYS\n    \n    \n      2\n      CHL\n      (POLYGON ((-69.51008875199994 -17.506588197999...\n      100.000000\n      CHL\n    \n    \n      3\n      PER\n      (POLYGON ((-69.51008875199994 -17.506588197999...\n      94.851746\n      PER\n    \n    \n      4\n      ARG\n      (POLYGON ((-68.4486097329999 -52.3466170159999...\n      100.000000\n      ARG\n    \n  \n\n\n\n\n\n\nFinally plotting!\n\n# Example borrowed from http://ramiro.org/notebook/geopandas-choropleth/\ncmap='OrRd'\nfigsize = (16, 5)\nax = merged_df_2016.plot(column='2016', cmap=cmap, figsize=figsize,legend=True)\ntitle = 'Electricity Access(% of population) in {}'.format('2016')\ngdf[~gdf.ADM0_A3.isin(merged_df_2016.ADM0_A3)].plot(ax=ax, color='#fffafa', hatch='///')\nax.set_title(title, fontdict={'fontsize': 15}, loc='left')\nax.set_axis_off()\n\n\n\n\n\n\n\nCreating animation for access across time\n\n!mkdir -p elec_access\n\n\ndef save_png_year(year, path=\"elec_access\"):\n    df_year = df_elec.unstack()[['{}'.format(year)]].dropna()\n    codes = [countries.get(country, 'Unknown code') for country in df_year.index]\n    df_year['Code'] = codes\n    merged_df_year = gpd.GeoDataFrame(pd.merge(gdf, df_year, left_on='ADM0_A3', right_on='Code'))\n    figsize = (16, 5)\n    ax = merged_df_year.plot(column='{}'.format(year), cmap=cmap, figsize=figsize,legend=True,vmin=0.0, vmax=100.0)\n    title = 'Electricity Access(% of population) in {}'.format(year)\n    gdf[~gdf.ADM0_A3.isin(merged_df_year.ADM0_A3)].plot(ax=ax, color='#fffafa', hatch='///')\n    ax.set_title(title, fontdict={'fontsize': 15}, loc='left')\n    ax.set_axis_off()\n    plt.savefig('{}/{}.png'.format(path, year), dpi=300)\n    plt.close()\n\n\nfor year in range(1990, 2017):\n    save_png_year(year)\n\n\n# Borrowed from http://www.kevinwampler.com/blog/2016/09/10/creating-animated-gifs-using-python.html\ndef create_gifv(input_files, output_base_name, fps):\n    import imageio\n    output_extensions = [\"gif\"]\n    input_filenames = ['elec_access/{}.png'.format(year) for year in range(1990, 2017)]\n\n    poster_writer = imageio.get_writer(\"{}.png\".format(output_base_name), mode='i')\n    video_writers = [\n        imageio.get_writer(\"{}.{}\".format(output_base_name, ext), mode='I', fps=fps)\n        for ext in output_extensions]\n\n    is_first = True\n    for filename in input_filenames:\n        img = imageio.imread(filename)\n\n        for writer in video_writers:\n            writer.append_data(img)\n        if is_first:\n            poster_writer.append_data(img)\n\n        is_first = False\n\n    for writer in video_writers + [poster_writer]:\n        writer.close()\n\n\ncreate_gifv(\"elec_access/*.png\", \"electricity_access\", 4)\n\n\nAcross Africa and SE Asia, one can clearly see a gradual improvement in access! Hope you had fun reading this post :)"
  },
  {
    "objectID": "posts/2022-02-24-audio-filtering.html",
    "href": "posts/2022-02-24-audio-filtering.html",
    "title": "Audio Filtering",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport seaborn as sns\nfrom functools import partial\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nimport librosa\ny, sr = librosa.load(\"/Users/nipun/Downloads/external-sensors_data_audio_audacity_recorded-mask-tidal-breathing.wav\")\n\n\nplt.plot(y), sr\n\n([<matplotlib.lines.Line2D at 0x139db2490>], 22050)\n\n\n\n\n\n\nfrom scipy import signal\n\n\nfrom scipy.fft import fft, fftfreq\n\n\nyf = fft(y)\nxf = fftfreq(len(y), 1 / sr)\n\nplt.plot(xf, np.abs(yf), lw=0.1)\nplt.xlim((0, 1000))\n\n(0.0, 1000.0)\n\n\n\n\n\n\nplt.specgram(x = y,Fs=sr);\n#plt.ylim((0, 100))\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1424d5fd0>\n\n\n\n\n\n\nsos = signal.butter(10, 20, 'lp', fs=sr, output='sos')\nfiltered = signal.sosfilt(sos, y)\nplt.plot(y)\nplt.plot(filtered)\n\n\n\n\n\nplt.plot(filtered)\n\n\n\n\n\nplt.specgram(x = filtered,Fs=sr);\n#plt.ylim((0, 100))\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1404d1d00>"
  },
  {
    "objectID": "posts/2022-02-11-pytorch-learn-normal-map.html",
    "href": "posts/2022-02-11-pytorch-learn-normal-map.html",
    "title": "Maximum A-Posteriori (MAP) for parameters of univariate and multivariate normal distribution in PyTorch",
    "section": "",
    "text": "dist = torch.distributions\n\n\nCreating a 1d normal distribution\n\nuv_normal = dist.Normal(loc=0.0, scale=1.0)\n\n\n\nSampling from the distribution\n\nsamples = uv_normal.sample(sample_shape=[100])\n\n\nsns.kdeplot(samples, bw_adjust=2)\nsns.despine()\n\n\n\n\n\n\nDefining the prior\n\nprior_mu = torch.tensor(5.0, requires_grad=True)\nprior = dist.Normal(loc=prior_mu, scale=1.0)\nprior\n\nNormal(loc: 5.0, scale: 1.0)\n\n\n\n\nComputing logprob of prior for a mu\n\ndef logprob_prior(mu):\n    return -prior.log_prob(mu)\n\n\n\nComputing logprob of observing data given a mu\n\nstdev_likelihood = 1.0\n\n\ndef log_likelihood(mu, samples):\n\n    to_learn = torch.distributions.Normal(loc=mu, scale=stdev_likelihood)\n    return -torch.sum(to_learn.log_prob(samples))\n\n\nmu = torch.tensor(-2.0, requires_grad=True)\n\nlog_likelihood(mu, samples), logprob_prior(mu)\nlog_likelihood(mu, samples).item()\n\n305.98101806640625\n\n\n\nout = {\"Logprob_Prior\": {}, \"LogLikelihood\": {}}\nfor mu_s in torch.linspace(-10, 10, 100):\n    t = mu_s.item()\n    mu = torch.tensor(mu_s)\n    out[\"Logprob_Prior\"][t] = logprob_prior(mu).item()\n    out[\"LogLikelihood\"][t] = log_likelihood(mu, samples).item()\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73152/3102909564.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  mu = torch.tensor(mu_s)\n\n\n\npd.DataFrame(out).plot(subplots=True)\n\narray([<AxesSubplot:>, <AxesSubplot:>], dtype=object)\n\n\n\n\n\n\ndef loss(mu):\n    return log_likelihood(mu, samples) + logprob_prior(mu)\n\n\nmu = torch.tensor(2.0, requires_grad=True)\n\nopt = torch.optim.Adam([mu], lr=0.01)\nfor i in range(1500):\n    loss_val = loss(mu)\n    loss_val.backward()\n    if i % 100 == 0:\n        print(f\"Iteration: {i}, Loss: {loss_val.item():0.2f}, Loc: {mu.item():0.6f}\")\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 374.37, Loc: 2.000000\nIteration: 100, Loss: 222.93, Loc: 1.092788\nIteration: 200, Loss: 166.98, Loc: 0.468122\nIteration: 300, Loss: 152.88, Loc: 0.119012\nIteration: 400, Loss: 150.57, Loc: -0.034995\nIteration: 500, Loss: 150.33, Loc: -0.088207\nIteration: 600, Loss: 150.31, Loc: -0.102667\nIteration: 700, Loss: 150.31, Loc: -0.105761\nIteration: 800, Loss: 150.31, Loc: -0.106279\nIteration: 900, Loss: 150.31, Loc: -0.106346\nIteration: 1000, Loss: 150.31, Loc: -0.106352\nIteration: 1100, Loss: 150.31, Loc: -0.106353\nIteration: 1200, Loss: 150.31, Loc: -0.106353\nIteration: 1300, Loss: 150.31, Loc: -0.106353\nIteration: 1400, Loss: 150.31, Loc: -0.106353\n\n\n\n\nAnalytical MAP estimate of location\n\\(\\hat{\\theta}_{MAP}=\\dfrac{n}{n+\\sigma^{2}} \\bar{x}+\\dfrac{\\sigma^{2}}{n+\\sigma^{2}} \\mu\\)\n\nprior_mu\n\ntensor(5., requires_grad=True)\n\n\n\nn = samples.shape[0]\nsample_mean = samples.mean()\nn_plus_variance = n + stdev_likelihood**2\n\nloc_map = ((n * sample_mean) / n_plus_variance) + (\n    (stdev_likelihood**2) / (n_plus_variance)\n) * prior_mu\nloc_map.item()\n\n-0.1063527911901474\n\n\n\ntorch.allclose(loc_map, mu)\n\nTrue\n\n\n\nSetting 2: Learning location and scale\nAn important difference from the previous code is that we need to use a transformed variable to ensure scale is positive. We do so by using softplus.\n\nmu = torch.tensor(1.0, requires_grad=True)\nscale = torch.tensor(2.0, requires_grad=True)\n\n\ndef log_likelihood(mu, scale, samples):\n    scale_softplus = torch.functional.F.softplus(scale)\n    to_learn = torch.distributions.Normal(loc=mu, scale=scale_softplus)\n    return -torch.sum(to_learn.log_prob(samples))\n\n\ndef loss(mu, scale):\n    return log_likelihood(mu, scale, samples) + logprob_prior(mu)\n\n\nopt = torch.optim.Adam([mu, scale], lr=0.01)\nfor i in range(1500):\n    loss_val = loss(mu, scale)\n    loss_val.backward()\n    if i % 100 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss_val.item():0.2f}, Loc: {mu.item():0.3f}, Scale: {torch.functional.F.softplus(scale).item():0.3f}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 200.89, Loc: 1.000, Scale: 2.127\nIteration: 100, Loss: 158.51, Loc: 0.086, Scale: 1.282\nIteration: 200, Loss: 149.98, Loc: -0.112, Scale: 0.942\nIteration: 300, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 400, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 500, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 600, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 700, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 800, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 900, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 1000, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 1100, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 1200, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 1300, Loss: 149.98, Loc: -0.112, Scale: 0.943\nIteration: 1400, Loss: 149.98, Loc: -0.112, Scale: 0.943\n\n\nWe can see that our gradient based methods parameters match those of the MLE computed analytically.\n\nmvn = dist.MultivariateNormal(\n    loc=torch.tensor([1.0, 1.0]),\n    covariance_matrix=torch.tensor([[2.0, 0.5], [0.5, 0.4]]),\n)\n\n\nmle_mvn_loc = mvn_samples = mvn.sample([1000])\n\n\nloss\n\n\nloc = torch.tensor([-1.0, 1.0], requires_grad=True)\ntril = torch.autograd.Variable(torch.tril(torch.ones(2, 2)), requires_grad=True)\nopt = torch.optim.Adam([loc, tril], lr=0.01)\n\nprior = dist.MultivariateNormal(\n    loc=torch.tensor([0.0, 0.0]),\n    covariance_matrix=torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n)\n\n\ndef log_likelihood(loc, tril, samples):\n    cov = tril @ tril.t()\n    to_learn = torch.distributions.MultivariateNormal(loc=loc, covariance_matrix=cov)\n    return -torch.sum(to_learn.log_prob(samples))\n\n\ndef logprob_prior(loc):\n    return -prior.log_prob(loc)\n\n\ndef loss(loc, tril, samples):\n    return log_likelihood(loc, tril, samples) + logprob_prior(loc)\n\n\nfor i in range(8100):\n    to_learn = dist.MultivariateNormal(loc=loc, covariance_matrix=tril @ tril.t())\n    loss_value = loss(loc, tril, mvn_samples)\n    loss_value.backward()\n    if i % 500 == 0:\n        print(f\"Iteration: {i}, Loss: {loss_value.item():0.2f}, Loc: {loc}\")\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 7663.86, Loc: tensor([-1.,  1.], requires_grad=True)\nIteration: 500, Loss: 2540.96, Loc: tensor([0.8229, 0.9577], requires_grad=True)\nIteration: 1000, Loss: 2526.40, Loc: tensor([1.0300, 1.0076], requires_grad=True)\nIteration: 1500, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 2000, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 2500, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 3000, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 3500, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 4000, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 4500, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 5000, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 5500, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 6000, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 6500, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 7000, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 7500, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\nIteration: 8000, Loss: 2526.40, Loc: tensor([1.0308, 1.0077], requires_grad=True)\n\n\n\ntril@tril.t(),mvn.covariance_matrix, prior.covariance_matrix\n\n(tensor([[1.9699, 0.4505],\n         [0.4505, 0.3737]], grad_fn=<MmBackward0>),\n tensor([[2.0000, 0.5000],\n         [0.5000, 0.4000]]),\n tensor([[1., 0.],\n         [0., 1.]]))\n\n\n\nTodo\n\n1. Expand on MVN case\n2. Clean up code\n3. Visualize, prior, likelihood, MLE, MAP \n4. Shrinkage estimation (reference Murphy book)\n5. Inverse Wishart distribution\n\nReferences\n\nhttps://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian\nhttps://forum.pyro.ai/t/mle-for-normal-distribution-parameters/3861/3\nhttps://ericmjl.github.io/notes/stats-ml/estimating-a-multivariate-gaussians-parameters-by-gradient-descent/\nhttps://www.youtube.com/watch?v=KogqeZ_88-g&list=PLD0F06AA0D2E8FFBA&index=32"
  },
  {
    "objectID": "posts/2022-02-17-ppca.html",
    "href": "posts/2022-02-17-ppca.html",
    "title": "Probabilstic PCA using PyTorch distributions",
    "section": "",
    "text": "Generative model for PPCA in PyTorch\n\ndata_dim = 2\nlatent_dim = 1\nnum_datapoints = 100\nz = dist.Normal(\n    loc=torch.zeros([latent_dim, num_datapoints]),\n    scale=torch.ones([latent_dim, num_datapoints]),)\n\nw = dist.Normal(\n    loc=torch.zeros([data_dim, latent_dim]),\n    scale=5.0 * torch.ones([data_dim, latent_dim]),\n)\n\n\nw_sample= w.sample()\nz_sample = z.sample()\n\n\nx = dist.Normal(loc = w_sample@z_sample, scale=1)\nx_sample = x.sample([100])\nplt.scatter(x_sample[:, 0], x_sample[:, 1], alpha=0.2, s=30)\n\n<matplotlib.collections.PathCollection at 0x16051f1c0>\n\n\n\n\n\n\n\nGenerative model for PPCA in Pyro\n\nimport pyro.distributions as dist\nimport pyro.distributions.constraints as constraints\nimport pyro\n\npyro.clear_param_store()\n\n\ndef ppca_model(data, latent_dim):\n    N, data_dim = data.shape\n    W = pyro.sample(\n        \"W\",\n        dist.Normal(\n            loc=torch.zeros([latent_dim, data_dim]),\n            scale=5.0 * torch.ones([latent_dim, data_dim]),\n        ),\n    )\n    Z = pyro.sample(\n        \"Z\",\n        dist.Normal(\n            loc=torch.zeros([N, latent_dim]),\n            scale=torch.ones([N, latent_dim]),\n        ),\n    )\n\n    mean = Z @ W\n\n    return pyro.sample(\"obs\", pyro.distributions.Normal(mean, 1.0), obs=data)\n\n\npyro.render_model(\n    ppca_model, model_args=(torch.randn(150, 2), 1), render_distributions=True\n)\n\n\n\n\n\nppca_model(x_sample[0], 3).shape\n\ntorch.Size([2, 100])\n\n\n\nfrom pyro import poutine\nwith pyro.plate(\"samples\", 10, dim=-3):\n    trace = poutine.trace(ppca_model).get_trace(x_sample[0], 1)\n\n\ntrace.nodes['W']['value'].squeeze()\n\ntorch.Size([10, 100])\n\n\n\ndata_dim = 3\nlatent_dim = 2\n\nW = pyro.sample(\n        \"W\",\n        dist.Normal(\n            loc=torch.zeros([latent_dim, data_dim]),\n            scale=5.0 * torch.ones([latent_dim, data_dim]),\n        ),\n    )\n\n\nN = 150\nZ = pyro.sample(\n        \"Z\",\n        dist.Normal(\n            loc=torch.zeros([N, latent_dim]),\n            scale=torch.ones([N, latent_dim]),\n        ),\n    )\n\n\nZ.shape, W.shape\n\n(torch.Size([150, 2]), torch.Size([2, 3]))\n\n\n\n(Z@W).shape\n\ntorch.Size([150, 3])"
  },
  {
    "objectID": "posts/2020-03-29-param-learning.html",
    "href": "posts/2020-03-29-param-learning.html",
    "title": "Learning Gaussian Process regression parameters using gradient descent",
    "section": "",
    "text": "In previous posts, I have talked about GP regression:\n\nPost 1 on programatically understanding GPs\nPost 2 on making use of a popular GP library called GPy\n\nIn this post, I will be talking about how to learn the parameters of a GP. I’ll keep this post simple and specific to a trivial example using RBF kernel (though the methods discussed are general.)\nTo keep things simple, we will assume a mean prior of zero and we will only be learning the parameters of the kernel function.\n\nKey Idea\n\nWrite the expression of log likelihood of data in terms of kernel parameters\nUse gradient descent to optimize the objective (negative log likelihood) and update the kernel parameters\n\n\n\nDefining log-likelihood\nIn our previous post we had mentioned (for the noiseless case):\nGiven train data \\[\nD=\\left(x_{i}, y_{i}\\right), i=1: N\n\\] Given a test set \\(X_{*}\\) of size \\(N_{*} \\times d\\) containing \\(N_{*}\\) points in \\(\\mathbb{R}^{d},\\) we want to predict function outputs \\(y_{*}\\) We can write: \\[\n\\left(\\begin{array}{l}\ny \\\\\ny_{*}\n\\end{array}\\right) \\sim \\mathcal{N}\\left(\\left(\\begin{array}{l}\n\\mu \\\\\n\\mu_{*}\n\\end{array}\\right),\\left(\\begin{array}{cc}\nK & K_{*} \\\\\nK_{*}^{T} & K_{* *}\n\\end{array}\\right)\\right)\n\\] where \\[\n\\begin{aligned}\nK &=\\operatorname{Ker}(X, X) \\in \\mathbb{R}^{N \\times N} \\\\\nK_{*} &=\\operatorname{Ker}\\left(X, X_{*}\\right) \\in \\mathbb{R}^{N \\times N} \\\\\nK_{* *} &=\\operatorname{Ker}\\left(X_{*}, X_{*}\\right) \\in \\mathbb{R}^{N_{*} \\times N_{*}}\n\\end{aligned}\n\\]\nThus, from the property of conditioning of multivariate Gaussian, we know that:\n\\[y \\sim \\mathcal{N}_N(\\mu, K)\\]\nWe will assume \\(\\mu\\) to be zero. Thus, we have for the train data, the following expression:\n\\[y \\sim \\mathcal{N}_N(0, K)\\]\nFor the noisy case, we have:\n\\[y \\sim \\mathcal{N}_N(0, K + \\sigma_{noise}^2\\mathcal{I}_N)\\]\nFrom this expression, we can write the log-likelihood of data computed over the kernel parameters \\(\\theta\\) as:\n\\[\\mathcal{LL}(\\theta) = \\log(\\frac{\\exp((-1/2)(y-0)^T (K+\\sigma_{noise}^2\\mathcal{I}_N)^{-1}(y-0))}{(2\\pi)^{N/2}|(K+\\sigma_{noise}^2\\mathcal{I}_N)|^{1/2}})\\]\nThus, we can write:\n\\[\\mathcal{LL}(\\theta) =\\log P(\\mathbf{y} | X, \\theta)=-\\frac{1}{2} \\mathbf{y}^{\\top} M^{-1} \\mathbf{y}-\\frac{1}{2} \\log |M|-\\frac{N}{2} \\log 2 \\pi\\]\nwhere \\[M = K + \\sigma_{noise}^2\\mathcal{I}_N\\]\n\n\nImports\nAs before, we will be using the excellent Autograd library for automatically computing the gradient of an objective function with respect to the parameters. We will also be using GPy for verifying our calculations.\nLet us start with some basic imports.\n\nimport autograd.numpy as np\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport GPy\n\n\n\nDefining our RBF kernel\nThe definition of the (1-dimensional) RBF kernel has a Gaussian-form, defined as:\n\\[\n    \\kappa_\\mathrm{rbf}(x_1,x_2) = \\sigma^2\\exp\\left(-\\frac{(x_1-x_2)^2}{2\\mathscr{l}^2}\\right)\n\\]\n\ndef rbf(x1, x2, sigma, l):\n    return (sigma**2)*(np.exp(-(x1-x2)**2/(2*(l**2))))    \n\n\n\nDefining GPy’s RBF kernel\n\n# Create a 1-D RBF kernel with default parameters\nk = GPy.kern.RBF(1)\n# Preview the kernel's parameters\nk\n\n\n\n  rbf.       valueconstraintspriors\n  variance     1.0    +ve          \n  lengthscale  1.0    +ve          \n\n\n\n\n\nMatching our RBF kernel with GPy’s kernel\n\nrbf(1, 0, 1, 1)==k.K(np.array([[1]]), np.array([[0]])).flatten()\n\narray([ True])\n\n\nLooks good. Our function is matching GPy’s kernel.\n\n\nGP Regresion\n\nCreating a data set\n\n# lambda function, call f(x) to generate data\nf = lambda x: 0.4*x**2 - 0.15*x**3 + 0.5*x**2 - 0.002*x**5 + 0.0002*x**6 +0.5*(x-2)**2\nn = 20\nnp.random.seed(0)\nX = np.linspace(0.05, 4.95, n)[:,None]\nY = f(X) + np.random.normal(0., 0.1, (n,1)) # note that np.random.normal takes mean and s.d. (not variance), 0.1^2 = 0.01\nplt.plot(X, Y, \"kx\", mew=2, label='Train points')\nplt.xlabel(\"x\"), plt.ylabel(\"f\")\nplt.legend();\n\n\n\n\n\n\n\nFunction to compute negative log likelihood\nBased on our above mentioned theory, we can now write the NLL function as follows\n\ndef nll(sigma=1, l=1, noise_std=1):\n    n = X.shape[0]\n    cov = rbf(X, X.T, sigma, l) + (noise_std**2)*np.eye(X.shape[0])\n    nll_ar =  0.5*(Y.T@np.linalg.inv(cov)@Y) + 0.5*n*np.log(2*np.pi) + 0.5*np.log(np.linalg.det(cov)) \n    return nll_ar[0,0]\n\n\n\nComparing the NLL from our method with the NLL from GPy\nWe will now compare the NLL from our method with GPy for a fixed set of parameters\n\nnll(1, 1, 1)\n\n40.103960984801276\n\n\n\nk.lengthscale = 1\nk.variance = 1\nm = GPy.models.GPRegression(X, Y, k, normalizer=False)\nm.Gaussian_noise = 1\nprint(m)\n\n\nName : GP regression\nObjective : 40.103961039553916\nNumber of Parameters : 3\nNumber of Optimization Parameters : 3\nUpdates : True\nParameters:\n  GP_regression.           |  value  |  constraints  |  priors\n  rbf.variance             |    1.0  |      +ve      |        \n  rbf.lengthscale          |    1.0  |      +ve      |        \n  Gaussian_noise.variance  |    1.0  |      +ve      |        \n\n\nExcellent, we can see that our method gives the same NLL. Looks like we are on the right track! One caveat here is that I have set the normalizer to be False, which means that GPy will not be mean centering the data.\n\n\nOptimizing the GP using GPy\nWe will now use GPy to optimize the GP parameters\n\nm = GPy.models.GPRegression(X, Y, k, normalizer=False)\nm.optimize()\nprint(m)\n\n\nName : GP regression\nObjective : -2.9419881541130053\nNumber of Parameters : 3\nNumber of Optimization Parameters : 3\nUpdates : True\nParameters:\n  GP_regression.           |                 value  |  constraints  |  priors\n  rbf.variance             |    27.837243180547883  |      +ve      |        \n  rbf.lengthscale          |     2.732180018958835  |      +ve      |        \n  Gaussian_noise.variance  |  0.007573211752763481  |      +ve      |        \n\n\nIt seems that variance close to 28 and length scale close to 2.7 give the optimum objective for the GP\n\n\nPlotting the NLL as a function of variance and lenghtscale\nWe will now plot the NLL obtained from our calculations as a function of variance and lengthscale. For comparing our solution with GPy solution, I will be setting noise variance to be 0.0075\n\nimport numpy as numpy\nx_grid_2, y_grid_2 = numpy.mgrid[0.1:6:0.04, 0.1:4:0.03]\n\nli = np.zeros_like(x_grid_2)\nfor i in range(x_grid_2.shape[0]):\n    for j in range(x_grid_2.shape[1]):\n        li[i, j] = nll(x_grid_2[i, j], y_grid_2[i, j], np.sqrt(.007573211752763481))\n\n\nplt.contourf(x_grid_2, y_grid_2, li)\nplt.gca().set_aspect('equal')\nplt.xlabel(r\"$\\sigma$\")\nplt.ylabel(r\"$l$\")\nplt.colorbar()\nplt.title(r\"NLL ($\\sigma, l$)\")\n\nText(0.5, 1.0, 'NLL ($\\\\sigma, l$)')\n\n\n\n\n\nWe will now try to find the “optimum” \\(\\sigma\\) and lengthscale from this NLL space.\n\nprint(li.min())\naa, bb = np.unravel_index(li.argmin(), li.shape)\nprint(x_grid_2[aa, 0]**2, y_grid_2[bb, 0])\n\n-2.9418973674348727\n28.09 0.1\n\n\nExcellent, it looks like we are pretty close to the optimum NLL as reported by GPy and our parameters learnt are also pretty similar. But, we have not even done a thorough search. We will now be using gradient descent to help us find the optimum set of parameters.\n\n\nGradient descent using autograd\n\nfrom autograd import elementwise_grad as egrad\nfrom autograd import grad\n\n\ngrad_objective = grad(nll, argnum=[0, 1, 2])\n\n\nVisualising the objective as a function of iteration\n\nsigma = 2.\nl = 2.\nnoise = 1.\nlr = 1e-3\nnum_iter = 100\nnll_arr = np.zeros(num_iter)\nfor iteration in range(num_iter):\n    nll_arr[iteration] = nll(sigma, l, noise)\n    del_sigma, del_l, del_noise = grad_objective(sigma, l, noise)\n    sigma = sigma - lr*del_sigma\n    l = l - lr*del_l\n    noise = noise - lr*del_noise\n\n\nprint(sigma**2, l, noise)\n\n5.108812267877177 1.9770216805277476 0.11095385387537618\n\n\n\nplt.plot(nll_arr)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"NLL\")\n\nText(0, 0.5, 'NLL')\n\n\n\n\n\n\n\nApplying gradient descent and visualising the learnt function\n\nsigma = 2.\nl = 2.\nnoise = 1.\nlr = 1e-3\nnum_iter = 100\nnll_arr = np.zeros(num_iter)\nfig, ax = plt.subplots()\nfor iteration in range(num_iter):\n    nll_arr[iteration] = nll(sigma, l, noise)\n    del_sigma, del_l, del_noise = grad_objective(sigma, l, noise)\n    sigma = sigma - lr*del_sigma\n    l = l - lr*del_l\n    noise = noise - lr*del_noise\n    k.lengthscale = l\n    k.variance = sigma**2\n    m = GPy.models.GPRegression(X, Y, k, normalizer=False)\n    m.Gaussian_noise = noise**2\n    m.plot(ax=ax)['dataplot'];\n    plt.ylim((0, 6))\n    plt.title(f\"Iteration: {iteration:04}, Objective :{nll_arr[iteration]}\")\n    plt.savefig(f\"/home/nipunbatra-pc/Desktop/gp_learning/{iteration:04}.png\")\n    plt.cla();\nplt.clf()\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n!convert -delay 20 -loop 0 /home/nipunbatra-pc/Desktop/gp_learning/*.png gp-learning.gif\n\n\nExcellent, we can see the “learning” process over time. Our final objective is comparable to GPy’s objective.\nThere are a few things I have mentioned, yet have not gone into their details and I would encourage you to try those out.\n\nFirst, you should try the gradient descent procedure with restarts. Run with different random initialisations and finally report the parameters which give the optimum likelihood.\nWe assume mean zero prior here. However, we are not processing the data and thus the zero mean assumption is not very well suited to our data. If you reduce the number of data points, you would quickly see the GP prediction to fall close to zero.\n\nThere you go. Till next time!"
  },
  {
    "objectID": "posts/2021-09-01-hello-julia-language.html",
    "href": "posts/2021-09-01-hello-julia-language.html",
    "title": "Linear Regression from scratch in Julia",
    "section": "",
    "text": "x = 1:20; y = 4*x + 8*(0.5.-rand(20));\nplot(x, y, seriestype = :scatter, title = \"Dataset\", xlabel = \"X\", ylabel= \"Y\", label=\"Noisy dataset\", legend = :outertopright)\nplot!(x, 4*x, seriestype = :line, label=\"True relationship\", lw=2 )\n\n\n\n\n\nfunction error(a, b)\n    err = norm(y .- a[1] .- (b[1] .* x))\nend\n\nerror (generic function with 1 method)\n\n\n\na = b = -5:0.1:7\n\n-5.0:0.1:7.0\n\n\n\nerror.([1, 2]', [1, 4])\n\n2×2 Matrix{Float64}:\n 152.259  148.372\n  12.56    15.7808\n\n\n\nz = error.(a', b)\nargmin_b, argmin_a = Tuple(argmin(z))\n\n(90, 54)\n\n\n\nsurface(a, b, z , xlabel=L\"\\theta_0\", ylabel=L\"\\theta_1\", zlabel=L\"Cost~(\\theta_0, \\theta_1)\")\n\n\n\n\n\na[argmin_a], b[argmin_b]\n\n(0.3, 3.9)\n\n\n\ncontourf(a, b, error.(a', b), xlabel=L\"\\theta_0\", ylabel=L\"\\theta_1\", title=\"Contour Plot\")\nplot!([a[argmin_a]], [b[argmin_b]], seriestype=:scatter, label=\"MLE\", markersize=10)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2013-06-01-hmm_simulate.html",
    "href": "posts/2013-06-01-hmm_simulate.html",
    "title": "HMM Simulation for Unfair Casino Problem",
    "section": "",
    "text": "In this notebook we shall create a Hidden Markov Model [1] for the Unfair Casino problem [2]. In short the problem is as follows: In a casino there may be two die- one fair and the other biased. The biased die is much more likely to produce a 6 than the other numbers. With the fair die all the outcomes (1 through 6) are equally likely. For the biased die, probability of observing a 6 is 0.5 and observing 1,2,3,4,5 is 0.1 each. Also, there are probabilies associated with the choice of a die to be thrown. The observer is only able to observe the values of die being thrown, without having a knowldge whether a fair or biased die were used.\nIn all it matches the description of a discrete Hidden Markov Model. The different components of the Discrete HMM are as follows:\nNext, we import the basic set of libraries used for matrix manipulation and for plotting.\nNext, we define the different components of HMM which were described above.\nNow based on these probability sequences we need to produce a sequence of observed and hidden states. We use the notion of weighted sampling, which basically means that terms/states with higher probabilies assigned to them are more likely to be selected/sampled. For example,let us consider the starting state. For this we need to use the pi matrix, since that encodes the likiliness of starting in a particular state. We observe that for starting in Fair state the probability is .667 and twice that of starting in Biased state. Thus, it is much more likely that we start in Fair state. We use Fitness Proportionate Selection [3] to sample states based on weights (probability). For selection of starting state we would proceed as follows:\nWe test the above function by making a call to it 1000 times and then we try to see how many times do we get a 0 (Fair) wrt 1 (Biased), given the pi vector.\nThus, we can see that we get approximately twice the number of Fair states as Biased states which is as expected.\nNext, we write the following functions:\nThus, using these functions and the HMM paramters we decided earlier, we create length 1000 sequence for hidden and observed states.\nNow, we create helper functions to plot the two sequence in a way we can intuitively understand the HMM.\nNow we plot the hidden and observation sequences"
  },
  {
    "objectID": "posts/2013-06-01-hmm_simulate.html#references",
    "href": "posts/2013-06-01-hmm_simulate.html#references",
    "title": "HMM Simulation for Unfair Casino Problem",
    "section": "References",
    "text": "References\n\nhttp://en.wikipedia.org/wiki/Hidden_Markov_model\nhttp://www.stanford.edu/class/stats366/hmmR2.html\nhttp://en.wikipedia.org/wiki/Fitness_proportionate_selection\nhttp://eli.thegreenplace.net/2010/01/22/weighted-random-generation-in-python/\nhttp://stackoverflow.com/questions/2154249/identify-groups-of-continuous-numbers-in-a-list"
  },
  {
    "objectID": "posts/2022-02-05-simple-dgm.html",
    "href": "posts/2022-02-05-simple-dgm.html",
    "title": "Simple Directed Graphical Models in TF Probability",
    "section": "",
    "text": "#import silence_tensorflow.auto\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport functools\nimport seaborn as sns\nimport tensorflow_probability as tfp\nimport pandas as pd\n\ntfd = tfp.distributions\ntfl = tfp.layers\ntfb = tfp.bijectors\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import Callback\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nimport pygraphviz as pgv\n\nA = pgv.AGraph(directed=True)\nA.node_attr[\"style\"] = \"filled\"\nA.add_edge(\"Rain\", \"Sprinkler\", minlen=1, arrowsize=1, directed=True)\nA.layout(\"dot\")\nA.graph_attr.update(size=\"4,4!\")\nA.draw(\"dgm.png\")\n\n\n\nimport tensorflow as tf\ntf.Variable(2.)\n\nMetal device set to: Apple M1\n\n\n2022-02-06 14:56:37.170460: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2022-02-06 14:56:37.170549: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n\n\n\ndef grass_wet_model(rain_prob, rain_to_sprinkler_probs):\n    rain = yield tfp.distributions.JointDistributionCoroutine.Root(\n        tfp.distributions.Bernoulli(probs=rain_prob, name=\"Rain\")\n    )\n\n    sprinkler = yield tfp.distributions.Bernoulli(\n        probs=rain_to_sprinkler_probs[rain], name=\"Sprinkler\"\n    )\n\n\ntheta_rain = tf.constant(0.2)\ntheta_sprinkler = tf.constant([0.8, 0.3])\n\n\nmodel_joint_original = tfp.distributions.JointDistributionCoroutineAutoBatched(\n    lambda: grass_wet_model(theta_rain, theta_sprinkler), name=\"Original\"\n)\n\n\nmodel_joint_original\n\n<tfp.distributions.JointDistributionCoroutineAutoBatched 'Original' batch_shape=[] event_shape=StructTuple(\n  Rain=[],\n  Sprinkler=[]\n) dtype=StructTuple(\n  Rain=int32,\n  Sprinkler=int32\n)>\n\n\n\nmodel_joint_original.sample(10)\n\nWARNING:tensorflow:Note that RandomUniformInt inside pfor op may not give same output as inside a sequential loop.\nWARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter\nWARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\nWARNING:tensorflow:Using a while_loop for converting StridedSlice\nWARNING:tensorflow:Note that RandomUniformInt inside pfor op may not give same output as inside a sequential loop.\nWARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter\nWARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2\n\n\n\ndataset = model_joint_original.sample(500)\n\nNameError: name 'model_joint_original' is not defined\n\n\n\ndataset\n\nStructTuple(\n  Rain=<tf.Tensor: shape=(500,), dtype=int32, numpy=\n    array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n           1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n           0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n           1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n           0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n           0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n           0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n           0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n           1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n           0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n           1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n           0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n           0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], dtype=int32)>,\n  Sprinkler=<tf.Tensor: shape=(500,), dtype=int32, numpy=\n    array([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n           0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n           1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n           0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n           0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n           0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n           0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n           1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n           1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n           1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n           0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n           1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n           1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n           0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n           1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n           0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n           1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n           0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n           1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n           1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0], dtype=int32)>\n)\n\n\n\ndataset.Sprinkler\n\n<tf.Tensor: shape=(500,), dtype=int32, numpy=\narray([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n       0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n       0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n       1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n       1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n       1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0], dtype=int32)>\n\n\n\ntheta_hat_rain = tfp.util.TransformedVariable(\n    0.5, bijector=tfp.bijectors.SoftClip(0.0, 1.0), name=\"theta_hat_rain\"\n)\n\ntheta_hat_sprinkler = tfp.util.TransformedVariable(\n    [0.6, 0.4], bijector=tfp.bijectors.SoftClip(0.0, 1.0), name=\"theta_hat_sprinkler\"\n)\n\n\nmodel_fit = tfp.distributions.JointDistributionCoroutineAutoBatched(\n    lambda: grass_wet_model(theta_hat_rain, theta_hat_sprinkler), name=\"Fit\"\n)\n\nprint(model_fit)\nmodel_fit.trainable_variables\n\ntfp.distributions.JointDistributionCoroutineAutoBatched(\"Fit\", batch_shape=[], event_shape=StructTuple(\n  Rain=[],\n  Sprinkler=[]\n), dtype=StructTuple(\n  Rain=int32,\n  Sprinkler=int32\n))\n\n\n(<tf.Variable 'theta_hat_rain:0' shape=() dtype=float32, numpy=0.65663093>,)\n\n\n\nneg_ll = lambda: -tf.reduce_mean(model_fit.log_prob(dataset))\n\n\ntrace_fn = lambda traceable_quantities: {\n    \"loss\": traceable_quantities.loss,\n    \"theta_hat_rain\": theta_hat_rain,\n    \"theta_hat_sprinkler\": theta_hat_sprinkler,\n}\n\n\ntrace = tfp.math.minimize(\n    loss_fn=neg_ll,\n    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n    num_steps=100,\n    trace_fn=trace_fn,\n)\n\nWARNING:tensorflow:Using a while_loop for converting StridedSlice\nWARNING:tensorflow:Using a while_loop for converting StridedSlice\n\n\n\nplt.plot(trace[\"theta_hat_rain\"])\nplt.plot(trace[\"loss\"])\n\n\n\n\n\nplt.plot(trace[\"theta_hat_sprinkler\"])\n\n\n\n\nReferences\n\nhttps://www.tensorflow.org/probability/examples/Probabilistic_PCA\nhttps://www.youtube.com/watch?v=l2f6Ic6SeqE&list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&index=4\nhttps://jeffpollock9.github.io/almost-always-auto-batched/\nhttps://jeffpollock9.github.io/bayesian-workflow-with-tfp-and-arviz/"
  },
  {
    "objectID": "posts/2022-02-09-pytorch-learn-normal.html",
    "href": "posts/2022-02-09-pytorch-learn-normal.html",
    "title": "Maximum Likelihood Estimation (MLE) for parameters of univariate and multivariate normal distribution in PyTorch",
    "section": "",
    "text": "dist = torch.distributions\n\n\nCreating a 1d normal distribution\n\nuv_normal = dist.Normal(loc=0.0, scale=1.0)\n\n\n\nSampling from the distribution\n\nsamples = uv_normal.sample(sample_shape=[1000])\n\n\nsns.histplot(samples)\nsns.despine()\n\n\n\n\n\nsns.kdeplot(samples, bw_adjust=2)\nsns.despine()\n\n\n\n\n\n\nComputing logprob and prob at a given x\n\nsns.kdeplot(samples, bw_adjust=2)\nplt.axvline(0.5, color=\"k\", linestyle=\"--\")\nlog_pdf_05 = uv_normal.log_prob(torch.Tensor([0.5]))\n\n\npdf_05 = torch.exp(log_pdf_05)\n\n\nplt.title(\n    \"Density at x = 0.5 is {:.2f}\\n Logprob at x = 0.5 is {:.2f}\".format(\n        pdf_05.numpy()[0], log_pdf_05.numpy()[0]\n    )\n)\nsns.despine()\n\n\n\n\n\n\nLearning parameters via MLE\nLet us generate some normally distributed data and see if we can learn the mean.\n\ntrain_data = uv_normal.sample([10000])\n\n\nuv_normal.loc, uv_normal.scale\n\n(tensor(0.), tensor(1.))\n\n\n\ntrain_data.mean(), train_data.std()\n\n(tensor(-0.0174), tensor(1.0049))\n\n\nThe above is the analytical MLE solution\n\nSetting 1: Fixed scale, learning only location\n\nloc = torch.tensor(-10.0, requires_grad=True)\nopt = torch.optim.Adam([loc], lr=0.01)\nfor i in range(3100):\n    to_learn = torch.distributions.Normal(loc=loc, scale=1.0)\n    loss = -torch.sum(to_learn.log_prob(train_data))\n    loss.backward()\n    if i % 500 == 0:\n        print(f\"Iteration: {i}, Loss: {loss.item():0.2f}, Loc: {loc.item():0.2f}\")\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 512500.16, Loc: -10.00\nIteration: 500, Loss: 170413.53, Loc: -5.61\nIteration: 1000, Loss: 47114.50, Loc: -2.58\nIteration: 1500, Loss: 18115.04, Loc: -0.90\nIteration: 2000, Loss: 14446.38, Loc: -0.22\nIteration: 2500, Loss: 14242.16, Loc: -0.05\nIteration: 3000, Loss: 14238.12, Loc: -0.02\n\n\n\nprint(\n    f\"MLE location gradient descent: {loc:0.2f}, MLE location analytical: {train_data.mean().item():0.2f}\"\n)\n\nMLE location gradient descent: -0.02, MLE location analytical: -0.02\n\n\n\n\nSetting 2: Learning location and scale\nAn important difference from the previous code is that we need to use a transformed variable to ensure scale is positive. We do so by using softplus.\n\nloc = torch.tensor(-10.0, requires_grad=True)\nscale = torch.tensor(2.0, requires_grad=True)\n\nopt = torch.optim.Adam([loc, scale], lr=0.01)\nfor i in range(5100):\n    scale_softplus = torch.functional.F.softplus(scale)\n\n    to_learn = torch.distributions.Normal(loc=loc, scale=scale_softplus)\n    loss = -torch.sum(to_learn.log_prob(train_data))\n    loss.backward()\n    if i % 500 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss.item():0.2f}, Loc: {loc.item():0.2f}, Scale: {scale_softplus.item():0.2f}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 127994.02, Loc: -10.00, Scale: 2.13\nIteration: 500, Loss: 37320.10, Loc: -6.86, Scale: 4.15\nIteration: 1000, Loss: 29944.32, Loc: -4.73, Scale: 4.59\nIteration: 1500, Loss: 26326.08, Loc: -2.87, Scale: 4.37\nIteration: 2000, Loss: 22592.90, Loc: -1.19, Scale: 3.46\nIteration: 2500, Loss: 15968.47, Loc: -0.06, Scale: 1.63\nIteration: 3000, Loss: 14237.87, Loc: -0.02, Scale: 1.01\nIteration: 3500, Loss: 14237.87, Loc: -0.02, Scale: 1.00\nIteration: 4000, Loss: 14237.87, Loc: -0.02, Scale: 1.00\nIteration: 4500, Loss: 14237.87, Loc: -0.02, Scale: 1.00\nIteration: 5000, Loss: 14237.87, Loc: -0.02, Scale: 1.00\n\n\n\nprint(\n    f\"MLE loc gradient descent: {loc:0.2f}, MLE loc analytical: {train_data.mean().item():0.2f}\"\n)\nprint(\n    f\"MLE scale gradient descent: {scale_softplus:0.2f}, MLE scale analytical: {train_data.std().item():0.2f}\"\n)\n\nMLE loc gradient descent: -0.02, MLE loc analytical: -0.02\nMLE scale gradient descent: 1.00, MLE scale analytical: 1.00\n\n\n\nmvn = dist.MultivariateNormal(\n    loc=torch.zeros(2), covariance_matrix=torch.tensor([[1.0, 0.5], [0.5, 2.0]])\n)\n\n\nmvn_samples = mvn.sample([1000])\n\n\nsns.kdeplot(\n    x=mvn_samples[:, 0],\n    y=mvn_samples[:, 1],\n    zorder=0,\n    n_levels=10,\n    shade=True,\n    cbar=True,\n    thresh=0.001,\n    cmap=\"viridis\",\n    bw_adjust=5,\n    cbar_kws={\n        \"format\": \"%.3f\",\n    },\n)\n\nplt.gca().set_aspect(\"equal\")\nsns.despine()\n\n\n\n\n\n\nSetting 1: Fixed scale, learning only location\n\nloc = torch.tensor([-10.0, 5.0], requires_grad=True)\nopt = torch.optim.Adam([loc], lr=0.01)\nfor i in range(4100):\n    to_learn = dist.MultivariateNormal(\n        loc=loc, covariance_matrix=torch.tensor([[1.0, 0.5], [0.5, 2.0]])\n    )\n    loss = -torch.sum(to_learn.log_prob(mvn_samples))\n    loss.backward()\n    if i % 500 == 0:\n        print(f\"Iteration: {i}, Loss: {loss.item():0.2f}, Loc: {loc}\")\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 81817.08, Loc: tensor([-10.,   5.], requires_grad=True)\nIteration: 500, Loss: 23362.23, Loc: tensor([-5.6703,  0.9632], requires_grad=True)\nIteration: 1000, Loss: 7120.20, Loc: tensor([-2.7955, -0.8165], requires_grad=True)\nIteration: 1500, Loss: 3807.52, Loc: tensor([-1.1763, -0.8518], requires_grad=True)\nIteration: 2000, Loss: 3180.41, Loc: tensor([-0.4009, -0.3948], requires_grad=True)\nIteration: 2500, Loss: 3093.31, Loc: tensor([-0.0965, -0.1150], requires_grad=True)\nIteration: 3000, Loss: 3087.07, Loc: tensor([-0.0088, -0.0259], requires_grad=True)\nIteration: 3500, Loss: 3086.89, Loc: tensor([ 0.0073, -0.0092], requires_grad=True)\nIteration: 4000, Loss: 3086.88, Loc: tensor([ 0.0090, -0.0075], requires_grad=True)\n\n\n\nloc, mvn_samples.mean(axis=0)\n\n(tensor([ 0.0090, -0.0075], requires_grad=True), tensor([ 0.0090, -0.0074]))\n\n\nWe can see that our approach yields the same results as the analytical MLE\n\n\nSetting 2: Learning scale and location\nWe need to now choose the equivalent of standard deviation in MVN case, this is the Cholesky matrix which should be a lower triangular matrix\n\nloc = torch.tensor([-10.0, 5.0], requires_grad=True)\ntril = torch.autograd.Variable(torch.tril(torch.ones(2, 2)), requires_grad=True)\nopt = torch.optim.Adam([loc, tril], lr=0.01)\n\nfor i in range(8100):\n    to_learn = dist.MultivariateNormal(loc=loc, covariance_matrix=tril @ tril.t())\n    loss = -torch.sum(to_learn.log_prob(mvn_samples))\n    loss.backward()\n    if i % 500 == 0:\n        print(f\"Iteration: {i}, Loss: {loss.item():0.2f}, Loc: {loc}\")\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 166143.42, Loc: tensor([-10.,   5.], requires_grad=True)\nIteration: 500, Loss: 9512.82, Loc: tensor([-7.8985,  3.2943], requires_grad=True)\nIteration: 1000, Loss: 6411.09, Loc: tensor([-6.4121,  2.5011], requires_grad=True)\nIteration: 1500, Loss: 5248.90, Loc: tensor([-5.0754,  1.8893], requires_grad=True)\nIteration: 2000, Loss: 4647.84, Loc: tensor([-3.8380,  1.3627], requires_grad=True)\nIteration: 2500, Loss: 4289.96, Loc: tensor([-2.6974,  0.9030], requires_grad=True)\nIteration: 3000, Loss: 4056.93, Loc: tensor([-1.6831,  0.5176], requires_grad=True)\nIteration: 3500, Loss: 3885.87, Loc: tensor([-0.8539,  0.2273], requires_grad=True)\nIteration: 4000, Loss: 3722.92, Loc: tensor([-0.2879,  0.0543], requires_grad=True)\nIteration: 4500, Loss: 3495.34, Loc: tensor([-0.0310, -0.0046], requires_grad=True)\nIteration: 5000, Loss: 3145.29, Loc: tensor([ 0.0089, -0.0075], requires_grad=True)\nIteration: 5500, Loss: 3080.54, Loc: tensor([ 0.0090, -0.0074], requires_grad=True)\nIteration: 6000, Loss: 3080.53, Loc: tensor([ 0.0090, -0.0074], requires_grad=True)\nIteration: 6500, Loss: 3080.53, Loc: tensor([ 0.0090, -0.0074], requires_grad=True)\nIteration: 7000, Loss: 3080.53, Loc: tensor([ 0.0090, -0.0074], requires_grad=True)\nIteration: 7500, Loss: 3080.53, Loc: tensor([ 0.0090, -0.0074], requires_grad=True)\nIteration: 8000, Loss: 3080.53, Loc: tensor([ 0.0090, -0.0074], requires_grad=True)\n\n\n\nto_learn.loc, to_learn.covariance_matrix\n\n(tensor([ 0.0090, -0.0074], grad_fn=<AsStridedBackward0>),\n tensor([[1.0582, 0.4563],\n         [0.4563, 1.7320]], grad_fn=<ExpandBackward0>))\n\n\n\nmle_loc = mvn_samples.mean(axis=0)\nmle_loc\n\ntensor([ 0.0090, -0.0074])\n\n\n\nmle_covariance = (\n    (mvn_samples - mle_loc).t() @ ((mvn_samples - mle_loc)) / mvn_samples.shape[0]\n)\nmle_covariance\n\ntensor([[1.0582, 0.4563],\n        [0.4563, 1.7320]])\n\n\nWe can see that our gradient based methods parameters match those of the MLE computed analytically.\nReferences\n\nhttps://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian\nhttps://forum.pyro.ai/t/mle-for-normal-distribution-parameters/3861/3\nhttps://ericmjl.github.io/notes/stats-ml/estimating-a-multivariate-gaussians-parameters-by-gradient-descent/"
  },
  {
    "objectID": "posts/2014-06-02-latexify.html",
    "href": "posts/2014-06-02-latexify.html",
    "title": "Latexify Matplotlib",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\n\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\n\nThe following is the latexify function. It allows you to create 2 column or 1 column figures. You may also wish to alter the height or width of the figure. The default settings are good for most cases. You may also change the parameters such as labelsize and fontsize based on your classfile. For this post, I’ll use the following ACM classfile.\n\ndef latexify(fig_width=None, fig_height=None, columns=1):\n    \"\"\"Set up matplotlib's RC params for LaTeX plotting.\n    Call this before plotting a figure.\n\n    Parameters\n    ----------\n    fig_width : float, optional, inches\n    fig_height : float,  optional, inches\n    columns : {1, 2}\n    \"\"\"\n\n    # code adapted from http://www.scipy.org/Cookbook/Matplotlib/LaTeX_Examples\n\n    # Width and max height in inches for IEEE journals taken from\n    # computer.org/cms/Computer.org/Journal%20templates/transactions_art_guide.pdf\n\n    assert(columns in [1,2])\n\n    if fig_width is None:\n        fig_width = 3.39 if columns==1 else 6.9 # width in inches\n\n    if fig_height is None:\n        golden_mean = (sqrt(5)-1.0)/2.0    # Aesthetic ratio\n        fig_height = fig_width*golden_mean # height in inches\n\n    MAX_HEIGHT_INCHES = 8.0\n    if fig_height > MAX_HEIGHT_INCHES:\n        print(\"WARNING: fig_height too large:\" + fig_height + \n              \"so will reduce to\" + MAX_HEIGHT_INCHES + \"inches.\")\n        fig_height = MAX_HEIGHT_INCHES\n\n    params = {'backend': 'ps',\n              'text.latex.preamble': [r'\\usepackage{gensymb}'],\n              'axes.labelsize': 8, # fontsize for x and y labels (was 10)\n              'axes.titlesize': 8,\n              'font.size': 8, # was 10\n              'legend.fontsize': 8, # was 10\n              'xtick.labelsize': 8,\n              'ytick.labelsize': 8,\n              'text.usetex': True,\n              'figure.figsize': [fig_width,fig_height],\n              'font.family': 'serif'\n    }\n\n    matplotlib.rcParams.update(params)\n\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\n%matplotlib inline\n\nLet us create a dummy data frame\n\ndf = pd.DataFrame(np.random.randn(10,2))\ndf.columns = ['Column 1', 'Column 2']\n\n\nax = df.plot()\nax.set_xlabel(\"X label\")\nax.set_ylabel(\"Y label\")\nax.set_title(\"Title\")\nplt.tight_layout()\nplt.savefig(\"image1.pdf\")\n\n\n\n\nNow, let us call the latexify function to alter matplotlib parameters suited to our LaTeX classfile.\n\nlatexify()\n\n\nax = df.plot()\nax.set_xlabel(\"X label\")\nax.set_ylabel(\"Y label\")\nax.set_title(\"Title\")\nplt.tight_layout()\nformat_axes(ax)\nplt.savefig(\"image2.pdf\")\n\n\n\n\nLet us have a quick look at our latex source file. I have scaled down the plot generated by default matploltib settings by 50%. The next plot which is generated using latexified settings doesn’t need any scaling.\n\n! cat 1.tex\n\n\\documentclass{sig-alternate}\n\\title{Python plots for LaTeX}\n\\begin{document}\n\\maketitle\n\\section{Introduction}\nSome random text out here!\n\\noindent\n\n\\begin{figure}\n\\centering \\includegraphics[scale=0.5]{image1.pdf}\n\\caption{Scaled down of the originial matplotlib plot. Now, the text looks very small.}\n\\label{image1}\n\\end{figure}\n\n\\begin{figure}\n\\centering \\includegraphics{image2.pdf}\n\\caption{LaTeXified plot :)}\n\\label{image2}\n\\end{figure}\n\n\n\\end{document}\n\n\nFinally, let us look at the “png” version of our generated pdf.\n\nClearly, the LaTeXified version is likely to save you a lot of figure tweaking! You don’t need to play with different scaling settings. Nor, do you have to play with font sizes and ofcourse not with a combination of these two which can be pretty hard."
  },
  {
    "objectID": "posts/2018-06-16-active-committee.html",
    "href": "posts/2018-06-16-active-committee.html",
    "title": "Active Learning",
    "section": "",
    "text": "Various strategies for active learning have been proposed in the past. In this post, I’ll work out a trivial example of what is called query by committee. The key idea is that we create a committee of learners and choose to acquire labels for the unlabelled points for which there is maximum disaggrement amongst the committee.\nI’d recommend the new readers to go through this survey.\nIn this particular post, I’d be looking at active learning via query by committee, where the committee members are trained on different subsets of the train data. In a future post, I’ll write about active learning via query by committee, where the committee members are trained on the same data, but with different parameters.\n\nStandard imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(0)\n%matplotlib inline\n\n\n\nCreating dataset\n\nX = np.arange(1, 1001, 1)\nY = 10*X + 4 + 400* np.random.randn(1000, ) \n\n\nplt.scatter(X, Y, s=0.1)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n\nText(0, 0.5, 'Y')\n\n\n\n\n\n\n\nLearning a linear regression model on the entire data\n\nfrom sklearn.linear_model import LinearRegression\nclf = LinearRegression()\n\n\nclf.fit(X.reshape(-1,1), Y)\n\nLinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\n\n\nclf.intercept_\n\n-10.370897712972692\n\n\n\nclf.coef_\n\narray([9.99254389])\n\n\n\n\nVisualising the fit\n\nplt.scatter(X, Y, s=0.1)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.plot(X, clf.coef_[0]*X + clf.intercept_, color='k', label='Best fit on all data')\nplt.legend()\nplt.text(500, clf.coef_[0]*500 + clf.intercept_ +4000, \"Y = {0:0.2f} X + {1:0.2f}\".format(clf.coef_[0], clf.intercept_) )\n\nText(500, 8985.90104506115, 'Y = 9.99 X + -10.37')\n\n\n\n\n\n\n\nCreating the initial train set, the test set and the pool\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5)\n\n\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, test_size=495)\n\n\nplt.scatter(train_X, train_Y)\n\n<matplotlib.collections.PathCollection at 0x1a211e9150>\n\n\n\n\n\n\n\nCreating a committee each learnt on different subset of the data\n\ncommittee_size = 5\n\n\ntrain_X_com = {0:{}}\ntrain_Y_com = {0:{}}\nmodels_com = {0:{}}\n\niteration = 0\n\nfor cur_committee in range(committee_size):\n    train_X_com[iteration][cur_committee], _, train_Y_com[iteration][cur_committee], _ = train_test_split(train_X, train_Y, train_size=0.5, \n                                                                              random_state=cur_committee)\n    models_com[iteration][cur_committee] = LinearRegression()\n    models_com[iteration][cur_committee].fit(train_X_com[iteration][cur_committee].reshape(-1,1), train_Y_com[iteration][cur_committee])\n\n\n\nPlotting the fit of the committee on the entire dataset\n\nplt.scatter(X, Y, s=0.2)\nfor cur_committee in range(committee_size):\n    plt.plot(X, models_com[0][cur_committee].coef_[0]*X + models_com[0][cur_committee].intercept_,\n             label='Model {0}\\nY = {1:0.2f} X + {2:0.2f}'.format(cur_committee,\n                                                                 models_com[0][cur_committee].coef_[0],\n                                                                models_com[0][cur_committee].intercept_))\n    plt.legend()\n\n\n\n\n\n\nEvaluate the performance on the test set\n\nestimations_com = {0:{}}\nfor cur_committee in range(committee_size):\n    estimations_com[0][cur_committee] = models_com[0][cur_committee].predict(test_X.reshape(-1, 1))\n\n\ntest_mae_error = {0:(pd.DataFrame(estimations_com[0]).mean(axis=1) - test_Y).abs().mean()}\n\nThe MAE on the test set is:\n\ntest_mae_error[0]\n\n565.8837967341798\n\n\n\n\nActive learning procedure\n\nnum_iterations = 20\npoints_added_x=[]\n\npoints_added_y=[]\n\nprint(\"Iteration, Cost\\n\")\nprint(\"-\"*40)\n\nfor iteration in range(1, num_iterations):\n    # For each committee: making predictions on the pool set based on model learnt in the respective train set \n    estimations_pool = {cur_committee: models_com[iteration-1][cur_committee].predict(pool_X.reshape(-1, 1)) for cur_committee in range(committee_size)}\n    # Finding points from the pool with highest disagreement among the committee - highest standard deviation\n    in_var = pd.DataFrame(estimations_pool).std(axis=1).argmax()\n    \n    to_add_x = pool_X[in_var]\n    to_add_y = pool_Y[in_var]\n    points_added_x.append(to_add_x)\n    points_added_y.append(to_add_y)\n    \n    # For each committee - Adding the point where the committe most disagrees\n    for com in range(committee_size):\n        if iteration not in train_X_com:\n            train_X_com[iteration] = {}\n            train_Y_com[iteration] = {}\n            models_com[iteration] = {}\n        train_X_com[iteration][com] = np.append(train_X_com[iteration-1][com], to_add_x)\n        train_Y_com[iteration][com] = np.append(train_Y_com[iteration-1][com], to_add_y)\n    \n    # Deleting the point from the pool\n    pool_X = np.delete(pool_X, in_var)\n    pool_Y = np.delete(pool_Y, in_var)\n    \n    # Training on the new set for each committee\n    for cur_committee in range(committee_size):\n        models_com[iteration][cur_committee] = LinearRegression()\n        models_com[iteration][cur_committee].fit(train_X_com[iteration][cur_committee].reshape(-1,1), train_Y_com[iteration][cur_committee])\n    \n    estimations_com[iteration] = {}\n    for cur_committee in range(committee_size):\n        estimations_com[iteration][cur_committee] = models_com[iteration][cur_committee].predict(test_X.reshape(-1, 1))\n    test_mae_error[iteration]=(pd.DataFrame(estimations_com[iteration]).mean(axis=1) - test_Y).abs().mean()\n    print(iteration, (test_mae_error[iteration]))\n\nIteration, Cost\n\n----------------------------------------\n1 406.17664898054875\n2 402.9897752715986\n3 348.45182739054235\n4 348.49519515039907\n5 349.04197938475716\n6 348.68188577804807\n7 352.40882668573266\n8 373.60417208279864\n9 377.25044571705723\n10 372.5302143045216\n11 335.30243056115603\n12 336.6073606660666\n13 343.2867837998923\n14 347.0491266373306\n15 349.7464195274436\n16 351.5990833631039\n17 349.21957548034976\n18 338.8765223206476\n19 337.0132510959355\n\n\n\npd.Series(test_mae_error).plot(style='ko-')\nplt.xlim((-0.5, num_iterations+0.5))\nplt.ylabel(\"MAE on test set\")\nplt.xlabel(\"# Points Queried\")\n\nText(0.5, 0, '# Points Queried')\n\n\n\n\n\nAs expected, the error goes down as we increase the number of points queried\n\nfig, ax = plt.subplots()\nimport os\nfrom matplotlib.animation import FuncAnimation\nplt.rcParams['animation.ffmpeg_path'] = os.path.expanduser('/Users/nipun/ffmpeg')\ndef update(iteration):\n    ax.cla()\n    ax.scatter(X, Y, s=0.2)\n    ax.set_title(\"Iteration: {} \\n MAE = {:0.2f}\".format(iteration, test_mae_error[iteration]))\n    for cur_committee in range(committee_size):\n        ax.plot(X, models_com[iteration][cur_committee].coef_[0]*X + models_com[iteration][cur_committee].intercept_,\n             label='Model {0}\\nY = {1:0.2f} X + {2:0.2f}'.format(cur_committee,\n                                                                 models_com[iteration][cur_committee].coef_[0],\n                                                                models_com[iteration][cur_committee].intercept_))\n        \n        ax.scatter(points_added_x[iteration], points_added_y[iteration],s=100, color='red')\n    ax.legend()\n    \n    fig.tight_layout()\n\nanim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations-1, 1), interval=1000)\nplt.close()\n\n\nfrom IPython.display import HTML\nHTML(anim.to_html5_video())\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nFrom the animation, we can see that how adding a new point to the train set (shown in red) reduces the variation in prediction amongst the different committee members."
  },
  {
    "objectID": "posts/2014-05-01-dtw.html",
    "href": "posts/2014-05-01-dtw.html",
    "title": "Programatically understanding dynamic time warping (DTW)",
    "section": "",
    "text": "“In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences which may vary in time or speed. For instance, similarities in walking patterns could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation.”\nIn this post I will try and put forward a naive implementation of DTW and also explain the different pieces of the problem.\n\nCustomary imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\nCreating two signals\n\nx = np.array([1, 1, 2, 3, 2, 0])\ny = np.array([0, 1, 1, 2, 3, 2, 1])\n\n\n\nPlotting the two signals\n\nplt.plot(x,'r', label='x')\nplt.plot(y, 'g', label='y')\nplt.legend();\n\n\n\n\nSo, it appears that both the signals show similar behaviour: they both have a peak and around the peak they slop downwards. They vary in their speed and total duration. So, all set for DTW.\n\n\nAim\nOur aim is to find a mapping between all points of x and y. For instance, x(3) may be mapped to y(4) and so on.\n\n\nMaking a 2d matrix to compute distances between all pairs of x and y\nIn this initial step, we will find out the distance between all pair of points in the two signals. Lesser distances implies that these points may be candidates to be matched together.\n\ndistances = np.zeros((len(y), len(x)))\n\n\ndistances\n\narray([[ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.]])\n\n\nWe will use euclidean distance between the pairs of points.\n\nfor i in range(len(y)):\n    for j in range(len(x)):\n        distances[i,j] = (x[j]-y[i])**2  \n\n\ndistances\n\narray([[ 1.,  1.,  4.,  9.,  4.,  0.],\n       [ 0.,  0.,  1.,  4.,  1.,  1.],\n       [ 0.,  0.,  1.,  4.,  1.,  1.],\n       [ 1.,  1.,  0.,  1.,  0.,  4.],\n       [ 4.,  4.,  1.,  0.,  1.,  9.],\n       [ 1.,  1.,  0.,  1.,  0.,  4.],\n       [ 0.,  0.,  1.,  4.,  1.,  1.]])\n\n\n\n\nVisualizing the distance matrix\nWe will write a small function to visualize the distance matrix we just created.\n\ndef distance_cost_plot(distances):\n    im = plt.imshow(distances, interpolation='nearest', cmap='Reds') \n    plt.gca().invert_yaxis()\n    plt.xlabel(\"X\")\n    plt.ylabel(\"Y\")\n    plt.grid()\n    plt.colorbar();\n\n\ndistance_cost_plot(distances)\n\n\n\n\nFrom the plot above, it seems like the diagonal entries have low distances, which means that the distance between similar index points in x and y is low.\n\n\nWarping path\nIn order to create a mapping between the two signals, we need to create a path in the above plot. The path should start at (0,0) and want to reach (M,N) where (M, N) are the lengths of the two signals. Our aim is to find the path of minimum distance. To do this, we thus build a matrix similar to the distances matrix. This matrix would contain the minimum distances to reach a specific point when starting from (0,0). We impose some restrictions on the paths which we would explore: 1. The path must start at (0,0) and end at (M,N) 2. We cannot go back in time, so the path only flows forwards, which means that from a point (i, j), we can only right (i+1, j) or upwards (i, j+1) or diagonal (i+1, j+1).\nThese restrictions would prevent the combinatorial explosion and convert the problem to a Dynamic Programming problem which can be solved in O(MN) time.\n\naccumulated_cost = np.zeros((len(y), len(x)))\n\n\n\nLet us now build up the accumulated cost\n\nSince we start from (0,0), the accumulated cost at this point is distance(0,0)\n\n\naccumulated_cost[0,0] = distances[0,0]\n\nLets see how accumulated cost looks at this point.\n\ndistance_cost_plot(accumulated_cost)\n\n\n\n\n\nIf we were to move along the first row, i.e. from (0,0) in the right direction only, one step at a time\n\n\nfor i in range(1, len(x)):\n    accumulated_cost[0,i] = distances[0,i] + accumulated_cost[0, i-1]    \n\n\ndistance_cost_plot(accumulated_cost)\n\n\n\n\n\nIf we were to move along the first column, i.e. from (0,0) in the upwards direction only, one step at a time\n\n\nfor i in range(1, len(y)):\n    accumulated_cost[i,0] = distances[i, 0] + accumulated_cost[i-1, 0]    \n\n\ndistance_cost_plot(accumulated_cost)\n\n\n\n\nFor all other elements, we have\n\n\nfor i in range(1, len(y)):\n    for j in range(1, len(x)):\n        accumulated_cost[i, j] = min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]) + distances[i, j]\n\n\ndistance_cost_plot(accumulated_cost)\n\n\n\n\nSo, now we have obtained the matrix containing cost of all paths starting from (0,0). We now need to find the path minimizing the distance which we do by backtracking.\n\n\nBacktracking and finding the optimal warp path\nBacktracking procedure is fairly simple and involves trying to move back from the last point (M, N) and finding which place we would reached there from (by minimizing the cost) and do this in a repetitive fashion.\n\npath = [[len(x)-1, len(y)-1]]\ni = len(y)-1\nj = len(x)-1\nwhile i>0 and j>0:\n    if i==0:\n        j = j - 1\n    elif j==0:\n        i = i - 1\n    else:\n        if accumulated_cost[i-1, j] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n            i = i - 1\n        elif accumulated_cost[i, j-1] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n            j = j-1\n        else:\n            i = i - 1\n            j= j- 1\n    path.append([j, i])\npath.append([0,0])\n\n\npath\n\n[[5, 6], [4, 5], [3, 4], [2, 3], [1, 2], [1, 1], [0, 1], [0, 0]]\n\n\n\npath_x = [point[0] for point in path]\npath_y = [point[1] for point in path]\n\n\ndistance_cost_plot(accumulated_cost)\nplt.plot(path_x, path_y);\n\n\n\n\nThe above plot shows the optimum warping path which minimizes the sum of distance (DTW distance) along the path. Let us wrap up the function by also incorporating the DTW distance between the two signals as well.\n\ndef path_cost(x, y, accumulated_cost, distances):\n    path = [[len(x)-1, len(y)-1]]\n    cost = 0\n    i = len(y)-1\n    j = len(x)-1\n    while i>0 and j>0:\n        if i==0:\n            j = j - 1\n        elif j==0:\n            i = i - 1\n        else:\n            if accumulated_cost[i-1, j] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n                i = i - 1\n            elif accumulated_cost[i, j-1] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n                j = j-1\n            else:\n                i = i - 1\n                j= j- 1\n        path.append([j, i])\n    path.append([0,0])\n    for [y, x] in path:\n        cost = cost +distances[x, y]\n    return path, cost    \n\n\npath, cost = path_cost(x, y, accumulated_cost, distances)\nprint path\nprint cost\n\n[[5, 6], [4, 5], [3, 4], [2, 3], [1, 2], [1, 1], [0, 1], [0, 0]]\n2.0\n\n\nLet us compare our naive implementation with that of mlpy which also provides a DTW implementation.\n\nimport mlpy\n\n\ndist, cost, path = mlpy.dtw_std(x, y, dist_only=False)\n\n\nimport matplotlib.cm as cm\nfig = plt.figure(1)\nax = fig.add_subplot(111)\nplot1 = plt.imshow(cost.T, origin='lower', cmap=cm.gray, interpolation='nearest')\nplot2 = plt.plot(path[0], path[1], 'w')\nxlim = ax.set_xlim((-0.5, cost.shape[0]-0.5))\nylim = ax.set_ylim((-0.5, cost.shape[1]-0.5))\n\n\n\n\nThe path looks almost identical to the one we got. The slight difference is due to the fact that the path chosen by our implementation and the one chosen by DTW have the same distance and thus we woulc choose either.\n\ndist\n\n2.0\n\n\nNot bad! Our implementation gets the same distance between x and y.\nLet us look at another interesting way to visualize the warp. We will place the two signals on the same axis and\n\nplt.plot(x, 'bo-' ,label='x')\nplt.plot(y, 'g^-', label = 'y')\nplt.legend();\npaths = path_cost(x, y, accumulated_cost, distances)[0]\nfor [map_x, map_y] in paths:\n    print map_x, x[map_x], \":\", map_y, y[map_y]\n    \n    plt.plot([map_x, map_y], [x[map_x], y[map_y]], 'r')\n\n5 0 : 6 1\n4 2 : 5 2\n3 3 : 4 3\n2 2 : 3 2\n1 1 : 2 1\n1 1 : 1 1\n0 1 : 1 1\n0 1 : 0 0\n\n\n\n\n\nThe above plot shows the mapping between the two signal. The red lines connect the matched points which are given by the DTW algorithm Looks neat isn’t it? Now, let us try this for some known signal. This example is inspired from the example used in R’s dtw implementation. We will see the DTW path between a sine and cosine on the same angles.\n\nidx = np.linspace(0, 6.28, 100)\n\n\nx = np.sin(idx)\n\n\ny = np.cos(idx)\n\n\ndistances = np.zeros((len(y), len(x)))\n\n\nfor i in range(len(y)):\n    for j in range(len(x)):\n        distances[i,j] = (x[j]-y[i])**2  \n\n\ndistance_cost_plot(distances)\n\n\n\n\n\naccumulated_cost = np.zeros((len(y), len(x)))\naccumulated_cost[0,0] = distances[0,0]\nfor i in range(1, len(y)):\n    accumulated_cost[i,0] = distances[i, 0] + accumulated_cost[i-1, 0]\nfor i in range(1, len(x)):\n    accumulated_cost[0,i] = distances[0,i] + accumulated_cost[0, i-1] \nfor i in range(1, len(y)):\n    for j in range(1, len(x)):\n        accumulated_cost[i, j] = min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]) + distances[i, j]\n\n\nplt.plot(x, 'bo-' ,label='x')\nplt.plot(y, 'g^-', label = 'y')\nplt.legend();\npaths = path_cost(x, y, accumulated_cost, distances)[0]\nfor [map_x, map_y] in paths:\n    #print map_x, x[map_x], \":\", map_y, y[map_y]\n    \n    plt.plot([map_x, map_y], [x[map_x], y[map_y]], 'r')\n\n\n\n\nOk, this does look nice. I am impressed!\n\n\nConclusions\nWe worked out a naive DTW implementation pretty much from scratch. It seems to do reasonably well on artificial data.\n\n\nReferences\n\nWikipedia page on dtw\nR’s dtw package\nmlpy page on dtw\nDTW review paper\n\nFeel free to comment!"
  },
  {
    "objectID": "posts/2022-02-05-lr.html",
    "href": "posts/2022-02-05-lr.html",
    "title": "Linear Regression in TF Probability using JointDistributionCoroutineAutoBatched",
    "section": "",
    "text": "from silence_tensorflow import silence_tensorflow\nsilence_tensorflow()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport functools\nimport seaborn as sns\nimport tensorflow_probability as tfp\nimport pandas as pd\n\ntfd = tfp.distributions\ntfl = tfp.layers\ntfb = tfp.bijectors\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nnp.random.seed(0)\ntf.random.set_seed(0)\n\n\ndef lr(x, stddv_datapoints):\n    num_datapoints, data_dim = x.shape\n    b = yield tfd.Normal(\n        loc=0.0,\n        scale=2.0,\n        name=\"b\",\n    )\n    w = yield tfd.Normal(\n        loc=tf.zeros([data_dim]), scale=2.0 * tf.ones([data_dim]), name=\"w\"\n    )\n\n    y = yield tfd.Normal(\n        loc=tf.linalg.matvec(x, w) + b, scale=stddv_datapoints, name=\"y\"\n    )\n\n\nx = tf.linspace(-5.0, 5.0, 100)\nx = tf.expand_dims(x, 1)\n\n\nstddv_datapoints = 1\n\nconcrete_lr_model = functools.partial(lr, x=x, stddv_datapoints=stddv_datapoints)\n\nmodel = tfd.JointDistributionCoroutineAutoBatched(concrete_lr_model)\n\n\nmodel\n\n<tfp.distributions.JointDistributionCoroutineAutoBatched 'JointDistributionCoroutineAutoBatched' batch_shape=[] event_shape=StructTuple(\n  b=[],\n  w=[1],\n  y=[100]\n) dtype=StructTuple(\n  b=float32,\n  w=float32,\n  y=float32\n)>\n\n\n\nactual_b, actual_w, y_train = model.sample()\n\n\nplt.scatter(x, y_train, s=30, alpha=0.6)\nplt.plot(x, tf.linalg.matvec(x, actual_w) + actual_b, color=\"k\")\nsns.despine()\n\n\n\n\n\ntrace_fn = lambda traceable_quantities: {\n    \"loss\": traceable_quantities.loss,\n    \"w\": w,\n    \"b\": b,\n}\n\n\ndata_dim = 1\nw = tf.Variable(tf.zeros_like(actual_w))\n\nb = tf.Variable(tf.zeros_like(actual_b))\n\ntarget_log_prob_fn = lambda w, b: model.log_prob((b, w, y_train))\ntarget_log_prob_fn\n\n<function __main__.<lambda>(w, b)>\n\n\n\ntrace = tfp.math.minimize(\n    lambda: -target_log_prob_fn(w, b),\n    optimizer=tf.optimizers.Adam(learning_rate=0.05),\n    trace_fn=trace_fn,\n    num_steps=200,\n)\n\n\nw, b, actual_w, actual_b\n\n(<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([2.1811483], dtype=float32)>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0149531>,\n <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.1337605], dtype=float32)>,\n <tf.Tensor: shape=(), dtype=float32, numpy=3.0221252>)\n\n\n\nplt.plot(trace[\"w\"], label=\"w\")\nplt.plot(trace[\"b\"], label=\"b\")\nplt.legend()\nsns.despine()\n\n\n\n\n\nqw_mean = tf.Variable(tf.random.normal([data_dim]))\nqb_mean = tf.Variable(tf.random.normal([1]))\nqw_stddv = tfp.util.TransformedVariable(\n    1e-4 * tf.ones([data_dim]), bijector=tfb.Softplus()\n)\nqb_stddv = tfp.util.TransformedVariable(1e-4 * tf.ones([1]), bijector=tfb.Softplus())\n\n\ndef factored_normal_variational_model():\n    qw = yield tfd.Normal(loc=qw_mean, scale=qw_stddv, name=\"qw\")\n    qb = yield tfd.Normal(loc=qb_mean, scale=qb_stddv, name=\"qb\")\n\n\nsurrogate_posterior = tfd.JointDistributionCoroutineAutoBatched(\n    factored_normal_variational_model\n)\n\nlosses = tfp.vi.fit_surrogate_posterior(\n    target_log_prob_fn,\n    surrogate_posterior=surrogate_posterior,\n    optimizer=tf.optimizers.Adam(learning_rate=0.05),\n    num_steps=200,\n)\n\n/Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/internal/vectorization_util.py:87: UserWarning: Saw Tensor seed Tensor(\"seed:0\", shape=(2,), dtype=int32), implying stateless sampling. Autovectorized functions that use stateless sampling may be quite slow because the current implementation falls back to an explicit loop. This will be fixed in the future. For now, you will likely see better performance from stateful sampling, which you can invoke by passing a Python `int` seed.\n  warnings.warn(\n/Users/nipun/miniforge3/lib/python3.9/site-packages/tensorflow_probability/python/internal/vectorization_util.py:87: UserWarning: Saw Tensor seed Tensor(\"seed:0\", shape=(2,), dtype=int32), implying stateless sampling. Autovectorized functions that use stateless sampling may be quite slow because the current implementation falls back to an explicit loop. This will be fixed in the future. For now, you will likely see better performance from stateful sampling, which you can invoke by passing a Python `int` seed.\n  warnings.warn(\n\n\n\nqw_mean, qw_stddv, qb_mean, qb_stddv\n\n(<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([2.1905935], dtype=float32)>,\n <TransformedVariable: name=softplus, dtype=float32, shape=[1], fn=\"softplus\", numpy=array([0.04352505], dtype=float32)>,\n <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([3.0260112], dtype=float32)>,\n <TransformedVariable: name=softplus, dtype=float32, shape=[1], fn=\"softplus\", numpy=array([0.09258726], dtype=float32)>)\n\n\n\ns_qw, s_qb = surrogate_posterior.sample(500)\n\n\nys = tf.linalg.matvec(x, s_qw) + s_qb\n\n\nx.shape, ys.shape\n\n(TensorShape([100, 1]), TensorShape([500, 100]))\n\n\n\nplt.plot(x, ys.numpy().T, color='k', alpha=0.05);\nplt.scatter(x, y_train, s=30, alpha=0.6)\nsns.despine()\n\n\n\n\nTODO\n\nHow to replace x in lr function from x_train to x_test?\n\nReferences\n\nhttps://www.tensorflow.org/probability/examples/Probabilistic_PCA\nhttps://www.youtube.com/watch?v=l2f6Ic6SeqE&list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&index=4\nhttps://jeffpollock9.github.io/almost-always-auto-batched/\nhttps://jeffpollock9.github.io/bayesian-workflow-with-tfp-and-arviz/"
  },
  {
    "objectID": "posts/2021-06-16-shortcuts-ipad.html",
    "href": "posts/2021-06-16-shortcuts-ipad.html",
    "title": "Some of my shortcuts on the iPad",
    "section": "",
    "text": "Create a simple chart in Charty"
  },
  {
    "objectID": "posts/2020-02-28-xor-relu-vector.html",
    "href": "posts/2020-02-28-xor-relu-vector.html",
    "title": "Learning neural network for XOR",
    "section": "",
    "text": "X = np.array([[0, 0],\n             [0, 1],\n             [1, 0],\n             [1, 1]\n             ])\n\ny = np.array([[0], [1], [1], [0]])\n\n\nX.shape, y.shape\n\n((4, 2), (4, 1))\n\n\n\nN, N_0 = X.shape\nN, N_2 = y.shape\nN_1 = 2\n\n\nW = [np.array([0]), np.array([[1, 1], [1, 1]]), np.array([[1, -2]])]\n\nb = [np.array([0]), np.array([[0], [-1]]), np.array([[0]])]\nB = []\n\n\nA = [X]\nA.extend([None]*(len(W)-1))\nZ = [None]*(len(W))\n\n\ndef relu(z):\n    temp = z.copy()\n    temp[temp<0] = 0\n    return temp\n\ndef sigmoid(z):\n    return 1./(1+np.exp(-z))\n\n\nfor i in range(1, len(W)):\n    Z[i] = A[i-1]@(W[i].T) + b[i].T\n    A[i] =relu(Z[i])\n\n\nA[2]==y\n\narray([[ True],\n       [ True],\n       [ True],\n       [ True]])\n\n\nExcellent, now let us start from random weight initialisations and use backprop to come to our result\n\nshapes = [X.shape[1], 2, 1]\nactivations = ['empty','sigmoid','sigmoid']\n\nactivation_func = {'sigmoid':sigmoid, 'relu':relu}\n\n\nW = [None]*(len(shapes))\nb = [None]*(len(shapes))\n\nnp.random.seed(0)\n# Dummy\nW[0] = np.array([0])\nb[0] = np.array([0])\n\nfor i in range(1, len(shapes)):\n    W[i] = np.random.randn(shapes[i], shapes[i-1])\n    b[i] = np.random.randn(shapes[i], 1)\n    \nZ = [None]*(len(W))\nZ[0] = np.array([0])\n\nA = [X]\nA.extend([None]*(len(W)-1))\n\n\ndef make_plot(iteration, loss, W, b, cmap='PRGn',close=True):\n    h = 100\n    xx, yy = np.meshgrid(np.linspace(-0.1, 1.1, h),\n                             np.linspace(-0.1, 1.1, h))\n    XX = np.c_[xx.ravel(), yy.ravel()]\n    A = [XX]\n    A.extend([None]*(len(W)-1))\n    Z = [None]*(len(W))\n    for i in range(1, len(W)):\n        Z[i] = A[i-1]@(W[i].T) + b[i].T\n        A[i] =sigmoid(Z[i])\n    pred= A[2].reshape(xx.shape)\n    pred[pred>0.5] = 1\n    pred[pred<=0.5] = 0\n    \n    contours = plt.contourf(xx, yy, pred, h , cmap=cmap, alpha=0.2)\n    plt.colorbar()\n    plt.title(f\"Iteration: {iteration}\\n Loss: {loss}\")\n    plt.scatter(X[:, 0], X[:, 1], c= y.flatten(), cmap=cmap, s=200)\n    plt.savefig(f\"/home/nipunbatra-pc/Desktop/xor/{iteration:04}.png\")\n    if close:\n        plt.clf()\n\n\nmake_plot(0, 2.9, W, b, close=False)\n\n\n\n\n\ndef objective(W, b):\n    for i in range(1, len(W)):\n        Z[i] = A[i-1]@(W[i].T) + b[i].T\n        A[i] = activation_func[activations[i]](Z[i])\n    y_hat = A[2]\n    loss = (-y.T@np.log(y_hat) - (1-y).T@np.log(1-y_hat)).squeeze()\n    return loss\n\n\nobjective(W, b)\n\narray(2.9991465)\n\n\n\nfrom autograd import elementwise_grad as egrad\nfrom autograd import grad\n\n\ngrad_objective = grad(objective, argnum=[0, 1])\n\n\n(del_W0_auto, del_W1_auto, del_W2_auto), (del_b0_auto, del_b1_auto, del_b2_auto) =  grad_objective(W, b)\n\n\ndel_W2_auto\n\narray([[0.60353799, 0.35399637]])\n\n\n\ndel_W2_ours = (A[2]-y).T@A[1]\n\n\ndel_W2_ours,del_W2_auto\n\n([[0.60353799 0.35399637]], array([[0.60353799, 0.35399637]]))\n\n\n\ndel_b2_ours = (A[2]-y).sum(axis=0).reshape(-1, 1)\n\n\ndel_b2_ours,del_b2_auto\n\n([[0.6632421]], array([[0.6632421]]))\n\n\n\ndel_A1_ours = (A[2]-y)@W[2]\ndel_Z1_ours  = np.multiply(del_A1_ours, sigmoid(Z[1])*(1-sigmoid(Z[1])))\ndel_W1_ours = del_Z1_ours.T@A[0]\nnp.allclose(del_W1_ours, del_W1_auto)\n\nTrue\n\n\n\ndel_b1_ours = (del_Z1_ours.sum(axis=0)).reshape(-1, 1)\nnp.allclose(del_b1_ours, del_b1_auto)\n\nTrue\n\n\n\nepochs = 140\nalpha =1\nlosses = np.zeros(epochs)\n\nprint_every = 20\n\nW = [None]*(len(shapes))\nb = [None]*(len(shapes))\n\nnp.random.seed(0)\n# Dummy\nW[0] = np.array([0])\nb[0] = np.array([0])\n\nfor i in range(1, len(shapes)):\n    W[i] = np.random.randn(shapes[i], shapes[i-1])\n    b[i] = np.random.randn(shapes[i], 1)\n    \nZ = [None]*(len(W))\nZ[0] = np.array([0])\n\nA = [X]\nA.extend([None]*(len(W)-1))\n\ndel_Z = [None]*(len(W)+1)\ndel_A = [None]*(len(W)+1)\ndel_W = [None]*(len(W))\ndel_b = [None]*(len(W))\n\nfor iteration in range(epochs):\n    \n    for i in range(1, len(W)):\n        Z[i] = A[i-1]@(W[i].T) + b[i].T\n        A[i] = activation_func[activations[i]](Z[i])\n\n    y_hat = A[2]\n    loss = (-y.T@np.log(y_hat) - (1-y).T@np.log(1-y_hat)).squeeze()\n    losses[iteration] = loss\n    if iteration%print_every==0:\n        print(iteration, loss)\n    \n    make_plot(iteration, loss, W, b, close=True)\n        \n    del_A[2] = -np.multiply(y, A[2]) + np.multiply((1-y), (1-A[2]))\n    del_Z[2] = A[2]-y\n    del_W[2] = (A[2]-y).T@A[1]\n    del_b[2] = (del_Z[2].sum(axis=0)).reshape(-1, 1)\n    del_A[1] = del_Z[2]@W[2]\n    del_Z[1]  = np.multiply(del_A[1], sigmoid(Z[1])*(1-sigmoid(Z[1])))\n    del_W[1] = del_Z[1].T@A[0]\n    del_b[1] = (del_Z[1].sum(axis=0)).reshape(-1, 1)\n    \n    for i in range(1, len(shapes)):\n        W[i] = W[i] - alpha*del_W[i]\n        b[i] = b[i] - alpha*del_b[i]\n\n0 2.9991464995409807\n20 2.850067543754094\n40 2.5045921819726082\n60 1.5756597251036364\n80 0.5779054501565161\n100 0.3097308274202594\n120 0.2028529568023768\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\nmake_plot(iteration, loss, W, b, close=False)\n\n\n\n\n\nmake_plot(0, 2.9, W, b, close=False)\n\narray([[0],\n       [1],\n       [1],\n       [0]])\n\n\n\n!convert -delay 20 -loop 0 /home/nipunbatra-pc/Desktop/xor/*.png xor-own.gif"
  },
  {
    "objectID": "posts/2021-05-31-gan.html",
    "href": "posts/2021-05-31-gan.html",
    "title": "A programming introduction to GANs",
    "section": "",
    "text": "This is a post about Generative Adversarial Networks (GANs). This post is very heavily influenced and borrows code from:\n\nVideo from Luis Serrano\nHeavily borrowed code from this article on machine learning mastery\n\nThese folks deserve all the credit! I am writing this post mostly for my learning.\nI’d highly recommend reading the above two mentioned resources."
  },
  {
    "objectID": "posts/2021-05-31-gan.html#goal",
    "href": "posts/2021-05-31-gan.html#goal",
    "title": "A programming introduction to GANs",
    "section": "Goal",
    "text": "Goal\nThe goal of GANs is to generate realistic data, i.e. data with similar statistics as the training data.\nSee below a “generated” face on https://thispersondoesnotexist.com\nRefresh this page to get a new face each time!\nThese are people that do not exist but their faces have been generated using GANs.\n\nfrom IPython.display import HTML, IFrame\nIFrame(\"https://thispersondoesnotexist.com\", 400, 400)"
  },
  {
    "objectID": "posts/2021-05-31-gan.html#overall-block-diagram",
    "href": "posts/2021-05-31-gan.html#overall-block-diagram",
    "title": "A programming introduction to GANs",
    "section": "Overall Block Diagram",
    "text": "Overall Block Diagram\nConceptually, GANs are simple.They have two main components:\n\nA discriminator: that tries to accurately tell generated and real data (from training data) apart\nA generator: that generates data given some random numbers\n\nThe goal of GANs is to use the generator to create realistic data such that the discriminator thinks it is real (coming from the training dataset)\n\nThe two components discriminator and generator are “fighting” where:\n\nthe goal of the discriminator is to tell apart fake (generated) data from true data (from training set) even when the generator is fairly good\nthe goal of the generator is to generate realistics data such that the discriminator thinks it is real data"
  },
  {
    "objectID": "posts/2021-05-31-gan.html#creating-true-distribution",
    "href": "posts/2021-05-31-gan.html#creating-true-distribution",
    "title": "A programming introduction to GANs",
    "section": "Creating “true” distribution",
    "text": "Creating “true” distribution\nLet us now create some data from the true/known distribution. We will be essentially creating a 2x2 matrix (image) as explained in Luis Serrano’s tutorial. The (0, 0) and (1, 1) position will be a high number between 0.8 and 1 whereas the other two positions (0, 1) and (1, 0) have values between 0 and 0.1\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport mediapy as media\n\n%matplotlib inline\nnp.random.seed(40)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport logging\nimport os\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\nlogging.getLogger('tensorflow').setLevel(logging.FATAL)\nimport tensorflow as tf   \ntf.get_logger().setLevel('ERROR')\n\ntf.random.set_seed(42)\n\n\nSIZE = 5000\nfaces = np.vstack((np.random.uniform(0.8, 1, SIZE), \n                   np.random.uniform(0., 0.1, SIZE),\n                  np.random.uniform(0., 0.1, SIZE),\n                  np.random.uniform(0.8, 1, SIZE))).T\nfaces.shape\n\n(5000, 4)\n\n\n\ndef plot_face(f):\n    f_reshape = f.reshape(2, 2)\n    plt.imshow(f_reshape, cmap=\"Greys\")\n\n\ndef plot_faces(faces, subset=1):\n    images = {\n        f'Image={im}': faces[im].reshape(2, 2)\n        for im in range(len(faces))[::subset]\n    }\n    media.show_images(images, border=True, columns=8, height=80, cmap='Greys')\n\nplot_faces(faces, subset=700)\n\n\n\n      \n      Image=0\n      \n      Image=700\n      \n      Image=1400\n      \n      Image=2100\n      \n      Image=2800\n      \n      Image=3500\n      \n      Image=4200\n      \n      Image=4900\n\n\nThe above shows some samples drawn from the true distibution. Let us also now create some random/noisy samples. These samples do not have any relationship between the 4 positions.\n\n# Examples of noisy images\nnoise = np.random.randn(40, 4)\nnoise = np.abs(noise)\nnoise = noise/noise.max()\n\n\nplot_faces(noise)\n\n\n\n      \n      Image=0\n      \n      Image=1\n      \n      Image=2\n      \n      Image=3\n      \n      Image=4\n      \n      Image=5\n      \n      Image=6\n      \n      Image=7\n      \n      Image=8\n      \n      Image=9\n      \n      Image=10\n      \n      Image=11\n      \n      Image=12\n      \n      Image=13\n      \n      Image=14\n      \n      Image=15\n      \n      Image=16\n      \n      Image=17\n      \n      Image=18\n      \n      Image=19\n      \n      Image=20\n      \n      Image=21\n      \n      Image=22\n      \n      Image=23\n      \n      Image=24\n      \n      Image=25\n      \n      Image=26\n      \n      Image=27\n      \n      Image=28\n      \n      Image=29\n      \n      Image=30\n      \n      Image=31\n      \n      Image=32\n      \n      Image=33\n      \n      Image=34\n      \n      Image=35\n      \n      Image=36\n      \n      Image=37\n      \n      Image=38\n      \n      Image=39"
  },
  {
    "objectID": "posts/2021-05-31-gan.html#creating-the-discriminator",
    "href": "posts/2021-05-31-gan.html#creating-the-discriminator",
    "title": "A programming introduction to GANs",
    "section": "Creating the discriminator",
    "text": "Creating the discriminator\nOur discriminator is simple.\n\nIt accepts as input a 4 dimensional input (the 2x2 image)\nIt outputs a single number with sigmoid activation denoting the probability of:\nimage being fake or generated by generator or belonging to class 0\nimage being real or sampled from training dataset or belonging to class 1\nWe use the binary cross entropy loss\n\nTo make the above crystal clear, I’ll use the following gist to draw this NN\n\nfrom draw_nn import draw_neural_net\n\n\nfig = plt.figure(figsize=(4, 4))\nax = fig.gca()\nax.axis('off')\ndraw_neural_net(ax, .1, 0.9, .1, .6, [4, 1])\nplt.tight_layout()\nplt.title(\"Discriminator NN\");\n\n\n\n\n\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.optimizers import Adam\n\ndiscriminator = Sequential([\n  Dense(1,activation='sigmoid', input_shape=(4, )),\n])\n\ndiscriminator._name = \"Discriminator\"\n\ndiscriminator.compile(\n    optimizer=Adam(0.001),\n    loss='binary_crossentropy'\n)\n\n\ndiscriminator.summary()\n\nModel: \"Discriminator\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 1)                 5         \n=================================================================\nTotal params: 5\nTrainable params: 5\nNon-trainable params: 0\n_________________________________________________________________\n\n\nAs expected, the discriminator has 5 parameters (4 weights coming from the 4 inputs to the output node and 1 bias term added). Now, let us create the generator."
  },
  {
    "objectID": "posts/2021-05-31-gan.html#creating-the-generator",
    "href": "posts/2021-05-31-gan.html#creating-the-generator",
    "title": "A programming introduction to GANs",
    "section": "Creating the generator",
    "text": "Creating the generator\nLet us now create the generator model. We create a very simple one\n\nIt accepts as input a single random number\nIt creates a vector of size 4\n\nThe illustration below shows this network. It should be noted that the single random input is an arbitrary choice. We could use any number really!\n\nfig = plt.figure(figsize=(4, 4))\nax = fig.gca()\nax.axis('off')\ndraw_neural_net(ax, .1, 0.9, .1, .6, [1, 4])\nplt.tight_layout()\nplt.title(\"Generator NN\");\n\n\n\n\n\nfrom keras.layers import ReLU\ngenerator = Sequential([\n  Dense(4, input_shape=(1, )),\n   ReLU(max_value=1.0) \n\n])\n\ngenerator._name = \"Generator\"\n\ngenerator.compile(\n    optimizer=Adam(0.001),\n    loss='binary_crossentropy'\n)\n\n\ngenerator.summary()\n\nModel: \"Generator\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 4)                 8         \n_________________________________________________________________\nmodule_wrapper (ModuleWrappe (None, 4)                 0         \n=================================================================\nTotal params: 8\nTrainable params: 8\nNon-trainable params: 0\n_________________________________________________________________\n\n\nWe can verify that the network has 8 parameters (4 weights and one bias value per output node)"
  },
  {
    "objectID": "posts/2021-05-31-gan.html#generating-samples-from-generator",
    "href": "posts/2021-05-31-gan.html#generating-samples-from-generator",
    "title": "A programming introduction to GANs",
    "section": "Generating samples from Generator",
    "text": "Generating samples from Generator\nWe can now use our generator to generate some samples and plot them.\n\ndef gen_fake(n_samples):\n    x_input = np.random.randn(n_samples, 1)\n    X = generator.predict(x_input)\n    y = np.zeros((n_samples, 1))\n    return X, y\n\nAs expected, the samples look random, without any specific pattern and do not resemble the training data as our generator is untrained. Further, it is important to reiterate that the class associated with the fake samples generated from the generator is 0. Thus, we have the line np.zeros((n_samples, 1)) in the code above.\n\nplot_faces(gen_fake(20)[0])\n\n\n\n      \n      Image=0\n      \n      Image=1\n      \n      Image=2\n      \n      Image=3\n      \n      Image=4\n      \n      Image=5\n      \n      Image=6\n      \n      Image=7\n      \n      Image=8\n      \n      Image=9\n      \n      Image=10\n      \n      Image=11\n      \n      Image=12\n      \n      Image=13\n      \n      Image=14\n      \n      Image=15\n      \n      Image=16\n      \n      Image=17\n      \n      Image=18\n      \n      Image=19"
  },
  {
    "objectID": "posts/2021-05-31-gan.html#sampling-from-the-real-train-dataset",
    "href": "posts/2021-05-31-gan.html#sampling-from-the-real-train-dataset",
    "title": "A programming introduction to GANs",
    "section": "Sampling from the Real (Train) Dataset",
    "text": "Sampling from the Real (Train) Dataset\n\ndef gen_real(n_samples):\n    ix = np.random.randint(0, faces.shape[0], n_samples)\n    X = faces[ix]\n    y = np.ones((n_samples, 1))\n    return X, y\n\n\nplot_faces(gen_real(20)[0])\n\n\n\n      \n      Image=0\n      \n      Image=1\n      \n      Image=2\n      \n      Image=3\n      \n      Image=4\n      \n      Image=5\n      \n      Image=6\n      \n      Image=7\n      \n      Image=8\n      \n      Image=9\n      \n      Image=10\n      \n      Image=11\n      \n      Image=12\n      \n      Image=13\n      \n      Image=14\n      \n      Image=15\n      \n      Image=16\n      \n      Image=17\n      \n      Image=18\n      \n      Image=19\n\n\nWe can clearly see the pattern in the images coming from the training dataset."
  },
  {
    "objectID": "posts/2021-05-31-gan.html#training-the-gan",
    "href": "posts/2021-05-31-gan.html#training-the-gan",
    "title": "A programming introduction to GANs",
    "section": "Training the GAN",
    "text": "Training the GAN\nThe block diagram below shows the main idea behind training GANs. The procedure is similar to alternative least squares.\n\n\ndef define_gan(g_model, d_model):\n    d_model.trainable = False\n    model = Sequential()\n    model.add(g_model)\n    model.add(d_model)\n    opt = Adam(lr=0.001)\n    model.compile(loss='binary_crossentropy', optimizer=opt)\n    return model\n\n\ngan_model = define_gan(generator, discriminator)\n\nIt is important to note that we will train two networks:\n\nDiscriminator on\nFake data (class 0)\nReal data (class 1)\nCombined model consisting og Generator + Discriminator (where the Discriminator is is fixed) on\nFake data (class 0) posing as real data (class 1) to the model\n\nThus, we do not train on the generator separately.\n\nsamples_saved = {}\nlosses = {}\nN_ITER = 1000\nSTEP = N_ITER//10\nfor i in range(N_ITER):\n    # Generate some fake data\n    X_fake, y_fake = gen_fake(2)\n    X_real, y_real = gen_real(2)\n    \n    X, y = np.vstack((X_fake, X_real)), np.vstack((y_fake, y_real))\n    \n    # Discriminator \n    d_loss = discriminator.train_on_batch(X, y)\n        \n    # Generator\n    n_samples = 4\n    g_loss= gan_model.train_on_batch(np.random.randn(n_samples, 1), np.ones(n_samples))\n    losses[i] = {'Gen. loss':g_loss, 'Disc. loss':d_loss}\n    \n    # Save 5 samples\n    samples_saved[i]= gen_fake(5)[0]\n    \n    if i%STEP==0:\n        # Save model\n        generator.save(f\"models/gen-{i}\")\n        print(\"\")\n        print(\"Iteration: {}\".format(i))\n        \n        print(\"Discriminator loss: {:0.2f}\".format(d_loss))\n\n\n        print(\"Generator loss: {:0.2f}\".format(g_loss))\n\nWARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n\n\n\nIteration: 0\nDiscriminator loss: 0.61\nGenerator loss: 0.72\n\n\nWARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n\n\n\nIteration: 100\nDiscriminator loss: 0.61\nGenerator loss: 0.75\n\n\nWARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n\n\n\nIteration: 200\nDiscriminator loss: 0.59\nGenerator loss: 0.76\n\n\nWARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n\n\n\nIteration: 300\nDiscriminator loss: 0.57\nGenerator loss: 0.72\n\n\nWARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n\n\n\nIteration: 400\nDiscriminator loss: 0.60\nGenerator loss: 0.77\n\n\nWARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n\n\n\nIteration: 500\nDiscriminator loss: 0.61\nGenerator loss: 0.68\n\n\nWARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n\n\n\nIteration: 600\nDiscriminator loss: 0.64\nGenerator loss: 0.66\n\n\nWARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n\n\n\nIteration: 700\nDiscriminator loss: 0.60\nGenerator loss: 0.71\n\n\nWARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n\n\n\nIteration: 800\nDiscriminator loss: 0.65\nGenerator loss: 0.66\n\n\nWARNING:absl:Found untraced functions such as re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_fn, re_lu_layer_call_fn, re_lu_layer_call_and_return_conditional_losses, re_lu_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n\n\n\nIteration: 900\nDiscriminator loss: 0.70\nGenerator loss: 0.63"
  },
  {
    "objectID": "posts/2021-05-31-gan.html#convergence",
    "href": "posts/2021-05-31-gan.html#convergence",
    "title": "A programming introduction to GANs",
    "section": "Convergence",
    "text": "Convergence\n\nimport pandas as pd\nlosses_df = pd.DataFrame(losses)\nlosses_df.T.plot();\nplt.xlabel(\"Iteration Number\");\n\n\n\n\nYou might epxect that over time the generator loss reduces as it becomes better and correspodingly the discriminator has a harder time!"
  },
  {
    "objectID": "posts/2021-05-31-gan.html#generating-some-fake-images-from-the-trained-generator",
    "href": "posts/2021-05-31-gan.html#generating-some-fake-images-from-the-trained-generator",
    "title": "A programming introduction to GANs",
    "section": "Generating some “fake” images from the trained generator",
    "text": "Generating some “fake” images from the trained generator\n\nplot_faces(gen_fake(20)[0])\n\n\n\n      \n      Image=0\n      \n      Image=1\n      \n      Image=2\n      \n      Image=3\n      \n      Image=4\n      \n      Image=5\n      \n      Image=6\n      \n      Image=7\n      \n      Image=8\n      \n      Image=9\n      \n      Image=10\n      \n      Image=11\n      \n      Image=12\n      \n      Image=13\n      \n      Image=14\n      \n      Image=15\n      \n      Image=16\n      \n      Image=17\n      \n      Image=18\n      \n      Image=19\n\n\nYou could not tell, right! The generator has been trained well!"
  },
  {
    "objectID": "posts/2021-05-31-gan.html#visualising-evolution-of-generator",
    "href": "posts/2021-05-31-gan.html#visualising-evolution-of-generator",
    "title": "A programming introduction to GANs",
    "section": "Visualising evolution of generator",
    "text": "Visualising evolution of generator\nLet us now visualise the evolution of the generator. To do so, we use the already saved generator models at different iterations and feed them the same “random” input.\n\no = {}\nfor i in range(0, N_ITER, STEP):\n    for inp in [0., 0.2, 0.4, 0.6, 1.]:\n        o[f'It:{i}-Inp:{inp}'] = load_model(f\"models/gen-{i}\").predict(np.array([inp])).reshape(2, 2)\n\n\nmedia.show_images(o,  border=True, columns=5, height=80, cmap='Greys')\n\n\n\n      \n      It:0-Inp:0.0\n      \n      It:0-Inp:0.2\n      \n      It:0-Inp:0.4\n      \n      It:0-Inp:0.6\n      \n      It:0-Inp:1.0\n      \n      It:100-Inp:0.0\n      \n      It:100-Inp:0.2\n      \n      It:100-Inp:0.4\n      \n      It:100-Inp:0.6\n      \n      It:100-Inp:1.0\n      \n      It:200-Inp:0.0\n      \n      It:200-Inp:0.2\n      \n      It:200-Inp:0.4\n      \n      It:200-Inp:0.6\n      \n      It:200-Inp:1.0\n      \n      It:300-Inp:0.0\n      \n      It:300-Inp:0.2\n      \n      It:300-Inp:0.4\n      \n      It:300-Inp:0.6\n      \n      It:300-Inp:1.0\n      \n      It:400-Inp:0.0\n      \n      It:400-Inp:0.2\n      \n      It:400-Inp:0.4\n      \n      It:400-Inp:0.6\n      \n      It:400-Inp:1.0\n      \n      It:500-Inp:0.0\n      \n      It:500-Inp:0.2\n      \n      It:500-Inp:0.4\n      \n      It:500-Inp:0.6\n      \n      It:500-Inp:1.0\n      \n      It:600-Inp:0.0\n      \n      It:600-Inp:0.2\n      \n      It:600-Inp:0.4\n      \n      It:600-Inp:0.6\n      \n      It:600-Inp:1.0\n      \n      It:700-Inp:0.0\n      \n      It:700-Inp:0.2\n      \n      It:700-Inp:0.4\n      \n      It:700-Inp:0.6\n      \n      It:700-Inp:1.0\n      \n      It:800-Inp:0.0\n      \n      It:800-Inp:0.2\n      \n      It:800-Inp:0.4\n      \n      It:800-Inp:0.6\n      \n      It:800-Inp:1.0\n      \n      It:900-Inp:0.0\n      \n      It:900-Inp:0.2\n      \n      It:900-Inp:0.4\n      \n      It:900-Inp:0.6\n      \n      It:900-Inp:1.0\n\n\nWe can see above the improvement of the generation over the different iterations and different inputs! That is it for this article. Happing GANning."
  },
  {
    "objectID": "posts/2017-06-14-widgets-matplotlib.html",
    "href": "posts/2017-06-14-widgets-matplotlib.html",
    "title": "Data exploration using widgets in Matplotlib",
    "section": "",
    "text": "In this post, I’ll be looking at a simple Matplotlib widget to sift through the samples and retain the ability to pan and zoom. This post is heavily inspired by Jake Vanderplas’ PyData 2013 Matplotlib tutorial. I would be creating 15 timeseries having recorded daily for an year for illustration purposes.\n\nSetting the backend to TK.\nFor some reasons, it works better than the default OSX one.\n\n%matplotlib tk\n\n\n\nCustomary imports\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sys\n\n\n\nCreating the data\n\n# Fixing the seed for reproducibility\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.randn(365, 15), index=pd.DatetimeIndex(start='2017',freq='D', periods=365))\n\n\ndf.head()[range(5)]\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      2017-01-01\n      1.764052\n      0.400157\n      0.978738\n      2.240893\n      1.867558\n    \n    \n      2017-01-02\n      0.333674\n      1.494079\n      -0.205158\n      0.313068\n      -0.854096\n    \n    \n      2017-01-03\n      0.154947\n      0.378163\n      -0.887786\n      -1.980796\n      -0.347912\n    \n    \n      2017-01-04\n      -0.438074\n      -1.252795\n      0.777490\n      -1.613898\n      -0.212740\n    \n    \n      2017-01-05\n      -0.672460\n      -0.359553\n      -0.813146\n      -1.726283\n      0.177426\n    \n  \n\n\n\n\n\nfig, ax  = plt.subplots()\ndf.plot(ax=ax)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x10afff890>\n\n\nNotice, that since I used %matplotlib TK backend, I don’t see the plot embedded in the notebook. Thus I’ll save the current figure as an image and then link it here.\n\nplt.savefig(\"all_data.png\")\n\n\nThis sure does not look pretty.\n\n\nProposed solution\n\nGreat. It seems to do the intended job. Let us now look at the individual pieces and how we can tie them up.\n\n\nCreating the initial frame\nIn the first frame we would like to plot the data for the first sample.\nfig, ax = plt.subplots()\ndf[0].plot(ax=ax, title=\"Sample number: 0\")\n\n\nCreating the buttons at the bottom\nFirst, we’d want to make space for the button at the bottom and place them there. We can do this as follows:\nfrom matplotlib.widgets import Button\n\nfig.subplots_adjust(bottom=0.2)\n\naxprev = plt.axes([0.7, 0.05, 0.1, 0.075])\naxnext = plt.axes([0.81, 0.05, 0.1, 0.075])\n\nbnext = Button(axnext, '>')\nbprev = Button(axprev, '<')\n\n\nLinking the buttons to functions\nWe’d next want to call some function each time either of the two buttons are pressed. We would also need a notion of currently selected data point. The idea would be that each time, > is pressed, we advance the currently selected point and plot correspondingly.\nWe’d have to define next() and prev() where we increment and decrement the selected data point.\n\nclass Index:\n    data = df\n    selected = 0\n    \n    def next(self, event):\n        self.selected += 1\n        ax.cla()\n        df[self.selected].plot(ax=ax)\n        ax.set_title(\"Sample number: %d\" %self.selected)\n\n    def prev(self, event):\n        self.selected -= 1\n        ax.cla()\n        df[self.selected].plot(ax=ax)\n        ax.set_title(\"Sample number: %d\" %self.selected)\nHere, ax.cla() clears the data for the current data point before drawing for the next one. df[self.selected].plot(ax=ax) plots for the newly selected data. ax.set_title(\"Sample number: %d\" %self.selected) would change the title to reflect the currently used data point.\nWe can link to callback as follows:\ncallback = Index()\n\nbnext.on_clicked(callback.next)\nbprev.on_clicked(callback.prev)\n\n\nEnsuring we do not select data point out of range\nIf you notice, we simply incremented and decremented the selected data point without considering going beyond (0, number of data points). So, we need to change the call back functions to check that we do not go beyond the range. This would require the following changes to next() with the changes to prev() being similar.\ndata_min = 0\ndata_max = data.shape[1]-1\nselected = 0\ndef next(self, event):\n    if self.selected >=self.data_max:\n        self.selected = self.data_max\n        ax.set_title('Last sample reached. Cannot go forwards')\n    else:\n        self.selected += 1\n        ax.cla()\n        df[self.selected].plot(ax=ax)\n        ax.set_title(\"Sample number: %d\" %self.selected)\nThere you go. This was fairly simple and fun to do, and yet can be very helpful!\n\n\nComplete code\n\nfrom matplotlib.widgets import Button\n\nfig, ax = plt.subplots()\nfig.subplots_adjust(bottom=0.2)\n\ndf[0].plot(ax=ax, title=\"Sample number: 0\")\n\nclass Index:\n    data = df\n    data_min = 0\n    data_max = data.shape[1]-1\n    selected = 0\n    def next(self, event):\n        if self.selected >=self.data_max:\n            self.selected = self.data_max\n            ax.set_title('Last sample reached. Cannot go forwards')\n        else:\n            self.selected += 1\n            ax.cla()\n            df[self.selected].plot(ax=ax)\n            ax.set_title(\"Sample number: %d\" %self.selected)\n\n    def prev(self, event):\n        if self.selected <=self.data_min:\n            self.selected = 0\n            ax.set_title('First sample reached. Cannot go backwards')\n        else:\n            self.selected -= 1\n            ax.cla()\n            df[self.selected].plot(ax=ax)\n            ax.set_title(\"Sample number: %d\" %self.selected)\n        \n\ncallback = Index()\naxprev = plt.axes([0.7, 0.05, 0.1, 0.075])\naxnext = plt.axes([0.81, 0.05, 0.1, 0.075])\n\nbnext = Button(axnext, '>')\nbnext.on_clicked(callback.next)\n\nbprev = Button(axprev, '<')\nbprev.on_clicked(callback.prev)\n\n0\n\n\n\n\nAdvanced example\nHere is another slightly more advanced wideget use case in action.\n\nI will just put the code up here and leave the understanding upto the reader as an exercise.\n\nwith pd.HDFStore('temp-store.h5', mode='w') as st:\n\n    # 15 home-> 2 columns, 365 rows (daily one reading)\n    for home in range(15):\n        df = pd.DataFrame(np.random.randn(365, 2), columns=['fridge','microwave'],\n                          index=pd.DatetimeIndex(start='2017',freq='D', periods=365))\n        df = df.abs()\n        st['/home_%d' %home] = df\n\n\nst = pd.HDFStore('temp-store.h5', mode='r')\n\n\nfrom matplotlib.widgets import Button, CheckButtons\n\nfig, ax = plt.subplots()\nfig.subplots_adjust(bottom=0.2)\nfig.subplots_adjust(left=0.2)\n\nhome_0 = st['/home_0']\n\nrax = plt.axes([0.02, 0.4, 0.13, 0.2], aspect='equal')\n\nlabels = tuple(home_0.columns)\nstates = tuple([True]*len(labels))\ncheck = CheckButtons(rax, labels, states)\n\n\nst['/home_0'].plot(ax=ax, title=\"Sample number: 0\").legend(loc=2)\nlines = ax.get_lines()\n\nclass Index:\n    store = st\n    data_min = 0\n    data_max = len(store.keys())-1\n    selected = 0\n    st, la = states, labels\n    states_dict = dict(zip(la, st))\n    def selected_column(self, label):\n        self.states_dict[label] = not self.states_dict[label]\n        self.plot()\n    \n    def plot(self):\n        ax.cla()\n        st['/home_%d' %self.selected].plot(ax=ax, title=\"Sample number: %d\" %self.selected).legend(loc=2)\n        lines = ax.get_lines()\n        for i ,(l, s) in enumerate(self.states_dict.items()):\n            lines[i].set_visible(s)\n        plt.legend(loc=1)\n        \n        \n    def next(self, event):\n        if self.selected >=self.data_max:\n            self.selected = self.data_max\n            ax.set_title('Last sample reached. Cannot go forwards')\n        else:\n            self.selected += 1\n            self.plot()\n            \n\n    def prev(self, event):\n        if self.selected <=self.data_min:\n            self.selected = 0\n            ax.set_title('First sample reached. Cannot go backwards')\n        else:\n            self.selected -= 1\n            self.plot()\n        \n\ncallback = Index()\naxprev = plt.axes([0.7, 0.05, 0.1, 0.075])\naxnext = plt.axes([0.81, 0.05, 0.1, 0.075])\n\nbnext = Button(axnext, '>')\nbnext.on_clicked(callback.next)\n\nbprev = Button(axprev, '<')\nbprev.on_clicked(callback.prev)\n\ncheck.on_clicked(callback.selected_column);"
  },
  {
    "objectID": "posts/2017-04-21-constrained-nmf-cvx.html",
    "href": "posts/2017-04-21-constrained-nmf-cvx.html",
    "title": "Constrained Non-negative matrix factorisation using CVXPY",
    "section": "",
    "text": "Creating a ratings matrix\nWe will now create a matrix where the relationship among items exists.\n\nimport numpy as np\nimport pandas as pd\n\n\nK, N, M = 2, 12, 30\nY_gen = np.random.rand(M, K)\nX_1 = np.random.rand(K, N/2)\n# So that atleast twice as much\nX_2 = 2* X_1 + np.random.rand(K, N/2)\nX_gen = np.hstack([X_2, X_1])\n# Normalizing\nX_gen = X_gen/np.max(X_gen)\n# Creating A (ratings matrix of size M, N)\nA = np.dot(Y_gen, X_gen)\npd.DataFrame(A).head()\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n  \n  \n    \n      0\n      0.732046\n      0.613565\n      0.961128\n      0.920089\n      0.244323\n      0.506472\n      0.280477\n      0.251049\n      0.324418\n      0.378219\n      0.075556\n      0.131750\n    \n    \n      1\n      0.903630\n      0.340956\n      0.784109\n      0.919741\n      0.190856\n      0.433635\n      0.321932\n      0.135134\n      0.290862\n      0.394680\n      0.052976\n      0.081148\n    \n    \n      2\n      0.972145\n      0.576558\n      1.046197\n      1.098279\n      0.261103\n      0.562996\n      0.358574\n      0.233405\n      0.368118\n      0.460967\n      0.077286\n      0.128344\n    \n    \n      3\n      0.292231\n      0.263864\n      0.401968\n      0.377116\n      0.102567\n      0.210890\n      0.113070\n      0.108163\n      0.134489\n      0.154266\n      0.031993\n      0.056299\n    \n    \n      4\n      0.694038\n      0.803459\n      1.125454\n      0.987344\n      0.290605\n      0.582178\n      0.278848\n      0.331075\n      0.365935\n      0.397023\n      0.093088\n      0.168300\n    \n  \n\n\n\n\nWe can see that for each user, the 0th item has higher rating compared to the 5th, 1st more than the 6th and so on. Now, in our alternating least squares implementation, we break down A as Y.X. Here X has dimensions of K, N. To ensure the relationship among the items, we will put contraints on X of the form: X[:, 0] > 2 x X[:, 5] and so on. We will create a simple for loop for the same.\n\ne = \"[\"\nfor a in range(N/2):\n    e+=\"X[:,%d] > 2 * X[:,%d],\" %(a, a+N/2)\ne = e[:-1] + \"]\"\ne\n\n'[X[:,0] > 2 * X[:,6],X[:,1] > 2 * X[:,7],X[:,2] > 2 * X[:,8],X[:,3] > 2 * X[:,9],X[:,4] > 2 * X[:,10],X[:,5] > 2 * X[:,11]]'\n\n\nAs we can see, we now have 6 constraints that we can feed into the optimisation routine. Whenever we learn X in the ALS, we apply these constraint.\n\n\nCVX routine for handling input constraints\n\ndef nmf_features(A, k,  MAX_ITERS=30, input_constraints_X=None, input_constraints_Y = None):\n    import cvxpy as cvx\n    np.random.seed(0)\n\n    # Generate random data matrix A.\n    m, n = A.shape\n    mask = ~np.isnan(A)\n\n    # Initialize Y randomly.\n    Y_init = np.random.rand(m, k)\n    Y = Y_init\n\n    # Perform alternating minimization.\n\n    residual = np.zeros(MAX_ITERS)\n    for iter_num in xrange(1, 1 + MAX_ITERS):\n    \n        # For odd iterations, treat Y constant, optimize over X.\n        if iter_num % 2 == 1:\n            X = cvx.Variable(k, n)\n            constraint = [X >= 0]\n            if input_constraints_X:\n                constraint.extend(eval(input_constraints_X))\n\n        # For even iterations, treat X constant, optimize over Y.\n        else:\n            Y = cvx.Variable(m, k)\n            constraint = [Y >= 0]\n           \n\n        Temp = Y * X\n        error = A[mask] - (Y * X)[mask]\n       \n        \n        obj = cvx.Minimize(cvx.norm(error, 'fro'))\n       \n\n        prob = cvx.Problem(obj, constraint)\n        prob.solve(solver=cvx.SCS)\n\n        if prob.status != cvx.OPTIMAL:\n            pass\n       \n        residual[iter_num - 1] = prob.value\n      \n        if iter_num % 2 == 1:\n            X = X.value\n        else:\n            Y = Y.value\n    return X, Y, residual\n\n\n# Without constraints\nX, Y, r = nmf_features(A, 3, MAX_ITERS=20)\n# With contstraints\nX_c, Y_c, r_c = nmf_features(A, 3, MAX_ITERS=20, input_constraints_X=e)\n\n\npd.DataFrame(X)\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n  \n  \n    \n      0\n      0.749994\n      0.112355\n      0.485850\n      0.674801\n      0.113004\n      0.281371\n      0.257239\n      0.04056\n      0.196474\n      0.297978\n      0.02745\n      0.033952\n    \n    \n      1\n      0.102384\n      0.222149\n      0.266055\n      0.199361\n      0.070403\n      0.133510\n      0.047174\n      0.09233\n      0.081233\n      0.076518\n      0.02375\n      0.045097\n    \n    \n      2\n      0.567213\n      0.558638\n      0.825066\n      0.756059\n      0.211427\n      0.430690\n      0.222174\n      0.22944\n      0.273260\n      0.307475\n      0.06659\n      0.118371\n    \n  \n\n\n\n\n\npd.DataFrame(X_c)\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n  \n  \n    \n      0\n      0.749882\n      0.112384\n      0.485923\n      0.674778\n      0.113027\n      0.281399\n      0.257206\n      0.040566\n      0.196489\n      0.297960\n      0.027461\n      0.033971\n    \n    \n      1\n      0.102366\n      0.222080\n      0.266058\n      0.199353\n      0.070404\n      0.133511\n      0.047168\n      0.092298\n      0.081233\n      0.076513\n      0.023751\n      0.045091\n    \n    \n      2\n      0.567363\n      0.558700\n      0.825253\n      0.756242\n      0.211473\n      0.430789\n      0.222234\n      0.229470\n      0.273319\n      0.307549\n      0.066604\n      0.118382\n    \n  \n\n\n\n\nOk. The obtained X matrix looks fairly similar. How about we reverse the constraints.\n\ne_rev = \"[\"\nfor a in range(N/2):\n    e_rev+=\" 2* X[:,%d]  < X[:,%d],\" %(a, a+N/2)\ne_rev = e_rev[:-1] + \"]\"\ne_rev\n\n'[ 2* X[:,0]  < X[:,6], 2* X[:,1]  < X[:,7], 2* X[:,2]  < X[:,8], 2* X[:,3]  < X[:,9], 2* X[:,4]  < X[:,10], 2* X[:,5]  < X[:,11]]'\n\n\n\nX_c_rev, Y_c_rev, r_c_rev = nmf_features(A, 3, MAX_ITERS=20, input_constraints_X=e_rev)\n\n\npd.DataFrame(X_c_rev)\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n  \n  \n    \n      0\n      0.250945\n      0.038070\n      0.174189\n      0.252085\n      0.033251\n      0.069176\n      0.502026\n      0.076147\n      0.348450\n      0.504277\n      0.066521\n      0.138405\n    \n    \n      1\n      0.030757\n      0.088033\n      0.085947\n      0.065135\n      0.024395\n      0.045976\n      0.061398\n      0.176002\n      0.171773\n      0.130146\n      0.048760\n      0.091882\n    \n    \n      2\n      0.220256\n      0.183292\n      0.269014\n      0.282814\n      0.065713\n      0.128120\n      0.440553\n      0.366600\n      0.538065\n      0.565669\n      0.131436\n      0.256263\n    \n  \n\n\n\n\nThere you go! We now have learnt latent factors that conform to our constraints."
  },
  {
    "objectID": "posts/2021-06-19-blur-affinity.html",
    "href": "posts/2021-06-19-blur-affinity.html",
    "title": "Blurring an image selectively using Affinity Photo",
    "section": "",
    "text": "convert -append Image-1.png blurred.jpg  -resize x500x new_image_conbined.png"
  },
  {
    "objectID": "posts/2014-06-01-em.html",
    "href": "posts/2014-06-01-em.html",
    "title": "Programatically understanding Expectation Maximization",
    "section": "",
    "text": "Maximum Likelihood\nWe are given two coins- A and B. Both these coins have a certain probability of getting heads. We choose one of the coin at random (with equal probability) and toss it 10 times noting down the heads-tails pattern. We also carefully account which coin was thrown. We repeat this procedure 5 times. The coin tosses observed in this case are show in the figure below in case A.\nOur aim is to determine the probability of getting a head on coin A and likewise for coin B. Intuitively, if we add up the number of heads observed when A was thrown and divide it by the total number of times A was tossed, we woud get this number. This comes from the well known principle of Maximum Likelihood.\n\nThis procedure of tossing a coin which may land as either heads or tails is an example of a Bernoulli trial. As per this Wiki page, its definition is as follows:\n\nIn the theory of probability and statistics, a Bernoulli trial (or binomial trial) is a random experiment with exactly two possible outcomes, “success” and “failure”, in which the probability of success is the same every time the experiment is conducted\n\nWhen \\(n\\) such trials are performed, it is called a binomial experiment. In the case of the coin toss experiment, if we have:\n\n\\(n\\) coin toss\n\\(p\\) probability of head in each trial -> \\(1-p\\) probability of head in each throw\n\nthen we observe \\(k\\) heads as per the following:\n\\[{n\\choose{k}} p^k(1-p)^{n-k}\\]\nLet us write some code to see how this function varies. We fix \\(n\\)=10 and for varying \\(p\\) and observe how the probability distirbution(pmf) varies.\n\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom ipywidgets import StaticInteract, RangeWidget\n%matplotlib inline\n\n\na=range(11)\ndef plot_binomial(p=0.5):\n    fig, ax = plt.subplots(figsize=(4,3))\n    y = [0]*11\n    for i in a:\n        y[i-1] =  stats.binom.pmf(i, 10, p)\n    ax.bar(a,y,label=\"$p = %.1f$\" % p)\n    ax.set_ylabel('PMF of $k$ heads')\n    ax.set_xlabel('$k$')\n    ax.set_ylim((0,0.5))\n    ax.set_xlim((0,10))\n    ax.legend()\n    return fig\n\n\nStaticInteract(plot_binomial, p=RangeWidget(0.0,1.0,0.1))\n\n\n    \n    \n    \n      \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n      p: \n    \n    \n\n\nAs expected, as we increase \\(p\\), there is a higher probability of getting more heads. Now, let us look at the Maximum likelihood problem. Say, we are given that we made \\(n\\) coin tosses and obtained \\(x\\) heads. Using this information and the nature of distribution, let us find what value of \\(p\\) would have most likely resulted in this data.\nIf we choose the probability of heads to be \\(p\\), the likelihood (probability) of obtaining \\(x\\) heads in \\(n\\) throws in our data $D $ is: \\[L(D|\\theta) \\propto p^x(1-p)^{n-x}\\] If we differentiate this term wrt \\(p\\) and equate to 0, we get: \\[ xp^{x-1}(1-p)^{n-x} - p^x (n-x)(1-p)^{n-x-1} = 0\\] or \\[ p^{x-1}(1-p)^{n-x-1}[x(1-p) - p(n-x)]= 0\\] or \\[ x - xp - pn + xp = 0\\] or \\[ p = \\frac{x}{n}\\]\nThis also generalizes over when multiple sets of throws are done. Thus, in the figure shown above, we find the probability of head for coins A and coins B, given the data, as : \\[ P (A=head|D) = \\frac{\\mathrm{heads~observed~when~A~was~thrown}}{\\mathrm{times~A~was~thrown}}\\] \\[ = \\frac{24}{24+6} = 0.8\\] Similarly, for coin B, we can find this ratio as 0.45.\n\n\nProblem description\nNow, we come to the main problem. What if we didn’t note down which coin was thrown in which iteration. Can we still find out the probabilities of heads for the different coins. It turns out that we can use the EM algorithm for this task. I would recommend reading the theory in the nature paper before resuming this section. Now, we present the programming route to EM and its different components under this setting.\n\n\nCreating the dataset\nA head is labeled as a 1 and a tail as a 0. The five rows correspond to the five set of throws.\n\nobservations = np.array([[1,0,0,0,1,1,0,1,0,1],\n                         [1,1,1,1,0,1,1,1,1,1],\n                         [1,0,1,1,1,1,1,0,1,1],\n                         [1,0,1,0,0,0,1,1,0,0],\n                         [0,1,1,1,0,1,1,1,0,1]])\n\nThe true coin choice- A =True, B= False\n\ncoins_id = np.array([False,True,True,False,True])\n\n\n\nCompletely observed case\nAs discussed before, if we know which coin was used when, this task reduces to Maximum likelihood.\nThe sets of observations corresponding to coin A can be found as:\n\nobservations[coins_id]\n\narray([[1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n       [1, 0, 1, 1, 1, 1, 1, 0, 1, 1],\n       [0, 1, 1, 1, 0, 1, 1, 1, 0, 1]])\n\n\nNumber of heads recorded when A was thrown\n\nnp.sum(observations[coins_id])\n\n24\n\n\nThus, the probability of head for A given the data would be:\n\n1.0*np.sum(observations[coins_id])/observations[coins_id].size\n\n0.8\n\n\nThe same quantity for coin B would be the following:\n\n1.0*np.sum(observations[~coins_id])/observations[~coins_id].size\n\n0.45\n\n\n\n\nUnseen data settings\nNow, we follow the Step b in the first figure on this page (which is Figure 1 from the Nature article)\n\nStep 0: Assuming initial set of parameters\n\\[\\theta_A^0 = 0.6\\] \\[\\theta_B^0 = 0.5\\]\n\n\nIteration 1, Step 1 : E-step\nLet us take the first group of observation. We have 5 Heads and 5 tails. PMF according to binomial distribution is given by : \\({n\\choose{k}}p^k(1-p)^{n-k}\\). For coin A we have \\(p=\\theta^0_A=0.6\\) and \\(n=10\\). We calculate the \\(pmf\\) as follows:\n\ncoin_A_pmf_observation_1 = stats.binom.pmf(5,10,0.6)\n\n\ncoin_A_pmf_observation_1\n\n0.20065812480000034\n\n\nSimilarly, for coin B, we get\n\ncoin_B_pmf_observation_1 = stats.binom.pmf(5,10,0.5)\n\n\ncoin_B_pmf_observation_1\n\n0.24609375000000025\n\n\nSince coin_B_pmf_observation_1 is greater than coin_A_pmf_observation_1, coin B is more likely to have produced the sequence of 5 Heads and 5 Tails. We now normalize these \\(pmfs\\) to 1 and \\(weigh\\) our observations.\n\nnormalized_coin_A_pmf_observation_1 = coin_A_pmf_observation_1/(coin_A_pmf_observation_1+coin_B_pmf_observation_1)\nprint \"%0.2f\" %normalized_coin_A_pmf_observation_1\nnormalized_coin_B_pmf_observation_1 = coin_B_pmf_observation_1/(coin_A_pmf_observation_1+coin_B_pmf_observation_1)\nprint \"%0.2f\" %normalized_coin_B_pmf_observation_1\n\n0.45\n0.55\n\n\nWe now weigh in the observations according to this ratio. Thus, for observation set 1, we have:\n\nweighted_heads_A_obervation_1 = 5*normalized_coin_A_pmf_observation_1\nprint \"Coin A Weighted count for heads in observation 1: %0.2f\" %weighted_heads_A_obervation_1\nweighted_tails_A_obervation_1 = 5*normalized_coin_A_pmf_observation_1\nprint \"Coin A Weighted count for tails in observation 1: %0.2f\" %weighted_tails_A_obervation_1\nweighted_heads_B_obervation_1 = 5*normalized_coin_B_pmf_observation_1\nprint \"Coin B Weighted count for heads in observation 1: %0.2f\" %weighted_heads_B_obervation_1\nweighted_tails_B_obervation_1 = 5*normalized_coin_B_pmf_observation_1\nprint \"Coin B Weighted count for tails in observation 1: %0.2f\" %weighted_tails_B_obervation_1\n\nCoin A Weighted count for heads in observation 1: 2.25\nCoin A Weighted count for tails in observation 1: 2.25\nCoin B Weighted count for heads in observation 1: 2.75\nCoin B Weighted count for tails in observation 1: 2.75\n\n\nWe can similarly find out the weigted count in each observation set for both coin A and B. For now, we will pick up the numbers from the paper.\n\n\nIteration 1, Step 2: M-step\nWe now take the sum of weighted count of heads for both coin A and B. For coin A, we have 21.3 heads and 8.6 tails. Thus, we get the probability of getting a head at the end of this iteration as:\n\n21.3/(21.3+8.6)\n\n0.7123745819397994\n\n\nWe can find the same quantity for coin B as well. Next, we repeat the same procedure using these latest calculated probabilities of obtaining heads for coins A and B.\n\n\n\nEM single iteration\nLet us now write a procedure to do a single iteration of the EM algorithm. The function takes in as argument the following:\n\npriors \\(\\theta_A\\) and \\(\\theta_B\\)\nobservation matrix (5 X 10) in this case\n\nand outputs the new set of priors based on EM iteration.\n\ndef em_single(priors, observations):\n    \"\"\"\n    Performs a single EM step\n    Arguments\n    ---------\n    priors : [theta_A, theta_B]\n    observations : [m X n matrix]\n    \n    Returns\n    --------\n    new_priors: [new_theta_A, new_theta_B]\n    \"\"\"\n    counts = {'A':{'H':0,'T':0}, 'B':{'H':0,'T':0}}\n    theta_A = priors[0]\n    theta_B = priors[1]\n    # E step\n    for observation in observations: \n        len_observation = len(observation)\n        num_heads = observation.sum()\n        num_tails = len_observation - num_heads\n        contribution_A = stats.binom.pmf(num_heads,len_observation,theta_A)\n        contribution_B = stats.binom.pmf(num_heads,len_observation,theta_B)\n        weight_A = contribution_A/(contribution_A+contribution_B)\n        weight_B = contribution_B/(contribution_A+contribution_B)\n        # Incrementing counts\n        counts['A']['H']+= weight_A*num_heads\n        counts['A']['T']+= weight_A*num_tails\n        counts['B']['H']+= weight_B*num_heads\n        counts['B']['T']+= weight_B*num_tails\n    # M step\n    new_theta_A = counts['A']['H']/(counts['A']['H']+counts['A']['T'])\n    new_theta_B = counts['B']['H']/(counts['B']['H']+counts['B']['T'])\n    return [new_theta_A, new_theta_B]\n\n\n\nEM procedure\nThis procedure calls the single EM iteration untill convergence or some stopping condition. We specificy two stopping conditions and the procedure stops when either condition is met. * 10000 iterations * the change in prior is less than 1e-6\n\ndef em(observations, prior, tol=1e-6, iterations=10000):\n    import math\n    iteration = 0\n    while iteration<iterations:\n        new_prior = em_single(prior, observations)\n        delta_change = np.abs(prior[0]-new_prior[0])\n        if delta_change<tol:\n            break\n        else:\n            prior = new_prior\n            iteration+=1\n    return [new_prior, iteration]\n\n\n\nResults\nLet us run the algorithm for the priors used in the paper\n\nem(observations, [0.6,0.5])\n\n[[0.79678875938310978, 0.51958393567528027], 14]\n\n\nGreat! Our results match exactly! It took 14 iterations of the EM algorithm to reach this value.\nWhat if we reverse the priors for A and B\n\nem(observations, [0.5,0.6])\n\n[[0.51958345063012845, 0.79678895444393927], 15]\n\n\nOk. EM does not have a notion of A and B!! For it, there exists two coins and it agains finds the same results.\nWhat if prior for both A and B were equal?\n\nem(observations, [0.3,0.3])\n\n[[0.66000000000000003, 0.66000000000000003], 1]\n\n\nSo, this clearly is not a very good initialization strategy!!\nWhat if one of the priors is very close to 1\n\nem(observations, [0.9999,0.00000001])\n\n[[0.79678850504581944, 0.51958235686544463], 13]\n\n\nSo EM is still smart enough!\n\n\nReferences\n\nQuestion on math stack exchange\nAnother question on math stack exhange\n\nIf you have any suggestions feel free let me know."
  },
  {
    "objectID": "posts/2017-08-12-linear-regression-adagrad-vs-gd.html",
    "href": "posts/2017-08-12-linear-regression-adagrad-vs-gd.html",
    "title": "Programatically understanding Adagrad",
    "section": "",
    "text": "It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data.\n\nAs I’d done previously, I’ll be using Autograd to compute the gradients. Please note Autograd and not Adagrad!\n\nFormulation ([borrowed from here])((http://ruder.io/optimizing-gradient-descent/)))\nIn regular gradient descent, we would update the \\(i^{th}\\) parameter in the \\(t+1^{th}\\) iteration, given the learning rate \\(\\eta\\), where \\(g_{t, i}\\) represents the gradient of the cost wrt \\(i^{th}\\) param at time \\(t\\).\n\\[ \\theta_{t+1, i} = \\theta_{t, i} - \\eta \\cdot g_{t, i}  \\tag{Eq 1} \\]\nIn Adagrad, we update as follows:\n\\[\\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} \\cdot g_{t, i} \\tag{Eq 2}\\]\nHere,\n\\(G_{t} \\in \\mathbb{R}^{d \\times d}\\) is a diagonal matrix where each diagonal element \\(i, i\\) is the sum of the squares of the gradients w.r.t. \\(\\theta_i\\) up to time step \\(t\\) , while \\(\\epsilon\\) is a smoothing term that avoids division by zero (usually on the order of 1e−8).\n\n\nCustomary imports\n\nimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n\n\n\nTrue model\n\\[Y = 10 X + 6\\]\n\n\nGenerating data\n\nnp.random.seed(0)\nn_samples = 50\nX = np.linspace(1, 50, n_samples)\nY = 10*X + 6 + 2*np.random.randn(n_samples)\n\n\nplt.plot(X, Y, 'k.')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\");\n\n\n\n\n\n\nModel to be learnt\nWe want to learn W and b such that:\n\\[Y = 10 W+ b\\]\n\n\nDefining the cost function\nWe will now write a general cost function that accepts a list of parameters.\n\ndef cost(param_list):\n    w, b = param_list\n    pred = w*X+b\n    return np.sqrt(((pred - Y) ** 2).mean(axis=None))/(2*len(Y))\n\n\n\nDry run of cost and gradient functioning\n\n# Cost of w=0, b=0\nw, b = 0., 0.\nprint(\"Cost at w={}, b={} is: {}\".format(w, b, cost([w, b])))\n\n# Cost of w=10, b=4. Should be lower than w=0, b=0\nw, b = 10., 4.\nprint(\"Cost at w={}, b={} is: {}\".format(w, b, cost([w, b])))\n\n# Computing the gradient at w=0, b=0\nfrom autograd import grad\ngrad_cost =grad(cost)\nw, b = 0., 0.\nprint(\"Gradient at w={}, b={} is: {}\".format(w, b, grad_cost([w, b])))\n\n# Computing the gradient at w=10, b=4. We would expect it to be smaller than at 0, 0\nw, b = 10., 4.\nprint(\"Gradient at w={}, b={} is: {}\".format(w, b, grad_cost([w, b])))\n\nCost at w=0.0, b=0.0 is: 2.98090446495\nCost at w=10.0, b=4.0 is: 0.0320479471939\nGradient at w=0.0, b=0.0 is: [array(-0.29297046699711365), array(-0.008765162440358071)]\nGradient at w=10.0, b=4.0 is: [array(-0.14406455246023858), array(-0.007117830452061141)]\n\n\n\n\nAdagrad algorithm (applied on whole data batch)\n\ndef adagrad_gd(param_init, cost, niter=5, lr=1e-2, eps=1e-8, random_seed=0):\n    \"\"\"\n    param_init: List of initial values of parameters\n    cost: cost function\n    niter: Number of iterations to run\n    lr: Learning rate\n    eps: Fudge factor, to avoid division by zero\n    \"\"\"\n    from copy import deepcopy\n    import math\n    # Fixing the random_seed\n    np.random.seed(random_seed)\n    \n    # Function to compute the gradient of the cost function\n    grad_cost = grad(cost)\n    params = deepcopy(param_init)\n    param_array, grad_array, lr_array, cost_array = [params], [], [[lr for _ in params]], [cost(params)]\n    # Initialising sum of squares of gradients for each param as 0\n    sum_squares_gradients = [np.zeros_like(param) for param in params]\n    for i in range(niter):\n        out_params = []\n        gradients = grad_cost(params)\n        # At each iteration, we add the square of the gradients to `sum_squares_gradients`\n        sum_squares_gradients= [eps + sum_prev + np.square(g) for sum_prev, g in zip(sum_squares_gradients, gradients)]\n        # Adapted learning rate for parameter list\n        lrs = [np.divide(lr, np.sqrt(sg)) for sg in sum_squares_gradients]\n        # Paramter update\n        params = [param-(adapted_lr*grad_param) for param, adapted_lr, grad_param in zip(params, lrs, gradients)]\n        param_array.append(params)\n        lr_array.append(lrs)\n        grad_array.append(gradients)\n        cost_array.append(cost(params))\n        \n    return params, param_array, grad_array, lr_array, cost_array\n\n\n\nExperiment time!\n\nEvolution of learning rates for W and b\nLet us see how the learning rate for W and b will evolve over time. I will fix the initial learning rate to 0.01 as mot of the Adagrad literature out there seems to suggest.\n\n# Fixing the random seed for reproducible init params for `W` and `b`\nnp.random.seed(0)\nparam_init = [np.random.randn(), np.random.randn()]\nlr = 0.01\neps=1e-8\nniter=1000\nada_params, ada_param_array, ada_grad_array, ada_lr_array, ada_cost_array = adagrad_gd(param_init, cost, niter=niter, lr=lr, eps=eps)\n\nLet us first see the evolution of cost wrt time\n\npd.Series(ada_cost_array, name='Cost').plot(title='Adagrad: Cost v/s # Iterations')\nplt.ylabel(\"Cost\")\nplt.xlabel(\"# Iterations\");\n\n\n\n\nOk. While There seems to be a drop in the cost, the converegence will be very slow. Remember that we had earlier found\n\nCost at w=10.0, b=4.0 is: 0.0320479471939\n\nI’m sure this means that our parameter estimates are similar to the initial parameters and far from the true parameters. Let’s just confirm the same.\n\nprint(\"After {} iterations, learnt `W` = {} and learnt `b` = {}\".format(niter, *ada_params))\n\nAfter 1000 iterations, learnt `W` = 2.38206194526 and learnt `b` = 1.01811878873\n\n\nI would suspect that the learning rate, courtesy of the adaptive nature is falling very rapidly! How would the vanilla gradient descent have done starting with the same learning rate and initial values? My hunch is it would do better. Let’s confirm!\n\n\nGD vs Adagrad!\n\ndef gd(param_init, cost,  niter=5, lr=0.01, random_seed=0):\n    np.random.seed(random_seed)\n    from copy import deepcopy\n    grad_cost = grad(cost)\n    params = deepcopy(param_init)\n    param_array, grad_array, cost_array = [params], [], [cost(params)]\n    for i in range(niter):\n        out_params = []\n        gradients = grad_cost(params)\n        params = [param-lr*grad_param for param, grad_param in zip(params, gradients)]\n        param_array.append(params)\n        grad_array.append(gradients)\n        cost_array.append(cost(params))\n    return params, param_array, grad_array, cost_array\n\n\n# Fixing the random seed for reproducible init params for `W` and `b`\nnp.random.seed(0)\nparam_init = [np.random.randn(), np.random.randn()]\nlr = 0.01\nniter=1000\ngd_params, gd_param_array, gd_grad_array, gd_cost = gd(param_init, cost, niter=niter, lr=lr)\n\n\npd.Series(ada_cost_array, name='Cost').plot(label='Adagrad')\npd.Series(gd_cost, name='Cost').plot(label='GD')\nplt.ylabel(\"Cost\")\nplt.xlabel(\"# Iterations\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x1153b4ad0>\n\n\n\n\n\nOk. So, indeed with learning rate of 0.01, gradient descent fares better. Let’s just confirm that for Adagrad, the learning rates diminish rapidly leading to little reduction in cost!\n\npd.DataFrame(np.array(ada_lr_array), columns=['LR for W', 'LR for b'])[::50].plot(subplots=True, marker='o')\nplt.xlabel(\"# Iterations\")\n\n<matplotlib.text.Text at 0x11569c4d0>\n\n\n\n\n\nThere are a couple of interesting observations:\n\nThe learning rate for b actually increases from its initial value of 0.01. Even after 1000 iterations, it remains more than its initial value. This can be explained by the fact that the suim of squares gradients wrt b would be less than 1. Thus, the denominator term by which the learning rate gets divided will be less than 1. Thus, increasing the learning rate wrt b. This can however be fixed by choosing \\(\\epsilon=1.0\\)\nThe learning rate for W falls very rapidly. Learning would be negligble for W after the initial few iterations. This can be fixed by choosing a larger initial learning rate \\(\\eta\\).\n\n\n\nEvolution of W and b, wrt \\(\\eta\\) and \\(\\epsilon\\)\n\n# Fixing the random seed for reproducible init params for `W` and `b`\nout = {}\nfor lr in [0.01, 0.1, 1, 10]:\n    out[lr] = {}\n    for eps in [1e-8, 1e-1, 1]:\n        print(lr, eps)\n        np.random.seed(0)\n        param_init = [np.random.randn(), np.random.randn()]\n        niter=10000\n        ada_params, ada_param_array, ada_grad_array, ada_lr_array, ada_cost_array = adagrad_gd(param_init,\n                                                                                               cost, \n                                                                                               niter=niter,\n                                                                                               lr=lr, \n                                                                                               eps=eps)\n        out[lr][eps] = {'Final-params':ada_params,\n                       'Param-array':ada_param_array,\n                       'Cost-array':ada_cost_array}\n\n(0.01, 1e-08)\n(0.01, 0.1)\n(0.01, 1)\n(0.1, 1e-08)\n(0.1, 0.1)\n(0.1, 1)\n(1, 1e-08)\n(1, 0.1)\n(1, 1)\n(10, 1e-08)\n(10, 0.1)\n(10, 1)\n\n\n\nPlotting cost v/s # Iterations\n\nfig, ax = plt.subplots(nrows=3, ncols=4, sharex=True, figsize=(8, 6), sharey=True)\nfor row, eps in enumerate([1e-8, 1e-1, 1]):\n    for column, lr in enumerate([0.01, 0.1, 1, 10]):\n        pd.Series(out[lr][eps]['Cost-array']).plot(ax=ax[row, column])\n        ax[0, column].set_title(\"Eta={}\".format(lr))\n    ax[row, 0].set_ylabel(\"Eps={}\".format(eps))\nfig.text(0.5, 0.0, '# Iterations')\nplt.suptitle(\"Cost v/s # Iterations\");\n\n\n\n\nIt seems that choosing \\(\\eta=1\\) or above the cost usually converges quickly. This seems to be different from most literature recommending \\(\\eta=0.01\\). Aside: I confirmed that even using Tensorflow on the same dataset with Adagrad optimizer, the optimal learning rates are similar to the ones we found here!\n\n\nW v/s # Iterations\n\nfig, ax = plt.subplots(nrows=3, ncols=4, sharex=True, figsize=(8, 6), sharey=True)\nfor row, eps in enumerate([1e-8, 1e-1, 1]):\n    for column, lr in enumerate([0.01, 0.1, 1, 10]):\n        pd.DataFrame(out[lr][eps]['Param-array'])[0].plot(ax=ax[row, column])\n        ax[0, column].set_title(\"Eta={}\".format(lr))\n    ax[row, 0].set_ylabel(\"Eps={}\".format(eps))\nfig.text(0.5, 0.0, '# Iterations')\nplt.suptitle(\"W v/s # Iterations\");\n\n\n\n\n\n\nb v/s # Iterations\n\nfig, ax = plt.subplots(nrows=3, ncols=4, sharex=True, figsize=(8, 6), sharey=True)\nfor row, eps in enumerate([1e-8, 1e-1, 1]):\n    for column, lr in enumerate([0.01, 0.1, 1, 10]):\n        pd.DataFrame(out[lr][eps]['Param-array'])[1].plot(ax=ax[row, column])\n        ax[0, column].set_title(\"Eta={}\".format(lr))\n    ax[row, 0].set_ylabel(\"Eps={}\".format(eps))\nfig.text(0.5, 0.0, '# Iterations')\nplt.suptitle(\"b v/s # Iterations\");\n\n\n\n\nAcross the above two plots, we can see that at high \\(\\eta\\), there are oscillations! In general, \\(\\eta=1\\) and \\(\\epsilon=1e-8\\) seem to give the best set of results.\n\n\n\nVisualising the model learning\n\nfrom matplotlib.animation import FuncAnimation\n\nfig, ax = plt.subplots(nrows=3, ncols=4, sharex=True, figsize=(8, 6), sharey=True)\n\ndef update(i):\n    #fig.clf()\n    for row, eps in enumerate([1e-8, 1e-1, 1]):\n        for column, lr in enumerate([0.01, 0.1, 1, 10]):\n            params_i =  out[lr][eps]['Param-array'][i]\n            ax[row, column].cla()\n            w_i, b_i = params_i\n            ax[row, column].plot(X, Y, 'k.', ms=1)\n            ax[row, column].plot(X, w_i*X+b_i, 'r')\n            ax[row, column].tick_params( #https://stackoverflow.com/questions/12998430/remove-xticks-in-a-matplotlib-plot\n                axis='both',         \n                which='both',      \n                bottom='off', \n                left='off',\n                top='off',         \n                labelbottom='off',\n                labelleft='off') \n            ax[0, column].set_title(\"Eta={}\".format(lr))\n        ax[row, 0].set_ylabel(\"Eps={}\".format(eps))\n    fig.suptitle(\"Iteration number: {}\".format(i))\n\nanim = FuncAnimation(fig, update, frames=np.arange(0, 5000, 200), interval=500)\nanim.save('adagrad.gif', dpi=80, writer='imagemagick')\nplt.close()\n\n\nSo, there you go. Implementing Adagrad and running this experiment was a lot of fun and learning. Feel free to comment!"
  },
  {
    "objectID": "posts/2022-02-07-coin-toss.html",
    "href": "posts/2022-02-07-coin-toss.html",
    "title": "Coin Toss (MLE, MAP, Fully Bayesian) in TF Probability",
    "section": "",
    "text": "Basic Imports\n\nfrom silence_tensorflow import silence_tensorflow\n\nsilence_tensorflow()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport functools\nimport seaborn as sns\nimport tensorflow_probability as tfp\nimport pandas as pd\n\ntfd = tfp.distributions\ntfl = tfp.layers\ntfb = tfp.bijectors\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nCreating a dataset\nLet us create a dataset. We will assume the coin toss to be given as per the Bernoulli distribution. We will assume that \\(\\theta = p(H) = 0.75\\) and generate 10 samples. We will fix the random seeds for reproducibility.\nWe will be encoding Heads as 1 and Tails as 0.\n\nnp.random.seed(0)\ntf.random.set_seed(0)\n\n\ndistribution = tfd.Bernoulli(probs=0.75)\n\ndataset_10 = distribution.sample(10)\nprint(dataset_10.numpy())\nmle_estimate_10 = tf.reduce_mean(tf.cast(dataset_10, tf.float32))\ntf.print(mle_estimate_10)\n\n[0 0 0 1 1 1 1 1 0 1]\n0.6\n\n\n\n\n\nMLE\n\nObtaining MLE analytically\nFrom the above 10 samples, we obtain 6 Heads (1) and 4 Tails. As per the principal of MLE, the best estimate for \\(\\theta = p(H) = \\dfrac{n_h}{n_h+n_t} = 0.6\\)\nWe may also notice that the value of 0.6 is far from the 0.75 value we had initially set. This is possible as our dataset is small.\nWe will now verify if we get the same result using TFP. But, first, we can create a graphical model for our problem.\n\n\nGraphical model\n\nimport daft\n\npgm = daft.PGM([4, 3], origin=[0, 0])\npgm.add_node(daft.Node(\"theta\", r\"$\\theta$\", 1, 2.5, aspect=1.8))\n\npgm.add_node(daft.Node(\"obs\", r\"$obs_i$\", 1, 1, aspect=1.2, observed=True))\n\npgm.add_edge(\"theta\", \"obs\")\npgm.add_plate([0, 0.5, 2, 1.0], label=r\"$N$\", shift=-0.1)\npgm.render()\n\n<Axes:>\n\n\n\n\n\n\n\nObtaining MLE analytically for different dataset sizes\n\ndataset_large = distribution.sample(100000)\n\nmle_estimate = {}\nfor dataset_size in [10, 50, 100, 500, 1000, 10000, 100000]:\n    mle_estimate[dataset_size] = tf.reduce_mean(\n        tf.cast(dataset_large[:dataset_size], tf.float32)\n    )\ntf.print(mle_estimate)\n\n{10: 0.9,\n 50: 0.76,\n 100: 0.71,\n 500: 0.746,\n 1000: 0.749,\n 10000: 0.749,\n 100000: 0.75144}\n\n\nAs we can see above, when we use larger dataset sizes, our estimate matches the value we set (0.75).\n\n\nUsing TFP for MLE\n\nModel setup\n\ntheta = tf.Variable(0.1)\nfit = tfd.Bernoulli(probs=theta)\n\nfit.log_prob(dataset_10)\n\n<tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([-0.10536052, -0.10536052, -0.10536052, -2.3025851 , -2.3025851 ,\n       -2.3025851 , -2.3025851 , -2.3025851 , -0.10536052, -2.3025851 ],\n      dtype=float32)>\n\n\n\n\nDefining loss\nWe now define the negative log likelihood as our loss function and work towards minimizing it.\n\ndataset = dataset_10\n\n\ndef loss():\n    return -tf.reduce_sum(fit.log_prob(dataset))\n\n\n\nTracing variables over training\n\ntrace_fn = lambda traceable_quantities: {\n    \"loss\": traceable_quantities.loss,\n    \"theta\": theta,\n}\n\nnum_steps = 150\n\n\n\nMinimizing the loss function\n\ntrace = tfp.math.minimize(\n    loss_fn=loss,\n    num_steps=num_steps,\n    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n    trace_fn=trace_fn,\n)\n\n\ntheta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.5981374>\n\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True, figsize=(6, 4))\nax[0].plot(range(num_steps), trace[\"loss\"])\nax[1].plot(range(num_steps), trace[\"theta\"])\nsns.despine()\nax[1].set_xlabel(\"Iterations\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_ylabel(r\"$\\theta$\")\nfig.tight_layout()\n\n\n\n\nFrom the above calculations, we can see that we have obtained the same estimate of ~0.6 using TFP.\n\n\nAlternate way to minimize\nPreviously, we used the tf.math.minimize, but we can also use tf.GradientTape() for the same purpose.\n\n@tf.function\ndef loss_and_grads(fit):\n    with tf.GradientTape() as tape:\n        loss = -tf.reduce_sum(fit.log_prob(dataset))\n    return loss, tape.gradient(loss, fit.trainable_variables)\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\n\ntheta = tf.Variable(0.1)\nfit = tfd.Bernoulli(probs=theta)\n\n\nfor i in range(num_steps):\n    loss, grads = loss_and_grads(fit)\n    optimizer.apply_gradients(zip(grads, fit.trainable_variables))\n\n\nfit.trainable_variables\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.5981374>,)\n\n\nWe can see that we obtain the same estimate.\n\n\n\n\nMAP\nWe will now be setting a prior over \\(\\theta\\). A general graphical model is shown below.\n\npgm = daft.PGM([4, 4], origin=[0, 0])\npgm.add_node(daft.Node(\"alpha\", r\"$\\alpha$\", 0.5, 3.5, aspect=1.8))\npgm.add_node(daft.Node(\"beta\", r\"$\\beta$\", 1.5, 3.5, aspect=1.8))\n\n\npgm.add_node(daft.Node(\"theta\", r\"$\\theta$\", 1, 2.5, aspect=2))\n# pgm.add_node(daft.Node(\"theta\", r\"$\\theta\\sim Beta (\\alpha, \\beta)$\", 1, 2.5, aspect=4))\n\npgm.add_node(daft.Node(\"obs\", r\"$obs_i$\", 1, 1, aspect=1.2, observed=True))\n\npgm.add_edge(\"theta\", \"obs\")\npgm.add_edge(\"alpha\", \"theta\")\npgm.add_edge(\"beta\", \"theta\")\n\n\npgm.add_plate([0, 0.5, 2, 1.0], label=r\"$N$\", shift=-0.1)\npgm.render()\n\n<Axes:>\n\n\n\n\n\n\nMAP with uniform prior\nFirst, we see the estimate for \\(\\theta\\) if we use the uniform prior. We should obtain the MLE answer.\n\ndef coin_toss_uniform_model():\n    theta = yield tfp.distributions.Uniform(low=0.0, high=1.0, name=\"Theta\")\n    coin = yield tfp.distributions.Bernoulli(probs=tf.ones(100) * theta, name=\"Coin\")\n\n\ncoin_toss_uniform_model\n\n<function __main__.coin_toss_uniform_model()>\n\n\n\nmodel_joint_uniform = tfp.distributions.JointDistributionCoroutineAutoBatched(\n    lambda: coin_toss_uniform_model(), name=\"Original\"\n)\n\n\nmodel_joint_uniform\n\n<tfp.distributions.JointDistributionCoroutineAutoBatched 'Original' batch_shape=[] event_shape=StructTuple(\n  Theta=[],\n  Coin=[100]\n) dtype=StructTuple(\n  Theta=float32,\n  Coin=int32\n)>\n\n\n\ndef uniform_model(dataset):\n    num_datapoints = len(dataset)\n    theta = yield tfp.distributions.Uniform(low=0.0, high=1.0, name=\"Theta\")\n\n    coin = yield tfp.distributions.Bernoulli(\n        probs=tf.ones(num_datapoints) * theta, name=\"Coin\"\n    )\n\n\nconcrete_uniform_model = functools.partial(uniform_model, dataset=dataset_10)\n\nmodel = tfd.JointDistributionCoroutineAutoBatched(concrete_uniform_model)\n\n\nmodel.sample()\n\nStructTuple(\n  Theta=<tf.Tensor: shape=(), dtype=float32, numpy=0.5930122>,\n  Coin=<tf.Tensor: shape=(10,), dtype=int32, numpy=array([1, 0, 1, 0, 1, 1, 1, 0, 1, 1], dtype=int32)>\n)\n\n\n\nth = tf.Variable(0.4)\n\ntarget_log_prob_fn = lambda th: model.log_prob((th, dataset_10))\n\n\nx_s = tf.linspace(0.0, 1.0, 1000)\ny_s = -target_log_prob_fn(x_s)\nplt.plot(x_s, y_s)\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(\"- Joint Log Prob \\n(Unnormalized)\")\n\nsns.despine()\n\n\n\n\n\ntrace = tfp.math.minimize(\n    lambda: -target_log_prob_fn(th),\n    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n    # trace_fn=trace_fn,\n    num_steps=200,\n)\n\n\nth\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.59999406>\n\n\n\nmle_estimate_10\n\n<tf.Tensor: shape=(), dtype=float32, numpy=0.6>\n\n\nWe see above that our MAP estimate is fairly close to the MLE when we used the uniform prior.\n\n\nMAP with Beta prior\nWe will now use a much more informative prior – the Beta prior. We will be setting \\(\\alpha=40\\) and \\(\\beta=10\\) indicating that we have a prior belief that Tails is much more likely than Heads. This is a bad assumption and in the limited data regime will lead to poor estimates.\n\ndef beta_prior_model(dataset, alpha, beta):\n    num_datapoints = len(dataset)\n    theta = yield tfp.distributions.Beta(\n        concentration0=alpha, concentration1=beta, name=\"Theta\"\n    )\n\n    coin = yield tfp.distributions.Bernoulli(\n        probs=tf.ones(num_datapoints) * theta, name=\"Coin\"\n    )\n\n\nconcrete_beta_prior_model_40_10 = functools.partial(\n    beta_prior_model, dataset=dataset_10, alpha=40, beta=10\n)\n\n\nmodel_2_40_10 = tfd.JointDistributionCoroutineAutoBatched(\n    concrete_beta_prior_model_40_10\n)\n\n\nmodel_2_40_10.sample()\n\nStructTuple(\n  Theta=<tf.Tensor: shape=(), dtype=float32, numpy=0.16982338>,\n  Coin=<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>\n)\n\n\n\nmodel_2_40_10.prob(Theta=0.1, Coin=[0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n\n<tf.Tensor: shape=(), dtype=float32, numpy=0.005809709>\n\n\n\nth = tf.Variable(0.2)\n\ntarget_log_prob_fn = lambda th: model_2_40_10.log_prob(Theta=th, Coin=dataset_10)\n\n\nx_s = tf.linspace(0.0, 1.0, 1000)\ny_s = -target_log_prob_fn(x_s)\nplt.plot(x_s, y_s)\nplt.xlabel(r\"$\\theta$\")\nplt.ylabel(\"- Joint Log Prob \\n(Unnormalized)\")\n\nsns.despine()\n\n\n\n\n\ntrace = tfp.math.minimize(\n    lambda: -target_log_prob_fn(th),\n    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n    # trace_fn=trace_fn,\n    num_steps=200,\n)\n\n\nth\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.25861916>\n\n\nWe now see that our MAP estimate for \\(\\theta\\) is 0.25, which is very far from the MLE. Choosing a better prior would have led to better estimates. Or, if we had more data, the likelihood would have dominated over the prior resulting in better estimates.\n\nconcrete_beta_prior_model_1_1 = functools.partial(\n    beta_prior_model, dataset=dataset_10, alpha=1, beta=1\n)\n\nmodel_2_1_1 = tfd.JointDistributionCoroutineAutoBatched(concrete_beta_prior_model_1_1)\n\nth = tf.Variable(0.2)\n\ntarget_log_prob_fn = lambda th: model_2_1_1.log_prob(Theta=th, Coin=dataset_10)\n\ntrace = tfp.math.minimize(\n    lambda: -target_log_prob_fn(th),\n    optimizer=tf.optimizers.Adam(learning_rate=0.01),\n    # trace_fn=trace_fn,\n    num_steps=200,\n)\n\nth\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.6000196>\n\n\nOur estimate for \\(\\theta\\) is more reasonable now.\n\n\n\nFully Bayesian\nWe now need to define a model \\(q(\\theta)\\) to act as the surrogate for our posterior \\(p(\\theta|D)\\). Let us use a Beta distribution.\n\nq_alpha = tf.Variable(1.0)\nq_beta = tf.Variable(1.0)\n\n\nsurrogate_posterior = tfd.Beta(concentration0=q_alpha, concentration1=q_beta, name=\"q\")\n\n\nsurrogate_posterior.sample()\n\n<tf.Tensor: shape=(), dtype=float32, numpy=0.7745516>\n\n\n\nlosses = tfp.vi.fit_surrogate_posterior(\n    target_log_prob_fn,\n    surrogate_posterior=surrogate_posterior,\n    optimizer=tf.optimizers.Adam(learning_rate=0.005),\n    num_steps=400,\n)\n\n\nplt.plot(losses)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nsns.despine()\n\n\n\n\n\nq_alpha, q_beta\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.1893775>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.5093094>)\n\n\n\nsns.kdeplot(surrogate_posterior.sample(500).numpy(), bw_adjust=2)\nsns.despine()\nplt.xlabel(r\"$\\theta$\")\n\nText(0.5, 0, '$\\\\theta$')\n\n\n\n\n\n\nGenerating samples on coin tosses conditioning on theta\nFirst, let us look at the syntax and then generate 1000 samples.\n\nmodel_2_1_1.sample(Theta=0.1)\n\nStructTuple(\n  Theta=<tf.Tensor: shape=(), dtype=float32, numpy=0.1>,\n  Coin=<tf.Tensor: shape=(10,), dtype=int32, numpy=array([1, 0, 0, 0, 0, 0, 0, 1, 0, 0], dtype=int32)>\n)\n\n\n\nmodel_2_1_1.sample(Theta=0.9)\n\nStructTuple(\n  Theta=<tf.Tensor: shape=(), dtype=float32, numpy=0.9>,\n  Coin=<tf.Tensor: shape=(10,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)>\n)\n\n\nWe can clearly see that conditioning on r\\(\\theta\\) changes the number of heads.\n\n\nFun check: What if we fix the dataset and sample on theta?\n\nmodel_2_1_1.sample(Coin=[0, 1, 1, 0, 1, 1, 1, 0])\n\nStructTuple(\n  Theta=<tf.Tensor: shape=(), dtype=float32, numpy=0.34792978>,\n  Coin=<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 1, 1, 0, 1, 1, 1, 0], dtype=int32)>\n)\n\n\n\nmodel_2_1_1.sample(Coin=[0, 1, 1, 0, 1, 1, 1, 0])\n\nStructTuple(\n  Theta=<tf.Tensor: shape=(), dtype=float32, numpy=0.74594486>,\n  Coin=<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 1, 1, 0, 1, 1, 1, 0], dtype=int32)>\n)\n\n\nAs we see above, we can get different \\(\\theta\\). If our dataset was large, this effect would be less pronounced.\n\nc = model_2_1_1.sample(Theta=surrogate_posterior.sample(1000)).Coin\n\n\npd.DataFrame(c)\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      2\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      3\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      1\n      1\n      1\n      0\n      0\n      1\n      1\n      1\n      1\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n    \n    \n      996\n      1\n      0\n      1\n      1\n      1\n      1\n      1\n      1\n      0\n      0\n    \n    \n      997\n      0\n      0\n      0\n      0\n      1\n      1\n      1\n      1\n      1\n      0\n    \n    \n      998\n      1\n      1\n      1\n      1\n      1\n      0\n      1\n      1\n      1\n      0\n    \n    \n      999\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n    \n  \n\n1000 rows × 10 columns\n\n\n\n\nsns.histplot(tf.reduce_sum(tf.cast(c, tf.float32), axis=1), bins=11)\nsns.despine()\n\n\n\n\nWe can see the count of number of heads in 1000 samples generated from the posterior.\n\n\nReferences (incomplete as of now)\n\nExcellent repo and video playlist (Playlist 1, Playlist 2) by Felix\nProbabilistic PCA tutorial on TFP\nDiscussion on joint log prob on TFP"
  },
  {
    "objectID": "posts/2021-08-20-bayesian.html",
    "href": "posts/2021-08-20-bayesian.html",
    "title": "Probabilistic Programming in Pyro",
    "section": "",
    "text": "In this post, I will look at a simple application:\nIs the number of COVID cases changing over time?\n\nI will not be using real data and this post will be purely educational in nature.\nThe main aim of this post is to review some distributions and concepts in probabilistic programming.\nThe post is heavily inspired (copied and modified) by the excellent book called Bayesian Methods for Hackers (BMH). I am also borrowing a small subset of code from a forked repository for BMF containing some code in Pyro.\nEventually, we should be able to learn something like the following image, where we detect the changepoint and also the values before and after the change.\n\n\nimport torch\nimport pyro\nimport numpy as np\n\npyro.set_rng_seed(101)\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pyro.infer import MCMC, NUTS, HMC\nimport pyro.distributions as dist\nplt.style.use('seaborn-colorblind')"
  },
  {
    "objectID": "posts/2021-08-20-bayesian.html#distributions-in-pyro",
    "href": "posts/2021-08-20-bayesian.html#distributions-in-pyro",
    "title": "Probabilistic Programming in Pyro",
    "section": "Distributions in Pyro",
    "text": "Distributions in Pyro\nWe will first look at some distributions in Pyro to understand the task better.\nWe first start with the unfirom distribution between 0 and 100 and generate 1000 samples.\n\nu = torch.distributions.Uniform(0, 100)\n\n\nn=1000\ns = pd.Series(u.sample((n, )))\ns.hist(density=True, grid=False, bins=10 )\nplt.ylabel(\"PDF at z\")\nplt.xlabel(\"z\")\nplt.title(\"Uniform Distribution\")\n\nText(0.5, 0, 'z')\n\n\n\n\n\nAs expected, all values in [0, 100] are equally likely.\nWe next look at the Categorical distribution which is a discrete distribution. Using it, we can create a discrete uniform distribution over [0, 10].\n\ndu = torch.distributions.Categorical(torch.tensor([1./n for _ in range(10)]))\n\n\nn = 1000\ndu_samples = du.sample((n, ))\n\n\ndu_series = pd.Series(du_samples).value_counts().sort_index()\ndu_prop = du_series/n\ndu_prop.plot(kind='bar',rot=0, color='green')\nplt.ylabel(\"PMF at k\")\nplt.xlabel(\"k\")\nplt.title(\"Discrete Uniform Distribution\")\n\nText(0.5, 1.0, 'Discrete Uniform Distribution')\n\n\n\n\n\nWe next look at the exponential distribution. It is controlled by a parameter \\(\\lambda\\) with the expected value of the random variable being \\(\\dfrac{1}{\\lambda}\\)\n\nexp1 = torch.distributions.Exponential(1)\nexp2 = torch.distributions.Exponential(5)\n\n\ns1 = pd.Series(exp1.sample((5000, )))\ns2 = pd.Series(exp2.sample((5000, )))\n\n\ns1.hist(density=True, alpha=0.3, bins=20, color='g', label=r'$\\lambda = 1$')\ns2.hist(density=True, alpha=0.6, bins=20, color='orange',label=r'$\\lambda = 5$')\nplt.xlim((0, 5))\n\nplt.ylabel(\"PDF at z\")\nplt.xlabel(\"z\")\nplt.title(\"Exponential distribution\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fe461b45850>\n\n\n\n\n\nWe finally look at the Poisson distribution. It is controlled by a parameter \\(\\lambda\\) with the expected value of the random variable being \\({\\lambda}\\). Poisson is a discrete distribution often used for modelling count data.\n\np1 = torch.distributions.Poisson(4)\np2 = torch.distributions.Poisson(8)\n\n\ns1 = pd.Series(p1.sample((5000, )))\ns2 = pd.Series(p2.sample((5000, )))\n\ns1 = s1.astype('int').value_counts().sort_index()\ns1 = s1/5000\n\ns2 = s2.astype('int').value_counts().sort_index()\ns2 = s2/5000\n\ns1.plot.bar(color='g', alpha=0.5, label=r'$\\lambda = 4$', rot=0)\ns2.plot.bar(color='orange', alpha=0.3, label=r'$\\lambda = 8$', rot=0)\n\nplt.legend()\nplt.ylabel(\"PMF at k\")\nplt.xlabel(\"k\")\nplt.title(\"Poisson Distribution\")\n\nText(0.5, 1.0, 'Poisson Distribution')"
  },
  {
    "objectID": "posts/2021-08-20-bayesian.html#creating-the-dataset",
    "href": "posts/2021-08-20-bayesian.html#creating-the-dataset",
    "title": "Probabilistic Programming in Pyro",
    "section": "Creating the dataset",
    "text": "Creating the dataset\nWe will be creating the dataset for daily COVID count with\n\nBefore day number 30, the cases are Poisson distributed with mean of 30\nAfter day number 30, the cases are Poisson distributed with mean of 40\nWe have data for a total of 50 days\n\n\ngt_tau = 30\ngt_lambda_1 = 30\ngt_lambda_2 = 40\n\ndef sample(day):\n    if day < gt_tau:\n        l = gt_lambda_1\n    else:\n        l = gt_lambda_2\n    \n    return torch.distributions.Poisson(l).sample()\n\n\ndata = np.array([sample(day) for day in range(50)])\n\n\nplt.bar(range(50), data, color='orange')\nplt.xlabel(\"Day\")\nplt.ylabel(\"Number of daily cases\")\nplt.savefig(\"cases-raw.png\")\n\n\n\n\n\nplt.bar(range(50), data, color='orange')\nplt.xlabel(\"Day\")\nplt.ylabel(\"Number of daily cases\")\nplt.axhline(30, 0, 30/50, label='Before changepoint', lw=3, color='green')\nplt.axhline(40, 30/50, 1, label='After changepoint', lw=3, color='red')\nplt.axvline(30, label='Changepoint', lw=5, color='black', alpha=0.8, linestyle='--')\n\n\nplt.legend()\nplt.savefig(\"cases-annotated.png\")"
  },
  {
    "objectID": "posts/2021-08-20-bayesian.html#modelling",
    "href": "posts/2021-08-20-bayesian.html#modelling",
    "title": "Probabilistic Programming in Pyro",
    "section": "Modelling",
    "text": "Modelling\nWe will now assume that we received the data and need to create a model.\nWe choose the following model\n\\[C_{i} \\sim \\operatorname{Poisson}(\\lambda)\\]\n\\[\\lambda= \\begin{cases}\\lambda_{1} & \\text { if } t<\\tau \\\\ \\lambda_{2} & \\text { if } t \\geq \\tau\\end{cases}\\]\n\\[\\begin{aligned}\n&\\lambda_{1} \\sim \\operatorname{Exp}(\\alpha) \\\\\n&\\lambda_{2} \\sim \\operatorname{Exp}(\\alpha)\n\\end{aligned}\n\\]\n\ndef model(data):\n    alpha = 1.0 / data.mean()\n    lambda_1 = pyro.sample(\"lambda_1\", dist.Exponential(alpha))\n    lambda_2 = pyro.sample(\"lambda_2\", dist.Exponential(alpha))\n    \n    tau = pyro.sample(\"tau\", dist.Uniform(0, 1))\n    lambda1_size = (tau * data.size(0) + 1).long()\n    lambda2_size = data.size(0) - lambda1_size\n    lambda_ = torch.cat([lambda_1.expand((lambda1_size,)),\n                         lambda_2.expand((lambda2_size,))])\n\n    with pyro.plate(\"data\", data.size(0)):\n        pyro.sample(\"obs\",dist.Poisson(lambda_), obs=data)"
  },
  {
    "objectID": "posts/2021-08-20-bayesian.html#inference-button",
    "href": "posts/2021-08-20-bayesian.html#inference-button",
    "title": "Probabilistic Programming in Pyro",
    "section": "Inference button",
    "text": "Inference button\n\nkernel = NUTS(model, jit_compile=True, ignore_jit_warnings=True, max_tree_depth=3)\nposterior = MCMC(kernel, num_samples=5000, warmup_steps=2000)\nposterior.run(torch.from_numpy(data));\n\nSample: 100%|██████████| 7000/7000 [00:33, 210.22it/s, step size=1.06e-02, acc. prob=0.963]"
  },
  {
    "objectID": "posts/2021-08-20-bayesian.html#obtaining-and-plotting-the-posteriors",
    "href": "posts/2021-08-20-bayesian.html#obtaining-and-plotting-the-posteriors",
    "title": "Probabilistic Programming in Pyro",
    "section": "Obtaining and plotting the posteriors",
    "text": "Obtaining and plotting the posteriors\n\nhmc_samples = {k: v.detach().cpu().numpy() for k, v in posterior.get_samples().items()}\nlambda_1_samples = hmc_samples['lambda_1']\nlambda_2_samples = hmc_samples['lambda_2']\ntau_samples = (hmc_samples['tau'] * torch.from_numpy(data).size(0) + 1).astype(int)\n\n\nfig, ax  = plt.subplots(nrows=2, sharex=True)\nax[0].hist(lambda_1_samples, density=True)\nax[1].hist(lambda_2_samples, density=True)\nplt.suptitle(r\"Posterior distribution for $\\lambda$\")\nax[0].set_title(r\"$\\lambda_1$\")\nax[1].set_title(r\"$\\lambda_2$\")\n\nText(0.5, 1.0, '$\\\\lambda_2$')\n\n\n\n\n\n\n(pd.Series(tau_samples).value_counts()/5000).sort_index().plot(kind='bar',rot=0)\nplt.suptitle(r\"Posterior distribution for $\\tau$\")\nplt.xlabel(r\"$\\tau=k$\")\nplt.ylabel(r\"$P(\\tau=k)$\")\n\nText(0, 0.5, '$P(\\\\tau=k)$')\n\n\n\n\n\n\nIt seems from our posterior that we have obtained a fairly good estimate our simulation parameters."
  },
  {
    "objectID": "posts/2020-03-26-gp.html",
    "href": "posts/2020-03-26-gp.html",
    "title": "Some experiments in Gaussian Processes Regression",
    "section": "",
    "text": "youtube: https://www.youtube.com/watch?v=V1bF37-_ytQ"
  },
  {
    "objectID": "posts/2020-03-26-gp.html#air-quality-2d-map",
    "href": "posts/2020-03-26-gp.html#air-quality-2d-map",
    "title": "Some experiments in Gaussian Processes Regression",
    "section": "Air quality 2d map",
    "text": "Air quality 2d map\nNow, we will be using GPs for predicting air quality in New Delhi. See my previous post on how to get AQ data for Delhi.https://nipunbatra.github.io/blog/air%20quality/2018/06/21/aq-india-map.html\nI will be creating a function to visualise the AQ estimations using GPs based on different kernels.\nThe shapefile for Delhi can be downloaded from here.\n\nimport pandas as pd\nimport os\ndf = pd.read_csv(os.path.expanduser(\"~/Downloads/2018-04-06.csv\"))\ndf = df[(df.country=='IN')&(df.city=='Delhi')&(df.parameter=='pm25')].dropna().groupby(\"location\").mean()\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      value\n      latitude\n      longitude\n    \n    \n      location\n      \n      \n      \n    \n  \n  \n    \n      Burari Crossing, New Delhi - IMD\n      245.583333\n      28.725650\n      77.201157\n    \n    \n      CRRI Mathura Road, New Delhi - IMD\n      265.666667\n      28.551200\n      77.273574\n    \n    \n      DTU, New Delhi - CPCB\n      214.333333\n      28.750050\n      77.111261\n    \n    \n      IGI Airport Terminal - 3, New Delhi - IMD\n      130.666667\n      28.562776\n      77.118005\n    \n    \n      IHBAS, Dilshad Garden,New Delhi - CPCB\n      212.583333\n      28.680275\n      77.201157\n    \n    \n      ITO, New Delhi - CPCB\n      220.500000\n      28.631694\n      77.249439\n    \n    \n      Lodhi Road, New Delhi - IMD\n      176.083333\n      28.591825\n      77.227307\n    \n    \n      Mandir Marg, New Delhi - DPCC\n      82.000000\n      28.637269\n      77.200560\n    \n    \n      NSIT Dwarka, New Delhi - CPCB\n      184.583333\n      28.609090\n      77.032541\n    \n    \n      North Campus, DU, New Delhi - IMD\n      147.833333\n      28.657381\n      77.158545\n    \n    \n      Pusa, New Delhi - IMD\n      112.000000\n      28.610304\n      77.099694\n    \n    \n      R K Puram, New Delhi - DPCC\n      103.600000\n      28.564610\n      77.167010\n    \n    \n      Shadipur, New Delhi - CPCB\n      213.833333\n      28.651478\n      77.147311\n    \n    \n      Sirifort, New Delhi - CPCB\n      222.250000\n      28.550425\n      77.215938\n    \n    \n      US Diplomatic Post: New Delhi\n      46.625000\n      28.635760\n      77.224450\n    \n  \n\n\n\n\n\nimport geopandas\ngdf = geopandas.GeoDataFrame(\n    df, geometry=geopandas.points_from_xy(df.longitude, df.latitude))\n\n\ngdf.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f5207d29110>\n\n\n\n\n\n\ndef plot_air_vis(df, k, shp, title):\n    m = GPy.models.GPRegression(df[['longitude','latitude']], df[['value']], k)\n    m.optimize(max_iters=2000)\n    y_t = np.linspace(28.38,28.9, 40)\n    x_t = np.linspace(76.82, 77.4, 40)\n\n    XX, YY = np.meshgrid(x_t, y_t)\n    Z_pred = np.zeros_like(YY)\n    Z_var = np.zeros_like(YY)\n    for i in range(40):\n        for j in range(40):\n            Z_pred[i, j], Z_var[i, j] = m.predict_noiseless(np.array([XX[i, j], YY[i, j]]).reshape(1, 2))\n    \n    data = geopandas.read_file(fp)\n    fig = plt.figure(figsize=(6, 6))\n    plt.contourf(XX, YY, Z_pred, levels=30,alpha=0.6,cmap='Purples')\n    plt.colorbar()\n    gdf.plot(ax=plt.gca(),markersize=gdf['value'],color='k')\n    data.plot(color='k',ax=plt.gca(),zorder=-1,alpha=0.4)\n    plt.gca().set_aspect(\"equal\")\n    for a in [100, 150, 200,250]:\n        plt.scatter([], [], c='k', alpha=1, s=a,\n                    label=str(a) + '$\\mu g/m^3$')\n    plt.legend(scatterpoints=1, frameon=True,\n               labelspacing=1, loc='upper left',ncol=2)\n    \n    plt.title(title+\"\\t\"+str(m.objective_function()))\n\n\nk_2d = GPy.kern.RBF(input_dim=2, lengthscale=1)\nk_2d_rbf_2 = GPy.kern.RBF(input_dim=2, lengthscale=3)*k_2d\nk_2d_rbf_3 = GPy.kern.RBF(input_dim=2, lengthscale=3) + k_2d_rbf_2\nk_matern32 = GPy.kern.Matern32(input_dim=2)\nk_matern52 = GPy.kern.Matern52(input_dim=2)\n\nk_rbf_matern = k_matern32 * k_matern52 + k_matern32*k_2d_rbf_3\n\n\nfp=os.path.expanduser(\"~/Downloads/wards delimited.shp\")\n\n\nplot_air_vis(df, k_2d, fp,\"RBF\")\nplot_air_vis(df, k_matern32, fp,\"Matern32\")\nplot_air_vis(df, k_matern52, fp,\"matern52\")\nplot_air_vis(df, k_2d_rbf_2, fp,\"RBF*RBF\")\nplot_air_vis(df, k_2d_rbf_3, fp,\"RBF*RBF+RBF\")\nplot_air_vis(df, k_rbf_matern, fp,\"Matern32*Matern52+Matern32*RBF\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere you go. Till next time!"
  },
  {
    "objectID": "posts/2013-09-01-denoising.html",
    "href": "posts/2013-09-01-denoising.html",
    "title": "Denoising using Least Squares",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn\n%matplotlib inline\n\n\nTrue data\nLet us create the true data, i.e. if there were no noise and the measuring system was perfect, how would our data look like. This is a modified sinusoid signal.\n\nn=400\nt=np.array(xrange(n))\nex=0.5*np.sin(np.dot(2*np.pi/n,t))*np.sin(0.01*t)\n\n\nplt.plot(ex);\n\n\n\n\n\n\nNoisy data\nNow, let us add some Gaussian noise about the true data\n\ncorr=ex+0.05*np.random.normal(0,1,n)\n\nThis is how the corrupted signal looks like.\n\nplt.plot(corr);\n\n\n\n\n\n\nThe optimization problem\nConstraints\n\nWe want the cleaned data to be as close to the corrupted data\nWe want the successive entries in the cleaned data to be as close as possible\n\nMathematically, we can write this as follows:\n\n$ $\n\nHere, \\(||(x-x_{corr})||^2\\) corresponds to the first constraint and ${k=1}^{n-1} (x{k+1}-x_{k})^{2} $ corresponds to the second constraint.\n\n\nFormulation as least squares problem\nFor the least squares problem, we would like a form of \\(Ax=b\\)\n\nFirst constraint\nLooking at the first constraint, \\(||(x-x_{corr})||^2\\), this can be expanded as follows:\n$ (x_1 -x_{corr}(1))^2 + (x_2 -x_{corr}(2))^2 + ..(x_n -x_{corr}(n))^2$\nwhich we can rewrite as :\n$ ([1,0,0 ..0][x_1, x_2,..x_n]^T - x_{corr}(1))^2 + ([0,1,0 ..0][x_1, x_2,..x_n]^T - x_{corr}(2))^2 + …([0,0,0 ..n][x_1, x_2,..x_n]^T - x_{corr}(n))^2$\nComparing with the standard least squares, we get\n\\(A_{first} = I^{nxn}\\) and \\(b_{first} = x_{corr}\\)\n\n\nSecond constraint\nFor the second constraint, \\(\\mu\\sum_{k=1}^{n-1} (x_{k+1}-x_{k})^{2}\\)\nThis can be written as: \\((\\sqrt{\\mu}(x_2-x_1))^2 + (\\sqrt{\\mu}(x_3-x_2))^2 + .. (\\sqrt{\\mu}(x_n-x_{n-1}))^2\\) which we can write as:\n\\((\\sqrt{\\mu}[-1,1,0,0,0,0..][x_1,x_2,..x_n]^T - 0)^2 + ...(\\sqrt{\\mu}[0,0,0,..,-1,1][x_1,x_2,..x_n]^T - 0)^2\\)\nFrom this formulation, we get for this constraint, \\(A_{second}\\) as follows (we ignore \\(\\sqrt{\\mu}\\) for now)\n[-1, 1, 0, ...., 0]\n\n[0, -1, 1, ......0]\n\n..\n\n[0, 0, ..... -1, 1]\nand \\(b_{second}\\) as a zeros matrix of size (n-1, n)\n\n\nCombining both constraints\nSo, we combine the two sets of As and bs in a bigger matrix to get \\(A_{combined}\\) and \\(b_{combined}\\) as follows:\n\\(A_{combined}\\)\n[A_first\nA_second]\nand \\(b_{combined}\\) as :\n[b_first\nb_second]\n\n\n\nConstructing \\(A_{second}\\) matrix with parameter \\(n\\) and \\(\\mu\\)\n\nroot_mu=10\n\n\nd1=np.eye(n-1,n)\n\n\nd1\n\narray([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n       ..., \n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])\n\n\n\nd1=np.roll(d1,1)\n\n\nd1\n\narray([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ..., \n       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])\n\n\n\nd2=-np.eye(n-1,n)\n\n\na_second=  root_mu*(d1+d2)\n\n\na_second\n\narray([[-10.,  10.,   0., ...,   0.,   0.,   0.],\n       [  0., -10.,  10., ...,   0.,   0.,   0.],\n       [  0.,   0., -10., ...,   0.,   0.,   0.],\n       ..., \n       [  0.,   0.,   0., ...,  10.,   0.,   0.],\n       [  0.,   0.,   0., ..., -10.,  10.,   0.],\n       [  0.,   0.,   0., ...,   0., -10.,  10.]])\n\n\n\n\nConstructing \\(A_{first}\\) matrix with parameter \\(n\\)\n\na_first=np.eye(n)\n\n\na_first\n\narray([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n       ..., \n       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])\n\n\n\n\nConstructing \\(A\\) matrix\n\nA=np.vstack((a_first,a_second))\n\n\nA\n\narray([[  1.,   0.,   0., ...,   0.,   0.,   0.],\n       [  0.,   1.,   0., ...,   0.,   0.,   0.],\n       [  0.,   0.,   1., ...,   0.,   0.,   0.],\n       ..., \n       [  0.,   0.,   0., ...,  10.,   0.,   0.],\n       [  0.,   0.,   0., ..., -10.,  10.,   0.],\n       [  0.,   0.,   0., ...,   0., -10.,  10.]])\n\n\nSimilarly, we construct \\(b\\) matrix\n\ncorr=corr.reshape((n,1))\n\n\nb_2=np.zeros((n-1,1))\n\n\nb=np.vstack((corr,b_2))\n\nLet us use least squares for our denoising for \\(\\mu=100\\)\n\nimport numpy.linalg.linalg as LA\n\n\nans = LA.lstsq(A,b)\n\n\n\nComparing original, noisy and denoised signal\n\nplt.plot(corr,label='noisy',alpha=0.4)\nplt.plot(ans[0],'r',linewidth=2, label='denoised')\nplt.plot(ex,'g',linewidth=2, label='original')\nplt.legend();\n\n\n\n\n\n\nObserving the variation with \\(\\mu\\)\nLet us quickly create an interactive widget to see how denoising would be affected by different values of \\(\\mu\\). We will view this on a log scale.\n\ndef create_A(n, mu):\n    \"\"\"\n    Create the A matrix\n    \"\"\"\n    d1=np.eye(n-1,n)\n    d1=np.roll(d1,1)\n    a_second=  np.sqrt(mu)*(d1+d2)\n    a_first=np.eye(n)\n    A=np.vstack((a_first,a_second))\n    return A\n\ndef create_b(n):\n    b_2=np.zeros((n-1,1))\n    b=np.vstack((corr,b_2))\n    \n    return b\n\ndef denoise(mu):\n    \"\"\"\n    Return denoised signal\n    \"\"\"\n    n =  len(corr)\n    A = create_A(n, mu)\n    b = create_b(n)\n    return LA.lstsq(A,b)[0]\n\ndef comparison_plot(mu):\n    fig, ax = plt.subplots(figsize=(4,3))  \n    ax.plot(corr,label='noisy',alpha=0.4)\n    ax.plot(denoise(np.power(10,mu)),'r',linewidth=2, label='denoised')\n    ax.plot(ex,'g',linewidth=2, label='original')\n    plt.title(r\"$\\mu=%d$\" % np.power(10,mu))\n    plt.legend()\n    return fig\n\nfrom ipywidgets import StaticInteract, RangeWidget\nStaticInteract(comparison_plot, mu = RangeWidget(0,5,1))    \n\n\n    \n    \n    \n      \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n    \n      \n    \n    \n      mu: \n    \n    \n\n\nFor smaller values of \\(\\mu\\), the first constraints is the dominant one and tries to keep the denoised signal close to the noisy one. For higher values of \\(\\mu\\), the second constraint dominates."
  },
  {
    "objectID": "posts/2020-02-20-bayesian-linear-regression.html",
    "href": "posts/2020-02-20-bayesian-linear-regression.html",
    "title": "Bayesian Linear Regression",
    "section": "",
    "text": "x = np.linspace(-1, 1, 50).reshape(-1, 1)\n\n\ny = 5*x + 4 \nnoise = (np.abs(x.flatten())*np.random.randn(len(x))).reshape(-1,1)\ny = y + noise\n\n\nplt.scatter(x, y)\nplt.plot(x, 5*x + 4, 'k')\n\n\n\n\n\nfrom scipy.stats import multivariate_normal\nfrom matplotlib import cm\ncov = np.array([[ 1 , 0], [0,  1]])\nvar = multivariate_normal(mean=[0,0], cov=cov)\nx_grid, y_grid = np.mgrid[-1:1:.01, -1:1:.01]\npos = np.dstack((x_grid, y_grid))\nz = var.pdf(pos)\nplt.contourf(x_grid, y_grid, z)\nplt.gca().set_aspect('equal')\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\nplt.title(r\"Prior distribution of $\\theta = f(\\mu, \\Sigma)$\")\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1a18423950>\n\n\n\n\n\n\\[\n\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-\\frac{(y_{i}-\\hat{y}_{i})^{2}}{2 \\sigma^{2}}}\n\\]\n\nSample from prior\n\nn_samples = 20\nfor n in range(n_samples):\n    theta_0_s, theta_1_s = var.rvs()\n    plt.plot(x, theta_1_s*x + theta_0_s, color='k',alpha=0.2)\nplt.scatter(x, y)\n\n<matplotlib.collections.PathCollection at 0x1a18598fd0>\n\n\n\n\n\n\n\nLikelihood of theta\n\ndef likelihood(theta_0, theta_1, x, y, sigma):\n    s = 0\n    x_plus_1 = np.hstack((np.ones_like(x), x))\n\n    for i in range(len(x)):\n        y_i_hat = x_plus_1[i, :]@np.array([theta_0, theta_1])\n        s += (y[i,:]-y_i_hat)**2\n    \n    \n    return np.exp(-s/(2*sigma*sigma))/np.sqrt(2*np.pi*sigma*sigma)\n\n\nlikelihood(-1, 1, x, y, 4)\n\narray([1.00683395e-22])\n\n\n\nx_grid_2, y_grid_2 = np.mgrid[0:8:.1, 0:8:.1]\n\nli = np.zeros_like(x_grid_2)\nfor i in range(x_grid_2.shape[0]):\n    for j in range(x_grid_2.shape[1]):\n        li[i, j] = likelihood(x_grid_2[i, j], y_grid_2[i, j], x, y, 4)\n        \n\n\nplt.contourf(x_grid_2, y_grid_2, li)\nplt.gca().set_aspect('equal')\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\nplt.colorbar()\nplt.scatter(4, 5, s=200, marker='*', color='r')\nplt.title(r\"Likelihood as a function of ($\\theta_0, \\theta_1$)\")\n\nText(0.5, 1.0, 'Likelihood as a function of ($\\\\theta_0, \\\\theta_1$)')\n\n\n\n\n\n\n\nLikelihood of \\(\\sigma^2\\)\n\nx_plus_1 = np.hstack((np.ones_like(x), x))\n\ntheta_mle = np.linalg.inv(x_plus_1.T@x_plus_1)@(x_plus_1.T@y)\nsigma_2_mle = np.linalg.norm(y - x_plus_1@theta_mle)**2\nsigma_mle = np.sqrt(sigma_2_mle)\nsigma_mle\n\n4.128685902124939\n\n\n\n\nPosterior\n\\[\n\\begin{aligned}\np(\\boldsymbol{\\theta} | \\mathcal{X}, \\mathcal{Y}) &=\\mathcal{N}\\left(\\boldsymbol{\\theta} | \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\\\\n\\boldsymbol{S}_{N} &=\\left(\\boldsymbol{S}_{0}^{-1}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{\\Phi}\\right)^{-1} \\\\\n\\boldsymbol{m}_{N} &=\\boldsymbol{S}_{N}\\left(\\boldsymbol{S}_{0}^{-1} \\boldsymbol{m}_{0}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{y}\\right)\n\\end{aligned}\n\\]\n\nS0 = np.array([[ 1 , 0], [0,  1]])\nM0 = np.array([0, 0])\n\nSN = np.linalg.inv(np.linalg.inv(S0) + (sigma_mle**-2)*x_plus_1.T@x_plus_1)\nMN = SN@(np.linalg.inv(S0)@M0 + (sigma_mle**-2)*(x_plus_1.T@y).squeeze())\n\n\nMN, SN\n\n(array([2.97803341, 2.54277597]), array([[2.54243881e-01, 2.97285330e-17],\n        [2.97285330e-17, 4.95625685e-01]]))\n\n\n\nfrom scipy.stats import multivariate_normal\nfrom matplotlib import cm\ncov = np.array([[ 1 , 0], [0,  1]])\nvar_pos = multivariate_normal(mean=MN, cov=SN)\nx_grid, y_grid = np.mgrid[0:8:.1, 0:8:.1]\npos = np.dstack((x_grid, y_grid))\nz = var_pos.pdf(pos)\nplt.contourf(x_grid, y_grid, z)\nplt.gca().set_aspect('equal')\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\nplt.title(r\"Posterior distribution of $\\theta = f(\\mu, \\Sigma)$\")\nplt.scatter(4, 5, s=200, marker='*', color='r', label='MLE')\nplt.scatter(MN[0], MN[1], s=100, marker='^', color='black', label='MAP')\n\nplt.colorbar()\nplt.legend()\nplt.savefig(\"../images/blr-map.png\")\n\n\n\n\nSample from posterior\n\nn_samples = 20\nfor n in range(n_samples):\n    theta_0_s, theta_1_s = var_pos.rvs()\n    plt.plot(x, theta_1_s*x + theta_0_s, color='k',alpha=0.2)\nplt.scatter(x, y)\n\n<matplotlib.collections.PathCollection at 0x1a18e7dd10>\n\n\n\n\n\n\n\nPosterior predictions\n\\[\n\\begin{aligned}\np\\left(y_{*} | \\mathcal{X}, \\mathcal{Y}, \\boldsymbol{x}_{*}\\right) &=\\int p\\left(y_{*} | \\boldsymbol{x}_{*}, \\boldsymbol{\\theta}\\right) p(\\boldsymbol{\\theta} | \\mathcal{X}, \\mathcal{Y}) \\mathrm{d} \\boldsymbol{\\theta} \\\\\n&=\\int \\mathcal{N}\\left(y_{*} | \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{\\theta}, \\sigma^{2}\\right) \\mathcal{N}\\left(\\boldsymbol{\\theta} | \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\mathrm{d} \\boldsymbol{\\theta} \\\\\n&=\\mathcal{N}\\left(y_{*} | \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{m}_{N}, \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{S}_{N} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{*}\\right)+\\sigma^{2}\\right)\n\\end{aligned}\n\\]\nFor a point \\(x*\\)\nPredictive mean = \\(X^Tm_N\\)\nPredictive variance = \\(X^TS_NX + \\sigma^2\\)\n\nx_plus_1.T.shape, SN.shape, x_plus_1.shape\n\n((2, 50), (2, 2), (50, 2))\n\n\n\npred_var = x_plus_1@SN@x_plus_1.T\npred_var.shape\n\n(50, 50)\n\n\n\n## Marginal\nindividual_var = pred_var.diagonal()\n\n\ny_hat_map = x_plus_1@MN\n\nplt.plot(x, y_hat_map, color='black')\nplt.fill_between(x.flatten(), y_hat_map-individual_var, y_hat_map+individual_var, alpha=0.2, color='black')\nplt.scatter(x, y)\n\n<matplotlib.collections.PathCollection at 0x1a1881e450>"
  },
  {
    "objectID": "posts/2018-08-18-placement-preparation-2018-1-hashmap.html",
    "href": "posts/2018-08-18-placement-preparation-2018-1-hashmap.html",
    "title": "Placement-Preparation-2018-1-HashMap",
    "section": "",
    "text": "Problem statement\nFind all integer solutions to the problem \\(a^3 + b^3 = c^3 + d^3\\)\nwhere \\(1<=a<=n, 1<=b<=n, 1<=c<=n, 1<=d<=n\\)\n\nFirst attempt : Naive bruteforce \\(O(n^4)\\)\nLet’s write a very simple first attempt. We will write four nested loops. This will be \\(O(n^4)\\) solution.\n\ndef f1(n):\n    out = []\n    for a in range(1, n+1):\n        for b in range(1, n+1):\n            for c in range(1, n+1):\n                for d in range(1, n+1):\n                    if a**3 + b**3 == c**3 + d**3:\n                        out.append((a, b, c, d))\n    return out \n\n\nf1_time = %timeit -o f1(50)\n\n6.65 s ± 203 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nf1_time.average\n\n6.646897936570895\n\n\n\n\nSecond attempt : Reduce computations in brute force method\nLet’s now try to optimise f1. We will still use a solution of \\(O(n^4)\\) solution. However, we add one small optimisation fo f1. We break from the innermost loop once we find a match. This will hopefull save us some computations.\n\ndef f2(n):\n    out = []\n    for a in range(1, n+1):\n        for b in range(1, n+1):\n            for c in range(1, n+1):\n                for d in range(1, n+1):\n                    if a**3 + b**3 == c**3 + d**3:\n                        out.append((a, b, c, d))\n                        break\n    return out \n\n\nf2_time = %timeit -o f2(50)\n\n6.29 s ± 26.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nOk. We’re little better than f1. Every reduced computation is time saved!\n\n\nThird attempt : Reduce repeated computations by saving cubes of numbers\nOne of the student came up with an excellent observation. Why should we keep on computing cubes of numbers? This is a repeated operation. Let’s instead store them in a dictionary.\n\ndef f3(n):\n    cubes = {}\n    for x in range(1, n+1):\n        cubes[x] = x**3\n    out = []\n    for a in range(1, n+1):\n        for b in range(1, n+1):\n            for c in range(1, n+1):\n                for d in range(1, n+1):\n                    if cubes[a] + cubes[b] == cubes[c] + cubes[d]:\n                        out.append((a, b, c, d))\n                        break\n    return out \n\n\nf3_time = %timeit -o f3(50)\n\n1.05 s ± 4.11 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nOk. We now mean business! This is about 6 times quicker than our previous version.\n\n\nFourth attempt : Reduce one loop \\(O(n^3)\\)\nIn this solution, we will reduce one loop. We can solve for \\(d^3 = a^3 + b^3 - c^3\\) and find all the integer solutions. Now, there’s another clever optimisation that I have added. We can precompute the cubes and the cuberoots corresponding to numbers from 1 to N and perfect cubes from 1 to \\(N^3\\) respectively.\n\ndef f4(n):\n    cubes = {}\n    cuberoots = {}\n    for x in range(1, n+1):\n        x3 = x**3\n        cubes[x] = x3\n        cuberoots[x3] = x\n    out = []\n    for a in range(1, n+1):\n        for b in range(1, n+1):\n            for c in range(1, n+1):\n                d3 = (cubes[a] + cubes[b] - cubes[c])\n                if d3 in cuberoots:\n                    out.append((a, b, c, cuberoots[d3]))\n    return out \n\n\nf4_time = %timeit -o f4(50)\n\n21.7 ms ± 1.99 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nThis is seriously fast now!\n\n\nFifth attempt : Reduce another loop \\(O(n^2)\\)\nIn this solution, we will reduce one more loop. We can compute \\(a^3 + b^3\\) for all a, b. And then find c and d where \\(c^3 + d^3\\) is the same as \\(a^3 + b^3\\). This has a few Python tricks inside! One of the special cases to handle is of the type \\(1^3 + 2^3 = 2^3 + 1^3\\)\n\ndef f5(n):\n    out = []\n    cubes = {}\n    for x in range(1, n+1):\n        cubes[x] = x**3\n    \n    sum_a3_b3 = {}\n    for a in range(1, n+1):\n        for b in range(1, n+1):\n            temp = cubes[a]+cubes[b]\n            if temp in sum_a3_b3:    \n                sum_a3_b3[temp].append((a, b))\n            else:\n                sum_a3_b3[temp] = [(a, b)]\n\n    for c in range(1, n+1):\n        for d in range(1, n+1):\n            sum_c3_d3 = cubes[c] + cubes[d]\n            if sum_c3_d3 in sum_a3_b3:\n                for (a, b) in sum_a3_b3[sum_c3_d3]:\n                    out.append((a, b, c, d))\n\n    return out\n\n\nf5_time = %timeit -o f5(50)\n\n1.97 ms ± 235 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nPlain Wow! Going from 6 seconds to about 2 ms! Let’s plot the timings on a log scale to learn more.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ns = pd.Series({'Naive (O(N^4))':f1_time.average,\n              'Naive (O(N^4)) with break':f2_time.average,\n              'Naive (O(N^4)) with break and precomputing cubes':f3_time.average,\n              '(O(N^3))':f4_time.average,\n              '(O(N^2))':f5_time.average})\n\n\ns.plot(kind='bar', logy=True)\nplt.ylabel(\"Time\");\n\n\n\n\nHope this was fun!"
  },
  {
    "objectID": "posts/2017-04-19-nmf-out-matrix.html",
    "href": "posts/2017-04-19-nmf-out-matrix.html",
    "title": "Out of matrix non-negative matrix factorisation",
    "section": "",
    "text": "Standard Problem\nOur goal is given a matrix A, decompose it into two non-negative factors, as follows:\n$ A_{M N} W_{M K} H_{K N} $, such that $ W_{M K} $ and $ H_{K N} $\n\n\n\nOur Problem- Out of matrix factorisation\nImagine that we have trained the model for M-1 users on N movies. Now, the \\(M^{th}\\) user has rated some movies. Do we retrain the model from scratch to consider \\(M^{th}\\) user? This can be a very expensive operation!\n\nInstead, as shown in above figure, we will learn the user factor for the \\(M^{th}\\) user. We can do this the shared movie factor (H) has already been learnt.\nWe can formulate as follows:\n\\[\nA[M,:] = W[M,:]H\n\\]\nTaking transpose both sides\n\\[\nA[M,:]^T = H^T W[M,:]^T\n\\]\nHowever, \\(A[M,:]^T\\) will have missing entries. Thus, we can mask those entries from the calculation as shown below.\n\nThus, we can write\n\\[\nW[M,:]^T = \\mathrm{Least Squares} (H^T[Mask], A[M,:]^T[Mask])\n\\]\nIf instead we want the factors to be non-negative, we can use non-negative least squares instead of usual least squares for this estimation.\n\n\nCode example\nI’ll now present a simple code example to illustrate the procedure.\n\nDefining matrix A\n\nimport numpy as np\nimport pandas as pd\n\nM, N = 20, 10\n\nnp.random.seed(0)\nA_orig = np.abs(np.random.uniform(low=0.0, high=1.0, size=(M,N)))\npd.DataFrame(A_orig).head()\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      0.548814\n      0.715189\n      0.602763\n      0.544883\n      0.423655\n      0.645894\n      0.437587\n      0.891773\n      0.963663\n      0.383442\n    \n    \n      1\n      0.791725\n      0.528895\n      0.568045\n      0.925597\n      0.071036\n      0.087129\n      0.020218\n      0.832620\n      0.778157\n      0.870012\n    \n    \n      2\n      0.978618\n      0.799159\n      0.461479\n      0.780529\n      0.118274\n      0.639921\n      0.143353\n      0.944669\n      0.521848\n      0.414662\n    \n    \n      3\n      0.264556\n      0.774234\n      0.456150\n      0.568434\n      0.018790\n      0.617635\n      0.612096\n      0.616934\n      0.943748\n      0.681820\n    \n    \n      4\n      0.359508\n      0.437032\n      0.697631\n      0.060225\n      0.666767\n      0.670638\n      0.210383\n      0.128926\n      0.315428\n      0.363711\n    \n  \n\n\n\n\n\n\nMasking a few entries\n\nA = A_orig.copy()\nA[0, 0] = np.NAN\nA[3, 1] = np.NAN\nA[6, 3] = np.NAN\n\n# Masking for last user. \nA[19, 2] = np.NAN\nA[19, 7] = np.NAN\n\nWe will be using A2 (first 19 users) matrix for learning the movie factors and the user factors for the 19 users\n\nA2 = A[:-1,:]\nA2.shape\n\n(19, 10)\n\n\n\nA_df = pd.DataFrame(A)\nA_df.head()\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      NaN\n      0.715189\n      0.602763\n      0.544883\n      0.423655\n      0.645894\n      0.437587\n      0.891773\n      0.963663\n      0.383442\n    \n    \n      1\n      0.791725\n      0.528895\n      0.568045\n      0.925597\n      0.071036\n      0.087129\n      0.020218\n      0.832620\n      0.778157\n      0.870012\n    \n    \n      2\n      0.978618\n      0.799159\n      0.461479\n      0.780529\n      0.118274\n      0.639921\n      0.143353\n      0.944669\n      0.521848\n      0.414662\n    \n    \n      3\n      0.264556\n      NaN\n      0.456150\n      0.568434\n      0.018790\n      0.617635\n      0.612096\n      0.616934\n      0.943748\n      0.681820\n    \n    \n      4\n      0.359508\n      0.437032\n      0.697631\n      0.060225\n      0.666767\n      0.670638\n      0.210383\n      0.128926\n      0.315428\n      0.363711\n    \n  \n\n\n\n\n\n\nDefining matrices W and H (learning on M-1 users and N movies)\n\nK = 4\nW = np.abs(np.random.uniform(low=0, high=1, size=(M-1, K)))\nH = np.abs(np.random.uniform(low=0, high=1, size=(K, N)))\nW = np.divide(W, K*W.max())\nH = np.divide(H, K*H.max())\n\n\npd.DataFrame(W).head()\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      0\n      0.078709\n      0.175784\n      0.095359\n      0.045339\n    \n    \n      1\n      0.006230\n      0.016976\n      0.171505\n      0.114531\n    \n    \n      2\n      0.135453\n      0.226355\n      0.250000\n      0.054753\n    \n    \n      3\n      0.167387\n      0.066473\n      0.005213\n      0.191444\n    \n    \n      4\n      0.080785\n      0.096801\n      0.148514\n      0.209789\n    \n  \n\n\n\n\n\npd.DataFrame(H).head()\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      0.239700\n      0.203498\n      0.160529\n      0.222617\n      0.074611\n      0.216164\n      0.157328\n      0.003370\n      0.088415\n      0.037721\n    \n    \n      1\n      0.250000\n      0.121806\n      0.126649\n      0.162827\n      0.093851\n      0.034858\n      0.209333\n      0.048340\n      0.130195\n      0.057117\n    \n    \n      2\n      0.024914\n      0.219537\n      0.247731\n      0.244654\n      0.230833\n      0.197093\n      0.084828\n      0.020651\n      0.103694\n      0.059133\n    \n    \n      3\n      0.033735\n      0.013604\n      0.184756\n      0.002910\n      0.196210\n      0.037417\n      0.020248\n      0.022815\n      0.171121\n      0.062477\n    \n  \n\n\n\n\n\n\nDefining the cost that we want to minimise\n\ndef cost(A, W, H):\n    from numpy import linalg\n    WH = np.dot(W, H)\n    A_WH = A-WH\n    return linalg.norm(A_WH, 'fro')\n\nHowever, since A has missing entries, we have to define the cost in terms of the entries present in A\n\ndef cost(A, W, H):\n    from numpy import linalg\n    mask = pd.DataFrame(A).notnull().values\n    WH = np.dot(W, H)\n    WH_mask = WH[mask]\n    A_mask = A[mask]\n    A_WH_mask = A_mask-WH_mask\n    # Since now A_WH_mask is a vector, we use L2 instead of Frobenius norm for matrix\n    return linalg.norm(A_WH_mask, 2)\n\nLet us just try to see the cost of the initial set of values of W and H we randomly assigned. Notice, we pass A2!\n\ncost(A2, W, H)\n\n7.2333001567031294\n\n\n\n\nAlternating NNLS procedure\n\nnum_iter = 1000\nnum_display_cost = max(int(num_iter/10), 1)\nfrom scipy.optimize import nnls\n\nfor i in range(num_iter):\n    if i%2 ==0:\n        # Learn H, given A and W\n        for j in range(N):\n            mask_rows = pd.Series(A2[:,j]).notnull()\n            H[:,j] = nnls(W[mask_rows], A2[:,j][mask_rows])[0]\n    else:\n        for j in range(M-1):\n            mask_rows = pd.Series(A2[j,:]).notnull()\n            W[j,:] = nnls(H.transpose()[mask_rows], A2[j,:][mask_rows])[0]\n    WH = np.dot(W, H)\n    c = cost(A2, W, H)\n    if i%num_display_cost==0:\n        print i, c\n        \n\n0 3.74162948918\n100 2.25416363991\n200 2.25258698617\n300 2.25229707846\n400 2.25131714233\n500 2.24968386447\n600 2.24967129897\n700 2.24965023589\n800 2.24961410381\n900 2.24955008837\n\n\n\nA_pred = pd.DataFrame(np.dot(W, H))\nA_pred.head()\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      0\n      0.590301\n      0.653038\n      0.531940\n      0.623272\n      0.584763\n      0.630835\n      0.574041\n      0.700139\n      0.841706\n      0.565808\n    \n    \n      1\n      0.802724\n      0.532299\n      0.482430\n      1.017968\n      0.149923\n      0.449312\n      0.097775\n      0.708727\n      0.506595\n      0.846219\n    \n    \n      2\n      0.764296\n      0.563711\n      0.527292\n      0.905236\n      0.306275\n      0.505674\n      0.223192\n      0.705882\n      0.604356\n      0.757878\n    \n    \n      3\n      0.373539\n      0.745239\n      0.334948\n      0.663219\n      0.132686\n      0.551844\n      0.760420\n      0.598546\n      0.808108\n      0.627732\n    \n    \n      4\n      0.467623\n      0.331457\n      0.617263\n      0.239449\n      0.634455\n      0.370041\n      0.294412\n      0.288539\n      0.484822\n      0.126945\n    \n  \n\n\n\n\n\n\nLearning home factors for \\(M^{th}\\) home\n\nA_m = A[-1,:]\nA_m_transpose = A_m.T\nmask = ~np.isnan(A_m_transpose)\nW_m = nnls(H.T[mask], A_m_transpose[mask])[0]\n\n\nW_m\n\narray([ 0.12248095,  0.20778687,  0.15185613,  0.        ])\n\n\n\n\nPredicting for \\(M^{th}\\) home\n\nratings_m_home = np.dot(H.T, W_m)\n\n\nratings_m_home[~mask]\n\narray([ 0.4245947 ,  0.57447552])\n\n\n\nA_orig[-1,:][~mask]\n\narray([ 0.18619301,  0.25435648])\n\n\nThere you go, we are able to get ratings for the \\(M^{th}\\) user for the movies that they have not seen. We only trained the model on the other users! Ofcourse, these numbers might not look so impressive. However, this was just a toy example based on random data. In reality, we could expect better results!"
  },
  {
    "objectID": "posts/2014-05-01-gibbs-sampling.html",
    "href": "posts/2014-05-01-gibbs-sampling.html",
    "title": "Gibbs sampling",
    "section": "",
    "text": "Defining the network\n\nnetwork={\n    \"V\": [\"Letter\", \"Grade\", \"Intelligence\", \"SAT\", \"Difficulty\"],\n    \"E\": [[\"Intelligence\", \"Grade\"],\n        [\"Difficulty\", \"Grade\"],\n        [\"Intelligence\", \"SAT\"],\n        [\"Grade\", \"Letter\"]],\n    \"Vdata\": {\n        \"Letter\": {\n            \"ord\": 4,\n            \"numoutcomes\": 2,\n            \"vals\": [\"weak\", \"strong\"],\n            \"parents\": [\"Grade\"],\n            \"children\": None,\n            \"cprob\": {\n                \"['A']\": [.1, .9],\n                \"['B']\": [.4, .6],\n                \"['C']\": [.99, .01]\n            }\n        },\n\n        \"SAT\": {\n            \"ord\": 3,\n            \"numoutcomes\": 2,\n            \"vals\": [\"lowscore\", \"highscore\"],\n            \"parents\": [\"Intelligence\"],\n            \"children\": None,\n            \"cprob\": {\n                \"['low']\": [.95, .05],\n                \"['high']\": [.2, .8]\n            }\n        },\n\n        \"Grade\": {\n            \"ord\": 2,\n            \"numoutcomes\": 3,\n            \"vals\": [\"A\", \"B\", \"C\"],\n            \"parents\": [\"Difficulty\", \"Intelligence\"],\n            \"children\": [\"Letter\"],\n            \"cprob\": {\n                \"['easy', 'low']\": [.3, .4, .3],\n                \"['easy', 'high']\": [.9, .08, .02],\n                \"['hard', 'low']\": [.05, .25, .7],\n                \"['hard', 'high']\": [.5, .3, .2]\n            }\n        },\n\n        \"Intelligence\": {\n            \"ord\": 1,\n            \"numoutcomes\": 2,\n            \"vals\": [\"low\", \"high\"],\n            \"parents\": None,\n            \"children\": [\"SAT\", \"Grade\"],\n            \"cprob\": [.7, .3]\n        },\n\n        \"Difficulty\": {\n            \"ord\": 0,\n            \"numoutcomes\": 2,\n            \"vals\": [\"easy\", \"hard\"],\n            \"parents\": None,\n            \"children\": [\"Grade\"],\n            \"cprob\":  [.6, .4]\n        }\n    }\n}\n\n\n\nDrawing the Bayesian Network\n\npgm = daft.PGM([8, 8], origin=[0, 0])\n\npgm.add_node(daft.Node('Difficulty',r\"Difficulty\",2,6,aspect=3))   \npgm.add_node(daft.Node('Intelligence',r\"Intelligence\",5,6,aspect=3))\npgm.add_node(daft.Node('Grade',r\"Grade\",3,4,aspect=3))    \npgm.add_node(daft.Node('SAT',r\"SAT\",6,4,aspect=3))    \npgm.add_node(daft.Node('Letter',r\"Letter\",4,2,aspect=3))   \n\nfor node in network['Vdata']:\n    parents=network['Vdata'][node]['parents']\n    if parents is not None:        \n        for parent in parents:\n            pgm.add_edge(parent, node) \npgm.render()\n\n<matplotlib.axes._axes.Axes at 0x10e58b110>\n\n\n\n\n\n\n\nFinding the Markov blanket of a node\n\ndef find_markov_blanket(node,network):\n    '''\n    Find the Markov Blanket of the node in the given network\n    Markov Blanket is given by:\n    1. The parents of the node\n    2. The children of the node\n    3. The parents of the children of the node\n    '''\n    \n    mb=[]\n    #Finding the parents of the node\n    parents=network['Vdata'][node]['parents']\n    if parents is not None:\n        mb.append(parents)\n    \n    #Finding children of the node\n    children=network['Vdata'][node]['children']\n    if children is not None:\n        mb.append(children)\n        \n        #Finding parent of each node\n        for child in children:\n            parents_child=network['Vdata'][child]['parents']\n            if parents is not None:\n                mb.append(parents)\n                \n    #Flattening out list of lists\n    mb=list(itertools.chain(*mb)) \n    \n    #Removing repeated elements\n    mb=list(set(mb))\n    return mb\n\n\nfind_markov_blanket('Grade',network)\n\n['Difficulty', 'Letter', 'Intelligence']\n\n\n\n\nGibbs Sampling Procedures\n\n\nAssigning a random state to a node in the network\n\ndef pick_random(node,network):\n    '''\n    Assigns a random state to a given node\n    N\n    '''\n    num_outcomes=network['Vdata'][node][\"numoutcomes\"]\n    random_index=random.randint(0,num_outcomes-1)\n    return network['Vdata'][node][\"vals\"][random_index]\n\n\npick_random('SAT',network)\n\n'lowscore'\n\n\n\n\nPick a random non evidence node to the update in the current iteration\n\ndef pick_random_non_evidence_node(non_evidence_nodes):\n    return non_evidence_nodes[random.randint(0,len(non_evidence_nodes)-1)]\n\n\n\nUpdate the value of a node given assignment in previous iteration\n\ndef get_next_value(node, network,simulation):\n    parents_current_node_to_update=network['Vdata'][node]['parents']\n    if parents_current_node_to_update is None:\n        #The node has no parent and we can update it based on the prior\n        cumsum=np.cumsum(network['Vdata'][node][\"cprob\"])    \n    else:\n        #Find the row corresponding to the values of the parents in the previous iteration\n        #NB We need to maintain the order, so we will do it \n        values_parents=[simulation[-1][parent] for parent in parents_current_node_to_update]\n        row=network['Vdata'][node][\"cprob\"][str(values_parents)]\n        cumsum=np.cumsum(row)\n    choice=random.random()\n    index=np.argmax(cumsum>choice)\n    return network['Vdata'][node][\"vals\"][index]\n    \n\n\n\nMain procedure: Iteratively pick up a non evidence node to update\n\ndef gibbs_sampling(network, evidence, niter=2):\n    simulation=[]\n    nodes=network['V']\n    non_evidence_nodes=[node for node in nodes if node not in evidence.keys()]        \n    #First iteration random value for all nodes\n    d={}\n    for node in nodes:\n        d[node]=pick_random(node,network)    \n    #Put evidence\n    for node in evidence:\n        d[node]=evidence[node]        \n    simulation.append(d.copy())        \n    #Now iterate \n    for count in xrange(niter):\n        #Pick up a random node to start\n        current_node_to_update=pick_random_non_evidence_node(non_evidence_nodes)\n        d[current_node_to_update]=get_next_value(current_node_to_update,network,simulation)\n        simulation.append(d.copy())\n    return simulation      \n\n\nIllustration 1\n\n\n\nDistribution of Letter given that the student is Intelligent\n\niterations=int(1e4)\nsim=gibbs_sampling(network, {\"Intelligence\":\"high\"},iterations)\n\nRemoving first 10% samples\n\nafter_removing_burnt_samples=sim[iterations/10:]\ncount={val:0 for val in network['Vdata']['Letter']['vals']}\n\nFinding the distribution of letter\n\nfor assignment in after_removing_burnt_samples:\n    count[assignment['Letter']]+=1    \n\nCounts\n\ncount\n\n{'strong': 7061, 'weak': 1940}\n\n\nCounts to Probabilites\n\nprobabilites={}\nfor l in count:\n    probabilites[l]=count[l]*1.0/(.90*iterations)\nprobabilites\n\n{'strong': 0.7845555555555556, 'weak': 0.21555555555555556}\n\n\n\n\nWait a min! What about the marginal distribution of Letter given NO evidence\n\niterations=int(1e4)\nsim=gibbs_sampling(network, {},iterations)\nafter_removing_burnt_samples=sim[iterations/10:]\ncount={val:0 for val in network['Vdata']['Letter']['vals']}\nfor assignment in after_removing_burnt_samples:\n    count[assignment['Letter']]+=1\nprobabilites_no_evidence={}\nfor l in count:\n    probabilites_no_evidence[l]=count[l]*1.0/(.90*iterations)\nprobabilites_no_evidence\n\n{'strong': 0.4766666666666667, 'weak': 0.5234444444444445}\n\n\n\n\nHow does the evidence about “Intelligent” student affect the quality of letters?\n\nplt.figure(figsize=(10, 8))\nplt.subplot(2,2,1)\nplt.bar(range(2),[probabilites['strong'],probabilites['weak']])\nplt.xticks([0.5,1.5],['Strong','Weak'])\nplt.title('Letter Quality given Intelligent Student')\nplt.ylim((0,1.0))\nplt.subplot(2,2,2)\nplt.bar(range(2),[probabilites_no_evidence['strong'],probabilites_no_evidence['weak']])\nplt.xticks([0.5,1.5],['Strong','Weak'])\nplt.title('Letter Quality given no prior information')\nplt.ylim((0,1.0));"
  },
  {
    "objectID": "posts/2018-01-13-denoising.html",
    "href": "posts/2018-01-13-denoising.html",
    "title": "Signal denoising using RNNs in PyTorch",
    "section": "",
    "text": "Problem description\nGiven a noisy sine wave as an input, we want to estimate the denoised signal. This is shown in the figure below.\n\n\n\nCustomary imports\n\nimport numpy as np\nimport math, random\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nnp.random.seed(0)\n\n\n\nCreating noisy and denoised signals\nLet’s now write functions to cerate a sine wave, add some noise on top of it. This way we’re able to create a noisy verison of the sine wave.\n\n# Generating a clean sine wave \ndef sine(X, signal_freq=60.):\n    return np.sin(2 * np.pi * (X) / signal_freq)\n\n# Adding uniform noise\ndef noisy(Y, noise_range=(-0.35, 0.35)):\n    noise = np.random.uniform(noise_range[0], noise_range[1], size=Y.shape)\n    return Y + noise\n\n# Create a noisy and clean sine wave \ndef sample(sample_size):\n    random_offset = random.randint(0, sample_size)\n    X = np.arange(sample_size)\n    out = sine(X + random_offset)\n    inp = noisy(out)\n    return inp, out\n\nLet’s now invoke the functions we defined to generate the figure we saw in the problem description.\n\ninp, out = sample(100)\nplt.plot(inp, label='Noisy')\nplt.plot(out, label ='Denoised')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x106beb828>\n\n\n\n\n\n\n\nCreating dataset\nNow, let’s write a simple function to generate a dataset of such noisy and denoised samples.\n\ndef create_dataset(n_samples=10000, sample_size=100):\n    data_inp = np.zeros((n_samples, sample_size))\n    data_out = np.zeros((n_samples, sample_size))\n    \n    for i in range(n_samples):\n        sample_inp, sample_out = sample(sample_size)\n        data_inp[i, :] = sample_inp\n        data_out[i, :] = sample_out\n    return data_inp, data_out\n\nNow, creating the dataset, and dividing it into train and test set.\n\ndata_inp, data_out = create_dataset()\ntrain_inp, train_out = data_inp[:8000], data_out[:8000]\ntest_inp, test_out = data_inp[8000:], data_out[8000:]\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\n\nCreating RNN\nWe have 1d sine waves, which we want to denoise. Thus, we have input dimension of 1. Let’s create a simple 1-layer RNN with 30 hidden units.\n\ninput_dim = 1\nhidden_size = 30\nnum_layers = 1\n\nclass CustomRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(CustomRNN, self).__init__()\n        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n        self.linear = nn.Linear(hidden_size, output_size, )\n        self.act = nn.Tanh()\n    def forward(self, x):\n        pred, hidden = self.rnn(x, None)\n        pred = self.act(self.linear(pred)).view(pred.data.shape[0], -1, 1)\n        return pred\n\nr= CustomRNN(input_dim, hidden_size, 1)\n\n\nr\n\nCustomRNN (\n  (rnn): RNN(1, 30, batch_first=True)\n  (linear): Linear (30 -> 1)\n  (act): Tanh ()\n)\n\n\n\n\nTraining\n\n# Storing predictions per iterations to visualise later\npredictions = []\n\noptimizer = torch.optim.Adam(r.parameters(), lr=1e-2)\nloss_func = nn.L1Loss()\n\nfor t in range(301):\n    hidden = None\n    inp = Variable(torch.Tensor(train_inp.reshape((train_inp.shape[0], -1, 1))), requires_grad=True)\n    out = Variable(torch.Tensor(train_out.reshape((train_out.shape[0], -1, 1))) )\n    pred = r(inp)\n    optimizer.zero_grad()\n    predictions.append(pred.data.numpy())\n    loss = loss_func(pred, out)\n    if t%20==0:\n        print(t, loss.data[0])\n    loss.backward()\n    optimizer.step()\n\n0 0.5774930715560913\n20 0.12028147280216217\n40 0.11251863092184067\n60 0.10834833979606628\n80 0.11243857443332672\n100 0.11533079296350479\n120 0.09951132535934448\n140 0.078636534512043\n160 0.08674494177103043\n180 0.07217984646558762\n200 0.06266186386346817\n220 0.05793667957186699\n240 0.0723448321223259\n260 0.05628745257854462\n280 0.050240203738212585\n300 0.06297950446605682\n\n\nGreat. As expected, the loss reduces over time.\n\n\nGenerating prediction on test set\n\nt_inp = Variable(torch.Tensor(test_inp.reshape((test_inp.shape[0], -1, 1))), requires_grad=True)\npred_t = r(t_inp)\n\n\n# Test loss\nprint(loss_func(pred_t, Variable(torch.Tensor(test_out.reshape((test_inp.shape[0], -1, 1))))).data[0])\n\n0.06105425953865051\n\n\n\n\nVisualising sample denoising\n\nsample_num = 23\nplt.plot(pred_t[sample_num].data.numpy(), label='Pred')\nplt.plot(test_out[sample_num], label='GT')\nplt.legend()\nplt.title(\"Sample num: {}\".format(sample_num))\n\n<matplotlib.text.Text at 0x1064675c0>\n\n\n\n\n\n\n\nBidirectional RNN\nSeems reasonably neat to me! If only the first few points were better esimtated. Any idea why they’re not? Maybe, we need a bidirectional RNN? Let’s try one, and I’ll also add dropout to prevent overfitting.\n\nbidirectional = True\nif bidirectional:\n    num_directions = 2\nelse:\n    num_directions = 1\nclass CustomRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(CustomRNN, self).__init__()\n        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, \n                          batch_first=True, bidirectional=bidirectional, dropout=0.1)\n        self.linear = nn.Linear(hidden_size*num_directions, output_size, )\n        self.act = nn.Tanh()\n    def forward(self, x):\n        pred, hidden = self.rnn(x, None)\n        pred = self.act(self.linear(pred)).view(pred.data.shape[0], -1, 1)\n        return pred\n\nr= CustomRNN(input_dim, hidden_size, 1)\nr\n\nCustomRNN (\n  (rnn): RNN(1, 30, batch_first=True, dropout=0.1, bidirectional=True)\n  (linear): Linear (60 -> 1)\n  (act): Tanh ()\n)\n\n\n\n# Storing predictions per iterations to visualise later\npredictions = []\n\noptimizer = torch.optim.Adam(r.parameters(), lr=1e-2)\nloss_func = nn.L1Loss()\n\nfor t in range(301):\n    hidden = None\n    inp = Variable(torch.Tensor(train_inp.reshape((train_inp.shape[0], -1, 1))), requires_grad=True)\n    out = Variable(torch.Tensor(train_out.reshape((train_out.shape[0], -1, 1))) )\n    pred = r(inp)\n    optimizer.zero_grad()\n    predictions.append(pred.data.numpy())\n    loss = loss_func(pred, out)\n    if t%20==0:\n        print(t, loss.data[0])\n    loss.backward()\n    optimizer.step()\n\n0 0.6825199127197266\n20 0.11104971915483475\n40 0.07732641696929932\n60 0.07210152596235275\n80 0.06964801251888275\n100 0.06717491149902344\n120 0.06266810745000839\n140 0.06302479654550552\n160 0.05954732000827789\n180 0.05402040109038353\n200 0.05266999825835228\n220 0.06145058199763298\n240 0.0500367134809494\n260 0.05388529226183891\n280 0.053044941276311874\n300 0.046826526522636414\n\n\n\nt_inp = Variable(torch.Tensor(test_inp.reshape((test_inp.shape[0], -1, 1))), requires_grad=True)\npred_t = r(t_inp)\n\n\n# Test loss\nprint(loss_func(pred_t, Variable(torch.Tensor(test_out.reshape((test_inp.shape[0], -1, 1))))).data[0])\n\n0.050666142255067825\n\n\n\nsample_num = 23\nplt.plot(pred_t[sample_num].data.numpy(), label='Pred')\nplt.plot(test_out[sample_num], label='GT')\nplt.legend()\nplt.title(\"Sample num: {}\".format(sample_num))\n\n<matplotlib.text.Text at 0x126f22710>\n\n\n\n\n\nHmm. The estimated signal looks better for the initial few points. But, gets worse for the final few points. Oops! Guess, now the reverse RNN causes problems for its first few points!\n\n\nFrom RNNs to GRU\nLet’s now replace our RNN with GRU to see if the model improves.\n\nbidirectional = True\nif bidirectional:\n    num_directions = 2\nelse:\n    num_directions = 1\nclass CustomRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(CustomRNN, self).__init__()\n        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, \n                          batch_first=True, bidirectional=bidirectional, dropout=0.1)\n        self.linear = nn.Linear(hidden_size*num_directions, output_size, )\n        self.act = nn.Tanh()\n    def forward(self, x):\n        pred, hidden = self.rnn(x, None)\n        pred = self.act(self.linear(pred)).view(pred.data.shape[0], -1, 1)\n        return pred\n\nr= CustomRNN(input_dim, hidden_size, 1)\nr\n\nCustomRNN (\n  (rnn): GRU(1, 30, batch_first=True, dropout=0.1, bidirectional=True)\n  (linear): Linear (60 -> 1)\n  (act): Tanh ()\n)\n\n\n\n# Storing predictions per iterations to visualise later\npredictions = []\n\noptimizer = torch.optim.Adam(r.parameters(), lr=1e-2)\nloss_func = nn.L1Loss()\n\nfor t in range(201):\n    hidden = None\n    inp = Variable(torch.Tensor(train_inp.reshape((train_inp.shape[0], -1, 1))), requires_grad=True)\n    out = Variable(torch.Tensor(train_out.reshape((train_out.shape[0], -1, 1))) )\n    pred = r(inp)\n    optimizer.zero_grad()\n    predictions.append(pred.data.numpy())\n    loss = loss_func(pred, out)\n    if t%20==0:\n        print(t, loss.data[0])\n    loss.backward()\n    optimizer.step()\n\n0 0.6294281482696533\n20 0.11452394723892212\n40 0.08548719435930252\n60 0.07101015746593475\n80 0.05964939296245575\n100 0.053830236196517944\n120 0.06312716007232666\n140 0.04494623467326164\n160 0.04309168830513954\n180 0.04010637104511261\n200 0.035212572664022446\n\n\n\nt_inp = Variable(torch.Tensor(test_inp.reshape((test_inp.shape[0], -1, 1))), requires_grad=True)\npred_t = r(t_inp)\n\n\n# Test loss\nprint(loss_func(pred_t, Variable(torch.Tensor(test_out.reshape((test_inp.shape[0], -1, 1))))).data[0])\n\n0.03618593513965607\n\n\n\nsample_num = 23\nplt.plot(pred_t[sample_num].data.numpy(), label='Pred')\nplt.plot(test_out[sample_num], label='GT')\nplt.legend()\nplt.title(\"Sample num: {}\".format(sample_num))\n\n<matplotlib.text.Text at 0x11661e208>\n\n\n\n\n\nThe GRU prediction seems to far better! Maybe, the RNNs suffer from the vanishing gradients problem?\n\n\nVisualising estimations as model improves\nLet’s now write a simple function to visualise the estimations as a function of iterations. We’d expect the estimations to improve over time.\n\nplt.rcParams['animation.ffmpeg_path'] = './ffmpeg'\nfrom matplotlib.animation import FuncAnimation\n\nfig, ax = plt.subplots(figsize=(4, 3))\nfig.set_tight_layout(True)\n\n# Query the figure's on-screen size and DPI. Note that when saving the figure to\n# a file, we need to provide a DPI for that separately.\nprint('fig size: {0} DPI, size in inches {1}'.format(\n    fig.get_dpi(), fig.get_size_inches()))\n\ndef update(i):\n    label = 'Iteration {0}'.format(i)\n    ax.cla()\n    ax.plot(np.array(predictions)[i, 0, :, 0].T, label='Pred')\n    ax.plot(train_out[0, :], label='GT')\n    ax.legend()\n    ax.set_title(label)\n \n\nanim = FuncAnimation(fig, update, frames=range(0, 201, 4), interval=20)\nanim.save('learning.mp4',fps=20)\nplt.close()\n\nfig size: 72.0 DPI, size in inches [ 4.  3.]\n\n\n\nfrom IPython.display import Video\nVideo(\"learning.mp4\")\n\n\n      Your browser does not support the video element.\n    \n\n\nThis looks great! We can see how our model learns to learn reasonably good denoised signals over time. It doesn’t start great though. Would a better initialisation help? I certainly feel that for this particular problem it would, as predicting the output the same as input is a good starting point!\n\n\nBonus: Handling missing values in denoised training data\nThe trick to handling missing values in the denoised training data (the quantity we wish to estimate) is to compute the loss only over the present values. This requires creating a mask for finding all entries except missing.\nOne such way to do so would be: mask = out > -1* 1e8 where out is the tensor containing missing values.\nLet’s first add some unknown values (np.NaN) in the training output data.\n\nfor num_unknown_values in range(50):\n    train_out[np.random.choice(list(range(0, 8000))), np.random.choice(list(range(0, 100)))] = np.NAN\n\n\nnp.isnan(train_out).sum()\n\n50\n\n\nTesting using a network with few parameters.\n\nr= CustomRNN(input_dim, 2, 1)\nr\n\nCustomRNN (\n  (rnn): GRU(1, 30, batch_first=True, dropout=0.1, bidirectional=True)\n  (linear): Linear (60 -> 1)\n  (act): Tanh ()\n)\n\n\n\n# Storing predictions per iterations to visualise later\npredictions = []\n\noptimizer = torch.optim.Adam(r.parameters(), lr=1e-2)\nloss_func = nn.L1Loss()\n\nfor t in range(20):\n    hidden = None\n    inp = Variable(torch.Tensor(train_inp.reshape((train_inp.shape[0], -1, 1))), requires_grad=True)\n    out = Variable(torch.Tensor(train_out.reshape((train_out.shape[0], -1, 1))) )\n    pred = r(inp)\n    optimizer.zero_grad()\n    predictions.append(pred.data.numpy())\n    # Create a mask to compute loss only on defined quantities\n    mask = out > -1* 1e8\n    loss = loss_func(pred[mask], out[mask])\n    if t%20==0:\n        print(t, loss.data[0])\n    loss.backward()\n    optimizer.step()\n\n0 0.6575785279273987\n\n\nThere you go! We’ve also learnt how to handle missing values!\nI must thank Simon Wang and his helpful inputs on the PyTorch discussion forum."
  },
  {
    "objectID": "posts/2020-03-08-keras-neural-non-linear.html",
    "href": "posts/2020-03-08-keras-neural-non-linear.html",
    "title": "Some Neural Network Classification",
    "section": "",
    "text": "X, y = make_moons()\n\n\nplt.scatter(X[:, 0], X[:, 1], c= y)\n\n<matplotlib.collections.PathCollection at 0x126d7af50>\n\n\n\n\n\n\nfrom keras.models import Sequential\nfrom sklearn.metrics import accuracy_score\nimport os\n\n\nfrom keras.layers import Dense, Activation\nfrom keras.utils import to_categorical\nmodel_simple = Sequential([\n    Dense(1, input_shape=(2,)),\n    Activation('relu'),\n    Dense(2),\n    Activation('softmax'),\n])\n\n\n\nmodel_complex = Sequential([\n    Dense(6, input_shape=(2,)),\n    Activation('relu'),\n    Dense(4),\n    Activation('relu'),\n    Dense(3),\n    Activation('relu'),\n    Dense(2),\n    Activation('softmax'),\n])\n\nmodel_complex_2 = Sequential([\n    Dense(10, input_shape=(2,)),\n    Activation('relu'),\n    Dense(8, ),\n    Activation('relu'),\n    Dense(8),\n    Activation('relu'),\n    Dense(2),\n    Activation('softmax'),\n])\n\n\nmodel_simple.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel_complex.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel_complex_2.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n\ndef make_plot(X, y, model, dataset, model_type, noise, n_iter=80,cmap='PRGn'):\n\n    h=200\n    if dataset==\"moon\":\n        X, y = make_moons(noise=noise)\n    if dataset==\"iris\":\n        X, y = load_iris()['data'][:, :2], load_iris()['target']\n    print(X.shape, y.shape)\n    y_binary = to_categorical(y)\n\n    xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-0.2, X[:, 0].max()+0.2, h),\n                             np.linspace(X[:, 1].min()-0.2, X[:, 1].max()+0.2, h))\n    XX = np.c_[xx.ravel(), yy.ravel()]\n\n    for i in range(n_iter):\n        model.fit(X, y_binary, epochs=1, verbose=0)\n        Z = np.argmax(model.predict(XX), axis=1).reshape(xx.shape)\n        y_hat = np.argmax(model.predict(X), axis=1)\n        train_accuracy = accuracy_score(y, y_hat)\n        contours = plt.contourf(xx, yy, Z, h , cmap=cmap, alpha=0.4)\n        plt.title(\"Iteration: \"+str(i)+\"\\n Accuracy:\"+str(train_accuracy))\n        plt.colorbar()\n        plt.scatter(X[:, 0], X[:, 1], c= y, cmap=cmap)\n        if not os.path.exists(f\"/Users/nipun/Desktop/animation-keras/{dataset}/{model_type}/{noise}/\"):\n            os.makedirs(f\"/Users/nipun/Desktop/animation-keras/{dataset}/{model_type}/{noise}/\")\n        plt.savefig(f\"/Users/nipun/Desktop/animation-keras/{dataset}/{model_type}/{noise}/{i:03}.png\")\n        plt.clf()\n    \n\n\nmake_plot(X, y, model_simple, \"moon\", \"simple\", None)\n\n(100, 2) (100,)\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n!convert -delay 20 -loop 0 /Users/nipun/Desktop/animation-keras/moon/simple/None/*.png moon-simple-none.gif\n\n\n\nmake_plot(X, y, model_complex, \"moon\", \"complex\", None, 500)\n\n(100, 2) (100,)\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n!convert -delay 20 -loop 0 /Users/nipun/Desktop/animation-keras/moon/complex/None/*.png moon-complex-none.gif\n\n\n\nmake_plot(X, y, model_complex_2, \"moon\", \"complex\", 0.3, 700)\n\n(100, 2) (100,)\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n!convert -delay 20 -loop 0 /Users/nipun/Desktop/animation-keras/moon/complex/0.3/*.png moon-complex-03.gif\n\n\n\nmodel_simple_2 = Sequential([\n    Dense(1, input_shape=(2,)),\n    Activation('relu'),\n    Dense(3),\n    Activation('softmax'),\n])\n\nmodel_simple_2.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nmake_plot(X, y, model_simple_2, \"iris\", \"simple\", None, 500)\n\n(150, 2) (150,)\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n!convert -delay 20 -loop 0 /Users/nipun/Desktop/animation-keras/iris/simple/None/*.png iris-simple.gif\n\n\n\nmodel_complex_iris = Sequential([\n    Dense(12, input_shape=(2,)),\n    Activation('relu'),\n    Dense(6),\n    Activation('relu'),\n    Dense(4),\n    Activation('relu'),\n    Dense(3),\n    Activation('softmax'),\n])\n\nmodel_complex_iris.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nmake_plot(X, y, model_complex_iris, \"iris\", \"complex\", None, 500)\n\n(150, 2) (150,)\n\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n!convert -delay 20 -loop 0 /Users/nipun/Desktop/animation-keras/iris/complex/None/*.png iris-complex.gif"
  },
  {
    "objectID": "posts/2022-02-12-variational-inference.html",
    "href": "posts/2022-02-12-variational-inference.html",
    "title": "Variational Inference",
    "section": "",
    "text": "Basic Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport seaborn as sns\nimport pandas as pd\n\ndist =torch.distributions\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\n\nCreating distributions\n\nCreating \\(p\\sim\\mathcal{N}(1.00, 4.00)\\)\n\np = dist.Normal(1, 4)\n\n\nz_values = torch.linspace(-5, 15, 200)\nprob_values_p = torch.exp(p.log_prob(z_values))\nplt.plot(z_values, prob_values_p, label=r\"$p\\sim\\mathcal{N}(1.00, 4.00)$\")\nsns.despine()\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\n\nText(0, 0.5, 'PDF')\n\n\n\n\n\n\n\nCreating \\(q\\sim\\mathcal{N}(loc, scale)\\)\n\ndef create_q(loc, scale):\n    return dist.Normal(loc, scale)\n\n\n\nGenerating a few qs for different location and scale value\n\nq = {}\nq[(0, 1)] = create_q(0.0, 1.0)\n\nfor loc in [0, 1]:\n    for scale in [1, 2]:\n        q[(loc, scale)] = create_q(float(loc), float(scale))\n\n\nplt.plot(z_values, prob_values_p, label=r\"$p\\sim\\mathcal{N}(1.00, 4.00)$\", lw=3)\nplt.plot(\n    z_values,\n    torch.exp(create_q(0.0, 2.0).log_prob(z_values)),\n    label=r\"$q_1\\sim\\mathcal{N}(0.00, 2.00)$\",\n    lw=2,\n    linestyle=\"--\",\n)\nplt.plot(\n    z_values,\n    torch.exp(create_q(1.0, 3.0).log_prob(z_values)),\n    label=r\"$q_2\\sim\\mathcal{N}(1.00, 3.00)$\",\n    lw=2,\n    linestyle=\"-.\",\n)\n\nplt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\nsns.despine()\nplt.tight_layout()\nplt.savefig(\n    \"dkl.png\",\n    dpi=150,\n)\n\n\n\n\n\n#### Computing KL-divergence\n\nq_0_2_dkl = dist.kl_divergence(create_q(0.0, 2.0), p)\nq_1_3_dkl = dist.kl_divergence(create_q(1.0, 3.0), p)\n\nprint(f\"D_KL (q(0, 2)||p) = {q_0_2_dkl:0.2f}\")\nprint(f\"D_KL (q(1, 3)||p) = {q_1_3_dkl:0.2f}\")\n\nD_KL (q(0, 2)||p) = 0.35\nD_KL (q(1, 3)||p) = 0.07\n\n\nAs mentioned earlier, clearly, \\(q_2\\sim\\mathcal{N}(1.00, 3.00)\\) seems closer to \\(p\\)\n\n\n\nOptimizing the KL-divergence between q and p\nWe could create a grid of (loc, scale) pairs and find the best, as shown below.\n\nplt.plot(z_values, prob_values_p, label=r\"$p\\sim\\mathcal{N}(1.00, 4.00)$\", lw=5)\n\n\nfor loc in [0, 1]:\n    for scale in [1, 2]:\n        q_d = q[(loc, scale)]\n        kl_d = dist.kl_divergence(q[(loc, scale)], p)\n        plt.plot(\n            z_values,\n            torch.exp(q_d.log_prob(z_values)),\n            label=rf\"$q\\sim\\mathcal{{N}}({loc}, {scale})$\"\n            + \"\\n\"\n            + rf\"$D_{{KL}}(q||p)$ = {kl_d:0.2f}\",\n        )\nplt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\nsns.despine()\n\n\n\n\nOr, we could use continuous optimization to find the best loc and scale parameters for q.\n\nloc = torch.tensor(8.0, requires_grad=True)\nscale = torch.tensor(0.1, requires_grad=True)\n\n\nloc_array = []\nscale_array = []\nloss_array = []\nopt = torch.optim.Adam([loc, scale], lr=0.05)\nfor i in range(401):\n    scale_softplus = torch.functional.F.softplus(scale)\n\n    to_learn = dist.Normal(loc=loc, scale=scale_softplus)\n    loss = dist.kl_divergence(to_learn, p)\n    loss_array.append(loss.item())\n    loc_array.append(to_learn.loc.item())\n    scale_array.append(to_learn.scale.item())\n\n    loss.backward()\n    if i % 100 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss.item():0.2f}, Loc: {loc.item():0.2f}, Scale: {scale_softplus.item():0.2f}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 2.73, Loc: 8.00, Scale: 0.74\nIteration: 100, Loss: 0.26, Loc: 3.75, Scale: 3.46\nIteration: 200, Loss: 0.01, Loc: 1.68, Scale: 3.98\nIteration: 300, Loss: 0.00, Loc: 1.10, Scale: 4.00\nIteration: 400, Loss: 0.00, Loc: 1.01, Scale: 4.00\n\n\n\nplt.plot(torch.tensor(scale_array))\nplt.plot(torch.tensor(loc_array))\nplt.plot(torch.tensor(loss_array))\n\n\n\n\nAfter training, we are able to recover the scale and loc very close to that of \\(p\\)\n\n\nAnimation!\n\nfrom matplotlib import animation\n\nfig = plt.figure(tight_layout=True, figsize=(8, 4))\nax = fig.gca()    \n\n\ndef animate(i):\n    ax.clear()\n    ax.plot(z_values, prob_values_p, label=r\"$p\\sim\\mathcal{N}(1.00, 4.00)$\", lw=5)\n    to_learn_q = dist.Normal(loc = loc_array[i], scale=scale_array[i])\n    loss = loss_array[i]\n    ax.plot(\n        z_values,\n        torch.exp(to_learn_q.log_prob(z_values)),\n        label=rf\"$q\\sim \\mathcal{{N}}({loc:0.2f}, {scale:0.2f})$\",\n    )\n\n    ax.set_title(rf\"Iteration: {i}, $D_{{KL}}(q||p)$: {loss:0.2f}\")\n    ax.legend(bbox_to_anchor=(1.1, 1), borderaxespad=0)\n    ax.set_ylim((0, 1))\n    ax.set_xlim((-5, 15))\n\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"PDF\")\n    sns.despine()\n\n\nani = animation.FuncAnimation(fig, animate, frames=350)\nplt.close()\n\n\nani.save(\"kl_qp.gif\", writer=\"imagemagick\", fps=60)\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\nFinding the KL divergence for two distributions from different families\nLet us rework our example with p coming from a mixture of Gaussian distribution and q being Normal.\n\np_s = dist.MixtureSameFamily(\n    mixture_distribution=dist.Categorical(probs=torch.tensor([0.5, 0.5])),\n    component_distribution=dist.Normal(\n        loc=torch.tensor([-0.2, 1]), scale=torch.tensor([0.4, 0.5])  # One for each component.\n    ),\n)  \n\np_s\n\nMixtureSameFamily(\n  Categorical(probs: torch.Size([2]), logits: torch.Size([2])),\n  Normal(loc: torch.Size([2]), scale: torch.Size([2])))\n\n\n\nplt.plot(z_values, torch.exp(p_s.log_prob(z_values)))\nsns.despine()\n\n\n\n\nLet us create two Normal distributions q_1 and q_2 and plot them to see which looks closer to p_s.\n\nq_1 = create_q(3, 1)\nq_2 = create_q(3, 4.5)\n\n\nprob_values_p_s = torch.exp(p_s.log_prob(z_values))\nprob_values_q_1 = torch.exp(q_1.log_prob(z_values))\nprob_values_q_2 = torch.exp(q_2.log_prob(z_values))\n\nplt.plot(z_values, prob_values_p_s, label=r\"MOG\")\nplt.plot(z_values, prob_values_q_1, label=r\"$q_1\\sim\\mathcal{N} (3, 1.0)$\")\nplt.plot(z_values, prob_values_q_2, label=r\"$q_2\\sim\\mathcal{N} (3, 4.5)$\")\n\nsns.despine()\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\nplt.tight_layout()\nplt.savefig(\n    \"dkl-different.png\",\n    dpi=150,\n)\n\n\n\n\n\ntry:\n    dist.kl_divergence(q_1, p_s)\nexcept NotImplementedError:\n    print(f\"KL divergence not implemented between {q_1.__class__} and {p_s.__class__}\")\n\nKL divergence not implemented between <class 'torch.distributions.normal.Normal'> and <class 'torch.distributions.mixture_same_family.MixtureSameFamily'>\n\n\nAs we see above, we can not compute the KL divergence directly. The core idea would now be to leverage the Monte Carlo sampling and generating the expectation. The following function does that.\n\ndef kl_via_sampling(q, p, n_samples=100000):\n    # Get samples from q\n    sample_set = q.sample([n_samples])\n    # Use the definition of KL-divergence\n    return torch.mean(q.log_prob(sample_set) - p.log_prob(sample_set))\n\n\ndist.kl_divergence(q_1, q_2)\n\ntensor(1.0288)\n\n\n\nkl_via_sampling(q_1, q_2)\n\ntensor(1.0268)\n\n\n\nkl_via_sampling(q_1, p_s), kl_via_sampling(q_2, p_s)\n\n(tensor(9.4963), tensor(45.4601))\n\n\nAs we can see from KL divergence calculations, q_1 is closer to our Gaussian mixture distribution.\n\n\nOptimizing the KL divergence for two distributions from different families\nWe saw that we can calculate the KL divergence between two different distribution families via sampling. But, as we did earlier, will we be able to optimize the parameters of our target surrogate distribution? The answer is No! As we have introduced sampling. However, there is still a way – by reparameterization!\nOur surrogate q in this case is parameterized by loc and scale. The key idea here is to generate samples from a standard normal distribution (loc=0, scale=1) and then apply an affine transformation on the generated samples to get the samples generated from q. See my other post on sampling from normal distribution to understand this better.\nThe loss can now be thought of as a function of loc and scale.\n\nn_samples = 1000\n\n\ndef loss(loc, scale):\n    q = dist.Normal(loc=loc, scale=scale)\n    std_normal = dist.Normal(loc=0.0, scale=1.0)\n    sample_set = std_normal.sample([n_samples])\n    sample_set = loc + scale * sample_set\n    return torch.mean(q.log_prob(sample_set) - p_s.log_prob(sample_set))\n\nHaving defined the loss above, we can now optimize loc and scale to minimize the KL-divergence.\n\noptimizer = tf.optimizers.Adam(learning_rate=0.05)\n\n\nloc = torch.tensor(8.0, requires_grad=True)\nscale = torch.tensor(0.1, requires_grad=True)\n\n\nloc_array = []\nscale_array = []\nloss_array = []\nopt = torch.optim.Adam([loc, scale], lr=0.05)\nfor i in range(401):\n    scale_softplus = torch.functional.F.softplus(scale)\n\n    to_learn = dist.Normal(loc=loc, scale=scale_softplus)\n    loss_value = loss(loc, scale_softplus)\n    loss_array.append(loss_value.item())\n    loc_array.append(to_learn.loc.item())\n    scale_array.append(to_learn.scale.item())\n\n    loss_value.backward()\n    if i % 100 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss_value.item():0.2f}, Loc: {loc.item():0.2f}, Scale: {scale_softplus.item():0.2f}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 99.76, Loc: 8.00, Scale: 0.74\nIteration: 100, Loss: 15.53, Loc: 3.75, Scale: 0.49\nIteration: 200, Loss: 1.57, Loc: 1.67, Scale: 0.51\nIteration: 300, Loss: 0.33, Loc: 0.96, Scale: 0.70\nIteration: 400, Loss: 0.08, Loc: 0.62, Scale: 0.74\n\n\n\nq_s = dist.Normal(loc=loc, scale=scale_softplus)\nq_s\n\nNormal(loc: 0.6216101050376892, scale: 0.739622175693512)\n\n\n\nprob_values_p_s = torch.exp(p_s.log_prob(z_values))\nprob_values_q_s = torch.exp(q_s.log_prob(z_values))\n\nplt.plot(z_values, prob_values_p_s.detach(), label=r\"p\")\nplt.plot(z_values, prob_values_q_s.detach(), label=r\"q\")\n\nsns.despine()\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"PDF\")\n\nText(0, 0.5, 'PDF')\n\n\n\n\n\n\nprob_values_p_s = torch.exp(p_s.log_prob(z_values))\n\nfig = plt.figure(tight_layout=True, figsize=(8, 4))\nax = fig.gca()\nn_iter = 300\n\ndef a(iteration):\n\n    ax.clear()\n    loc = loc_array[iteration]\n    scale = scale_array[iteration]\n    q_s = dist.Normal(loc=loc, scale=scale)\n\n    prob_values_q_s = torch.exp(q_s.log_prob(z_values))\n\n    ax.plot(z_values, prob_values_p_s, label=r\"p\")\n    ax.plot(z_values, prob_values_q_s, label=r\"q\")\n    ax.set_title(f\"Iteration {iteration}, Loss: {loss_array[iteration]:0.2f}\")\n    ax.set_ylim((-0.05, 1.05))\n    ax.legend()\n\nani_mg = animation.FuncAnimation(fig, a, frames=n_iter)\nplt.close()\n\n\nplt.plot(loc_array, label=\"loc\")\nplt.plot(scale_array, label=\"scale\")\nplt.xlabel(\"Iterations\")\nsns.despine()\nplt.legend()\n\n<matplotlib.legend.Legend at 0x140988be0>\n\n\n\n\n\n\nani_mg.save(\"kl_qp_mg.gif\", writer=\"imagemagick\")\n\n\n\nKL-Divergence and ELBO\nLet us consider linear regression. We have parameters \\(\\theta \\in R^D\\) and we define a prior over them. Let us assume we define prior \\(p(\\theta)\\sim \\mathcal{N_D} (\\mu, \\Sigma)\\). Now, given our dataset \\(D = \\{X, y\\}\\) and a parameter vector \\(\\theta\\), we can deifine our likelihood as \\(p(D|\\theta)\\) or $p(y|X, ) = {i=1}^{n} p(y_i|x_i, ) = {i=1}^{n} (y_i|x_i^T, ^2) $\nAs per Bayes rule, we can obtain the posterior over \\(\\theta\\) as:\n\\(p(\\theta|D) = \\dfrac{p(D|\\theta)p(\\theta)}{p(D)}\\)\nNow, in general \\(p(D)\\) is hard to compute.\nSo, in variational inference, our aim is to use a surrogate distribution \\(q(\\theta)\\) such that it is very close to \\(p(\\theta|D)\\). We do so by minimizing the KL divergence between \\(q(\\theta)\\) and \\(p(\\theta|D)\\).\nAim: \\[q^*(\\theta) = \\underset{q(\\theta) \\in \\mathcal{Q}}{\\mathrm{argmin~}} D_{KL}[q(\\theta)||p(\\theta|D)]\\]\nNow, \\[D_{KL}[q(\\theta)||p(\\theta|D)] = \\mathbb{E}_{q(\\theta)}[\\log\\frac{q(\\theta)}{p(\\theta|D)}]\\] Now, \\[ = \\mathbb{E}_{q(\\theta)}[\\log\\frac{q(\\theta)p(D)}{p(\\theta, D)}]\\] Now, \\[ = \\mathbb{E}_{q(\\theta)}[\\log q(\\theta)]- \\mathbb{E}_{q(\\theta)}[\\log p(\\theta, D)] + \\mathbb{E}_{q(\\theta)}[\\log p(D)] \\] \\[= \\mathbb{E}_{q(\\theta)}[\\log q(\\theta)]- \\mathbb{E}_{q(\\theta)}[\\log p(\\theta, D)] + \\log p(D) \\]\nNow, \\(p(D) \\in \\{0, 1\\}\\). Thus, \\(\\log p(D) \\in \\{-\\infty, 0 \\}\\)\nNow, let us look at the quantities:\n\\[\\underbrace{D_{KL}[q(\\theta)||p(\\theta|D)]}_{\\geq 0} = \\underbrace{\\mathbb{E}_{q(\\theta)}[\\log q(\\theta)]- \\mathbb{E}_{q(\\theta)}[\\log p(\\theta, D)]}_{-\\text{ELBO(q)}} +  \\underbrace{\\log p(D)}_{\\leq 0}\\]\nThus, we know that \\(\\log p(D) \\geq \\text{ELBO(q)}\\)\nThus, finally we can rewrite the optimisation from\n\\[q^*(\\theta) = \\underset{q(\\theta) \\in \\mathcal{Q}}{\\mathrm{argmin~}} D_{KL}[q(\\theta)||p(\\theta|D)]\\]\nto\n\\[q^*(\\theta) = \\underset{q(\\theta) \\in \\mathcal{Q}}{\\mathrm{argmax~}} \\text{ELBO(q)}\\]\nNow, given our linear regression problem setup, we want to maximize the ELBO.\nWe can do so by the following. As a simple example, let us assume \\(\\theta \\in R^2\\)\n\nAssume some q. Say, a Normal distribution. So, \\(q\\sim \\mathcal{N}_2\\)\nDraw samples from q. Say N samples.\nInitilize ELBO = 0.0\nFor each sample:\n\nLet us assume drawn sample is \\([\\theta_1, \\theta_2]^T\\)\nCompute log_prob of prior on \\([\\theta_1, \\theta_2]^T\\) or lp = p.log_prob(θ1, θ2)\nCompute log_prob of likelihood on \\([\\theta_1, \\theta_2]^T\\) or ll = l.log_prob(θ1, θ2)\nCompute log_prob of q on \\([\\theta_1, \\theta_2]^T\\) or lq = q.log_prob(θ1, θ2)\nELBO = ELBO + (ll+lp-q)\n\nReturn ELBO/N\n\n\nprior = dist.Normal(loc = 0., scale = 1.)\np = dist.Normal(loc = 5., scale = 1.)\n\n\nsamples = p.sample([1000])\n\n\nmu = torch.tensor(1.0, requires_grad=True)\n\ndef surrogate_sample(mu):\n    std_normal = dist.Normal(loc = 0., scale=1.)\n    sample_std_normal  = std_normal.sample()\n    return mu + sample_std_normal\n\n\nsamples_from_surrogate = surrogate_sample(mu)\n\n\nsamples_from_surrogate\n\ntensor(2.7988, grad_fn=<AddBackward0>)\n\n\n\ndef logprob_prior(mu):\n    return prior.log_prob(mu)\n\nlp = logprob_prior(samples_from_surrogate)\n\ndef log_likelihood(mu, samples):\n    di = dist.Normal(loc=mu, scale=1)\n    return torch.sum(di.log_prob(samples))\n\nll = log_likelihood(samples_from_surrogate, samples)\n\nls = surrogate.log_prob(samples_from_surrogate)\n\n\n\ndef elbo_loss(mu, data_samples):\n    samples_from_surrogate = surrogate_sample(mu)\n    lp = logprob_prior(samples_from_surrogate)\n    ll = log_likelihood(samples_from_surrogate, data_samples)\n    ls = surrogate.log_prob(samples_from_surrogate)\n\n    return -lp - ll + ls\n\n\nmu = torch.tensor(1.0, requires_grad=True)\n\nloc_array = []\nloss_array = []\n\nopt = torch.optim.Adam([mu], lr=0.02)\nfor i in range(2000):\n    loss_val = elbo_loss(mu, samples)\n    loss_val.backward()\n    loc_array.append(mu.item())\n    loss_array.append(loss_val.item())\n\n    if i % 100 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss_val.item():0.2f}, Loc: {mu.item():0.3f}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 11693.85, Loc: 1.000\nIteration: 100, Loss: 2550.90, Loc: 2.744\nIteration: 200, Loss: 2124.30, Loc: 3.871\nIteration: 300, Loss: 2272.48, Loc: 4.582\nIteration: 400, Loss: 2025.17, Loc: 4.829\nIteration: 500, Loss: 1434.45, Loc: 5.079\nIteration: 600, Loss: 1693.33, Loc: 5.007\nIteration: 700, Loss: 1495.89, Loc: 4.957\nIteration: 800, Loss: 2698.28, Loc: 5.149\nIteration: 900, Loss: 2819.85, Loc: 5.117\nIteration: 1000, Loss: 1491.79, Loc: 5.112\nIteration: 1100, Loss: 1767.87, Loc: 4.958\nIteration: 1200, Loss: 1535.30, Loc: 4.988\nIteration: 1300, Loss: 1458.61, Loc: 4.949\nIteration: 1400, Loss: 1400.21, Loc: 4.917\nIteration: 1500, Loss: 2613.42, Loc: 5.073\nIteration: 1600, Loss: 1411.46, Loc: 4.901\nIteration: 1700, Loss: 1587.94, Loc: 5.203\nIteration: 1800, Loss: 1461.40, Loc: 5.011\nIteration: 1900, Loss: 1504.93, Loc: 5.076\n\n\n\nplt.plot(loss_array)\n\n\n\n\n\nfrom numpy.lib.stride_tricks import sliding_window_view\nplt.plot(np.average(sliding_window_view(loss_array, window_shape = 10), axis=1))\n\n\n\n\n\n\nLinear Regression\n\ntrue_theta_0 = 3.\ntrue_theta_1 = 4.\n\nx = torch.linspace(-5, 5, 100)\ny_true = true_theta_0 + true_theta_1*x\ny_noisy = y_true + torch.normal(mean = torch.zeros_like(x), std = torch.ones_like(x))\n\nplt.plot(x, y_true)\nplt.scatter(x, y_noisy, s=20, alpha=0.5)\n\n<matplotlib.collections.PathCollection at 0x1407aaac0>\n\n\n\n\n\n\ny_pred = x_dash@theta_prior.sample()\nplt.plot(x, y_pred, label=\"Fit\")\nplt.scatter(x, y_noisy, s=20, alpha=0.5, label='Data')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x14064e850>\n\n\n\n\n\n\ntheta_prior = dist.MultivariateNormal(loc = torch.tensor([0., 0.]), covariance_matrix=torch.eye(2))\n\n\ndef likelihood(theta, x, y):\n    x_dash = torch.vstack((torch.ones_like(x), x)).t()\n    d = dist.Normal(loc=x_dash@theta, scale=torch.ones_like(x))\n    return torch.sum(d.log_prob(y))\n\n\nlikelihood(theta_prior.sample(), x, y_noisy)\n\ntensor(-3558.0769)\n\n\n\nloc = torch.tensor([-1., 1.], requires_grad=True)\nsurrogate_mvn = dist.MultivariateNormal(loc = loc, covariance_matrix=torch.eye(2))\nsurrogate_mvn\n\nMultivariateNormal(loc: torch.Size([2]), covariance_matrix: torch.Size([2, 2]))\n\n\n\nsurrogate_mvn.sample()\n\ntensor([-1.1585,  2.6212])\n\n\n\ndef surrogate_sample_mvn(loc):\n    std_normal_mvn = dist.MultivariateNormal(loc = torch.zeros_like(loc), covariance_matrix=torch.eye(loc.shape[0]))\n    sample_std_normal  = std_normal_mvn.sample()\n    return loc + sample_std_normal\n\n\ndef elbo_loss(loc, x, y):\n    samples_from_surrogate_mvn = surrogate_sample_mvn(loc)\n    lp = theta_prior.log_prob(samples_from_surrogate_mvn)\n    ll = likelihood(samples_from_surrogate_mvn, x, y_noisy)\n    ls = surrogate_mvn.log_prob(samples_from_surrogate_mvn)\n\n    return -lp - ll + ls\n\n\nloc.shape, x.shape, y_noisy.shape\n\n(torch.Size([2]), torch.Size([100]), torch.Size([100]))\n\n\n\nelbo_loss(loc, x, y_noisy)\n\ntensor(2850.3154, grad_fn=<AddBackward0>)\n\n\n\nloc = torch.tensor([-1., 1.], requires_grad=True)\n\n\nloc_array = []\nloss_array = []\n\nopt = torch.optim.Adam([loc], lr=0.02)\nfor i in range(10000):\n    loss_val = elbo_loss(loc, x, y_noisy)\n    loss_val.backward()\n    loc_array.append(mu.item())\n    loss_array.append(loss_val.item())\n\n    if i % 1000 == 0:\n        print(\n            f\"Iteration: {i}, Loss: {loss_val.item():0.2f}, Loc: {loc}\"\n        )\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 5479.97, Loc: tensor([-1.,  1.], requires_grad=True)\nIteration: 1000, Loss: 566.63, Loc: tensor([2.9970, 4.0573], requires_grad=True)\nIteration: 2000, Loss: 362.19, Loc: tensor([2.9283, 3.9778], requires_grad=True)\nIteration: 3000, Loss: 231.23, Loc: tensor([2.8845, 4.1480], requires_grad=True)\nIteration: 4000, Loss: 277.94, Loc: tensor([2.9284, 3.9904], requires_grad=True)\nIteration: 5000, Loss: 1151.51, Loc: tensor([2.9620, 4.0523], requires_grad=True)\nIteration: 6000, Loss: 582.19, Loc: tensor([2.8003, 4.0540], requires_grad=True)\nIteration: 7000, Loss: 178.48, Loc: tensor([2.8916, 3.9968], requires_grad=True)\nIteration: 8000, Loss: 274.76, Loc: tensor([3.0807, 4.1957], requires_grad=True)\nIteration: 9000, Loss: 578.37, Loc: tensor([2.9830, 4.0174], requires_grad=True)\n\n\n\nlearnt_surrogate = dist.MultivariateNormal(loc = loc, covariance_matrix=torch.eye(2))\n\n\ny_samples_surrogate = x_dash@learnt_surrogate.sample([500]).t()\nplt.plot(x, y_samples_surrogate, alpha = 0.02, color='k');\nplt.scatter(x, y_noisy, s=20, alpha=0.5)\n\n<matplotlib.collections.PathCollection at 0x144709a90>\n\n\n\n\n\n\nx_dash@learnt_surrogate.loc.detach().t()\ntheta_sd = torch.linalg.cholesky(learnt_surrogate.covariance_matrix)\n\n\n#y_samples_surrogate = x_dash@learnt_surrogate.loc.t()\n#plt.plot(x, y_samples_surrogate, alpha = 0.02, color='k');\n#plt.scatter(x, y_noisy, s=20, alpha=0.5)\n\ntensor([-1.6542e+01, -1.6148e+01, -1.5754e+01, -1.5360e+01, -1.4966e+01,\n        -1.4572e+01, -1.4178e+01, -1.3784e+01, -1.3390e+01, -1.2996e+01,\n        -1.2602e+01, -1.2208e+01, -1.1814e+01, -1.1420e+01, -1.1026e+01,\n        -1.0632e+01, -1.0238e+01, -9.8441e+00, -9.4501e+00, -9.0561e+00,\n        -8.6621e+00, -8.2681e+00, -7.8741e+00, -7.4801e+00, -7.0860e+00,\n        -6.6920e+00, -6.2980e+00, -5.9040e+00, -5.5100e+00, -5.1160e+00,\n        -4.7220e+00, -4.3280e+00, -3.9340e+00, -3.5400e+00, -3.1460e+00,\n        -2.7520e+00, -2.3579e+00, -1.9639e+00, -1.5699e+00, -1.1759e+00,\n        -7.8191e-01, -3.8790e-01,  6.1054e-03,  4.0011e-01,  7.9412e-01,\n         1.1881e+00,  1.5821e+00,  1.9761e+00,  2.3702e+00,  2.7642e+00,\n         3.1582e+00,  3.5522e+00,  3.9462e+00,  4.3402e+00,  4.7342e+00,\n         5.1282e+00,  5.5222e+00,  5.9162e+00,  6.3102e+00,  6.7043e+00,\n         7.0983e+00,  7.4923e+00,  7.8863e+00,  8.2803e+00,  8.6743e+00,\n         9.0683e+00,  9.4623e+00,  9.8563e+00,  1.0250e+01,  1.0644e+01,\n         1.1038e+01,  1.1432e+01,  1.1826e+01,  1.2220e+01,  1.2614e+01,\n         1.3008e+01,  1.3402e+01,  1.3796e+01,  1.4190e+01,  1.4584e+01,\n         1.4978e+01,  1.5372e+01,  1.5766e+01,  1.6160e+01,  1.6554e+01,\n         1.6948e+01,  1.7342e+01,  1.7736e+01,  1.8131e+01,  1.8525e+01,\n         1.8919e+01,  1.9313e+01,  1.9707e+01,  2.0101e+01,  2.0495e+01,\n         2.0889e+01,  2.1283e+01,  2.1677e+01,  2.2071e+01,  2.2465e+01])\n\n\nTODO\n\nPyro for linear regression example\nHandle more samples in ELBO\nReuse some methods\nAdd figure on reparameterization\nLinear regression learn covariance also\nLinear regression posterior compare with analytical posterior (refer Murphy book)\nClean up code and reuse code whwrever possible\nImprove figures and make them consistent\nAdd background maths wherever needed\nplot the Directed graphical model (refer Maths ML book and render in Pyro)\nLook at the TFP post on https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression\nShow the effect of data size (less data, solution towards prior, else dominated by likelihood)\nMean Firld (full covariance v/s diagonal) for surrogate\n\nReferences\n\nhttps://www.youtube.com/watch?v=HUsznqt2V5I\nhttps://www.youtube.com/watch?v=x9StQ8RZ0ag&list=PLISXH-iEM4JlFsAp7trKCWyxeO3M70QyJ&index=9\nhttps://colab.research.google.com/github/goodboychan/goodboychan.github.io/blob/main/_notebooks/2021-09-13-02-Minimizing-KL-Divergence.ipynb#scrollTo=gd_ev8ceII8q\nhttps://goodboychan.github.io/python/coursera/tensorflow_probability/icl/2021/09/13/02-Minimizing-KL-Divergence.html"
  },
  {
    "objectID": "posts/2020-01-14-test-markdown-post.html",
    "href": "posts/2020-01-14-test-markdown-post.html",
    "title": "An Example Markdown Post",
    "section": "",
    "text": "Footnotes\n\n\nThis is the footnote.↩︎"
  },
  {
    "objectID": "posts/2022-02-11-matrix.html",
    "href": "posts/2022-02-11-matrix.html",
    "title": "Matrix as transformation and interpreting low rank matrix",
    "section": "",
    "text": "Multiplying a matrix A with a vector x transforms x\n\n\n\n\nTransforming a vector via a low rank matrix in the shown examples leads to a line\n\nWe first study Goal 1. The interpretation of matrix vector product is borrowed from the excellent videos from the 3Blue1Brown channel. I’ll first set up the environment by importing a few relevant libraries.\n\n\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nfrom sympy import Matrix, MatrixSymbol, Eq, MatMul\n\nsns.reset_defaults()\nsns.set_context(context=\"talk\", font_scale=0.75)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nsympy_A = MatrixSymbol(\"A\", 2, 2)\nsympy_x = MatrixSymbol(\"x\", 2, 1)\ny = MatrixSymbol(\"y\", 2, 1)\n\nEq(y, sympy_A*sympy_x, evaluate=False)\n\nEq(y, A*x)\n\n\nGiven a matrix A and a vector x, we are trying to get y=Ax. Let us first see the values for a specific instance in the 2d space.\n\nA = np.array([[2, 1], [1, 4]])\n\nx = np.array([1, 1])\nAx = A @ x\n\nEq(Matrix(Ax), MatMul(Matrix(A), Matrix(x)),evaluate=False)\n\nEq(Matrix([\n[3],\n[5]]), Matrix([\n[2, 1],\n[1, 4]])*Matrix([\n[1],\n[1]]))\n\n\nHere, we have A = \\(\\left[\\begin{matrix}2 & 1\\\\1 & 4\\end{matrix}\\right]\\) and x = \\({\\text{[1 1]}}\\)\nNow some code to create arrows to represent arrows.\n\ndef plot_arrow(ax, x, color, label):\n    x_head, y_head = x[0], x[1]\n    x_tail = 0.0\n    y_tail = 0.0\n    dx = x_head - x_tail\n    dy = y_head - y_tail\n\n    arrow = mpatches.FancyArrowPatch(\n        (x_tail, y_tail), (x_head, y_head), mutation_scale=10, color=color, label=label\n    )\n\n    ax.add_patch(arrow)\n    ax.legend(bbox_to_anchor=(1.6, 1), borderaxespad=0)\n\nNow some code to plot the vector corresponding to Ax\n\ndef plot_transform(A, x):\n    Ax = A @ x\n    fig, ax = plt.subplots()\n    plot_arrow(ax, x, \"k\", f\"Original (x) {x}\")\n    plot_arrow(ax, Ax, \"g\", f\"Transformed (Ax) {Ax}\")\n    plt.xlim((-5, 5))\n    plt.ylim((-5, 5))\n    plt.grid(alpha=0.1)\n    ax.set_aspect(\"equal\")\n    plt.title(f\"A = {A}\")\n    sns.despine(left=True, bottom=True)\n    plt.tight_layout()\n\n\nplot_transform(np.array([[1.0, 1.0], [1.0, -1.0]]), [1.0, 2.0])\nplt.savefig(\"Ax1.png\", dpi=100)\n\n\n\n\nIn the plot above, we can see that the vector [1, 2] is transformed to [3, -1] via the matrix A.\nLet us now write some code to create the rotation matrix and apply it on our input x\n\ndef rot(angle):\n    theta = np.radians(angle)\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.array(((c, -s), (s, c)))\n    return np.round(R, 2)\n\n\nx = np.array([1.0, 2.0])\nplot_transform(rot(90), x)\nplt.savefig(\"Ax2\", dpi=100)\n\n\n\n\nAs we can see above, creating the 90 degree rotation matrix indeed transforms our vector anticlockwise 90 degrees.\nNow let us talk about matrices A that are low rank. I am creating a simple low rank matrix where the second row is some constant times the first row.\n\ndef plot_lr(x, slope):\n    low_rank = np.array([1.0, 2.0])\n    low_rank = np.vstack((low_rank, slope * low_rank))\n    plot_transform(low_rank, x)\n    x_lin = np.linspace(-5, 5, 100)\n    y = x_lin * slope\n    plt.plot(x_lin, y, alpha=0.4, lw=5, label=f\"y = {slope}x\")\n    plt.legend(bbox_to_anchor=(1.2, 1), borderaxespad=0)\n\n\nplot_lr(x, 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-1.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\n\nplot_lr([1.0, -1.0], 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-2.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\n\nplot_lr([0.5, -0.7], 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-3.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\n\nplot_lr([-1.0, 0.0], 1.01)\nplt.tight_layout()\nplt.savefig(\"lr-4.png\", bbox_inches=\"tight\", dpi=100)\n\n\n\n\nTo summarize\n\nIn the above plots we can see that changing our x to any vector in the 2d space leads to us to transformed vector not covering the whole 2d space, but on line in the 2d space. One can easily take this learning to higher dimensional matrices A."
  },
  {
    "objectID": "posts/2022-02-14-gmm.html",
    "href": "posts/2022-02-14-gmm.html",
    "title": "GMM learning",
    "section": "",
    "text": "from wand.image import Image as WImage\nfrom wand.color import Color\nimg = WImage(filename='../pgm/gmm.pdf', resolution=400)\nimg.crop(500, 600, 2680, 1600)\nimg\n\n\n\n\n\nK = 3\n\nmu_1 = -1.\nmu_2 = 4.\nmu_3 = 10.\n\nsigma_1 = 2.\nsigma_2 = 1.\nsigma_3 = 1.5\n\nmu = torch.tensor([mu_1, mu_2, mu_3])\nsigma = torch.tensor([sigma_1, sigma_2, sigma_3])\n\n\ndists = dist.Normal(loc = mu, scale = sigma)\n\n\npi = torch.tensor([0.5, 0.25, 0.25])\nz = dist.Categorical(probs=pi)\n\n\nN = 1000\nsamples = {0: [], 1: [], 2: []}\nall_samples = []\nfor n in range(N):\n    z_n = z.sample()\n    mu_n = mu[z_n]\n    sigma_n = sigma[z_n]\n    x = dist.Normal(loc = mu_n, scale= sigma_n).sample()\n    samples[z_n.item()].append(x.item())\n    all_samples.append(x.item())\n\n\nfor k, v in samples.items():\n    print(k, len(v))\n\n0 502\n1 252\n2 246\n\n\n\nsns.kdeplot(samples[0], bw_adjust=2, color='red', label='Component 1')\nsns.kdeplot(samples[1], bw_adjust=2, color='green', label='Component 2')\nsns.kdeplot(samples[2], bw_adjust=2, color='blue', label='Component 3')\n\nsns.kdeplot(all_samples, bw_adjust=2, color='black', label='Mixture')\nsns.despine()\nplt.legend()\n#sns.kdeplot(samples_n, bw_adjust=2)\n\n<matplotlib.legend.Legend at 0x13c32b3d0>\n\n\n\n\n\n\nmixture = dist.MixtureSameFamily(mixture_distribution=z, component_distribution=dists)\n\n\nsamples_n = mixture.sample([1000])\n\n\nmixture.mixture_distribution\n\nCategorical(probs: torch.Size([3]), logits: torch.Size([3]))\n\n\n\nsns.kdeplot(samples[0], bw_adjust=2, color='red', label='Component 1')\nsns.kdeplot(samples[1], bw_adjust=2, color='green', label='Component 2')\nsns.kdeplot(samples[2], bw_adjust=2, color='blue', label='Component 3')\n\nsns.kdeplot(all_samples, bw_adjust=2, color='black', label='Our Mixture')\nsns.despine()\nsns.kdeplot(samples_n, bw_adjust=2, color='purple', lw = 10, alpha=0.5, label='PyTorch Mixture')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x13c757190>\n\n\n\n\n\n\nto_learn_locs = torch.tensor([0.0, 1.0, 2.0], requires_grad=True)\nto_learn_scales = torch.tensor([1.0, 1.0, 1.0], requires_grad=True)\nto_learn_scales_softplus = torch.functional.F.softplus(to_learn_scales)\nto_learn_dists = dist.Normal(loc=to_learn_locs, scale=to_learn_scales_softplus)\n\nto_learn_mix = torch.tensor([0.3, 0.4, 0.2], requires_grad=True)\n\nto_learn_mixture = dist.Categorical(probs=to_learn_mix)\noverall = dist.MixtureSameFamily(\n    mixture_distribution=to_learn_mixture, component_distribution=to_learn_dists\n)\n\n\ndef nll(loc, scale, mix):\n    to_learn_scales_softplus = torch.functional.F.softplus(scale)\n    to_learn_dists = dist.Normal(loc=loc, scale=to_learn_scales_softplus)\n    mix_softmax = torch.functional.F.softmax(mix)\n    to_learn_mixture = dist.Categorical(probs=mix_softmax)\n    overall = dist.MixtureSameFamily(\n        mixture_distribution=to_learn_mixture, component_distribution=to_learn_dists\n    )\n    return -torch.sum(overall.log_prob(samples))\n\n\nopt = torch.optim.Adam([to_learn_locs, to_learn_scales, to_learn_mix], lr=0.05)\nfor i in range(101):\n    loss =nll(to_learn_locs, to_learn_scales, to_learn_mix)\n    if i % 10 == 0:\n        print(f\"Iteration: {i}, Loss: {loss.item():0.2f}\")\n    loss.backward()\n    # loc_array.append(theta_learn_loc)\n    # loss_array.append(loss_val.item())\n\n    opt.step()\n    opt.zero_grad()\n\nIteration: 0, Loss: 16368.27\nIteration: 10, Loss: 10628.43\nIteration: 20, Loss: 8328.17\nIteration: 30, Loss: 7339.84\nIteration: 40, Loss: 6849.51\nIteration: 50, Loss: 6566.24\nIteration: 60, Loss: 6376.24\nIteration: 70, Loss: 6226.55\nIteration: 80, Loss: 6086.78\nIteration: 90, Loss: 5958.18\nIteration: 100, Loss: 5888.01\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_48017/2345747116.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  mix_softmax = torch.functional.F.softmax(mix)\n\n\n\nto_learn_locs\n\ntensor([-1.6593,  6.8915,  4.1946], requires_grad=True)\n\n\n\nto_learn_scales\n\ntensor([1.4718, 3.8919, 2.9224], requires_grad=True)\n\n\n\ns\n\ntensor([0.3407, 0.3356, 0.1757], requires_grad=True)\n\n\n\ntorch.functional.F.softmax(to_learn_mix)\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_48017/1334526509.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  torch.functional.F.softmax(to_learn_mix)\n\n\ntensor([0.3518, 0.3500, 0.2983], grad_fn=<SoftmaxBackward0>)"
  },
  {
    "objectID": "posts/2020-03-02-linear-scratch.html",
    "href": "posts/2020-03-02-linear-scratch.html",
    "title": "Neural Networks from scratch",
    "section": "",
    "text": "def relu(x):\n    return np.max(0, x)\n\n\nX = np.array([[0, 0],\n             [0, 1],\n             [1, 0],\n             [1, 1]])\ny = np.array([0, 1, 1, 0])\n\n\nplt.scatter(X[:, 0], X[:, 1],c=y)\n\n<matplotlib.collections.PathCollection at 0x120334d50>\n\n\n\n\n\n\nX.shape\n\n(4, 2)\n\n\n\nX\n\narray([[0, 0],\n       [0, 1],\n       [1, 0],\n       [1, 1]])\n\n\n\nlayers = [2, 1]\n\n\nB = [np.zeros(n) for n in layers]\n\n\nW = [None]*len(layers)\n\n\nW[0] = np.zeros((X.shape[1], layers[0]))\n\n\nW\n\n[array([[0., 0.],\n        [0., 0.]]), None]\n\n\n\nfor i in range(1, len(layers)):\n    W[i] = np.zeros((layers[i-1], layers[i]))\n\n\nW[1]\n\narray([[0.],\n       [0.]])\n\n\n\nX.shape, W[0].shape\n\n((4, 2), (2, 2))\n\n\n\nX.shape\n\n(4, 2)"
  },
  {
    "objectID": "posts/2018-06-21-aq-india-map.html",
    "href": "posts/2018-06-21-aq-india-map.html",
    "title": "Mapping location of air quality sensing in India",
    "section": "",
    "text": "Standard Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n\n\n\nDownloading data from OpenAQ for 2018-04-06\n\n!wget --no-check-certificate https://openaq-data.s3.amazonaws.com/2018-04-06.csv -P /Users/nipun/Downloads/\n\n--2020-02-29 17:52:50--  https://openaq-data.s3.amazonaws.com/2018-04-06.csv\nResolving openaq-data.s3.amazonaws.com (openaq-data.s3.amazonaws.com)... 52.216.99.123\nConnecting to openaq-data.s3.amazonaws.com (openaq-data.s3.amazonaws.com)|52.216.99.123|:443... connected.\nWARNING: cannot verify openaq-data.s3.amazonaws.com's certificate, issued by ‘CN=DigiCert Baltimore CA-2 G2,OU=www.digicert.com,O=DigiCert Inc,C=US’:\n  Unable to locally verify the issuer's authority.\nHTTP request sent, awaiting response... 200 OK\nLength: 133839107 (128M) [text/csv]\nSaving to: ‘/Users/nipun/Downloads/2018-04-06.csv.1’\n\n2018-04-06.csv.1     37%[======>             ]  47.37M  3.79MB/s    eta 40s    ^C\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"/Users/nipun/Downloads/2018-04-06.csv\")\ndf = df[(df.country=='IN')&(df.parameter=='pm25')].dropna().groupby(\"location\").mean()\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      value\n      latitude\n      longitude\n    \n    \n      location\n      \n      \n      \n    \n  \n  \n    \n      Adarsh Nagar, Jaipur - RSPCB\n      79.916667\n      26.902909\n      75.836853\n    \n    \n      Anand Kala Kshetram, Rajamahendravaram - APPCB\n      42.750000\n      16.987287\n      81.736318\n    \n    \n      Ardhali Bazar, Varanasi - UPPCB\n      103.666667\n      25.350599\n      82.908307\n    \n    \n      Asanol Court Area, Asanol - WBPCB\n      56.833333\n      23.685297\n      86.945968\n    \n    \n      Ashok Nagar, Udaipur - RSPCB\n      114.750000\n      24.588617\n      73.632140\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      Vasundhara, Ghaziabad, UP - UPPCB\n      223.333333\n      28.660335\n      77.357256\n    \n    \n      Vikas Sadan, Gurgaon, Haryana - HSPCB\n      280.250000\n      28.450124\n      77.026305\n    \n    \n      Vindhyachal STPS, Singrauli - MPPCB\n      144.000000\n      24.108970\n      82.645580\n    \n    \n      Ward-32 Bapupara, Siliguri - WBPCB\n      195.000000\n      26.688305\n      88.412668\n    \n    \n      Zoo Park, Hyderabad - TSPCB\n      82.500000\n      17.349694\n      78.451437\n    \n  \n\n79 rows × 3 columns\n\n\n\n\n\nDownloading World GeoJson file\n\n!wget --no-check-certificate https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json\n\n--2020-02-29 17:53:17--  https://raw.githubusercontent.com/python-visualization/folium/master/examples/data/world-countries.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.\nWARNING: cannot verify raw.githubusercontent.com's certificate, issued by ‘CN=DigiCert SHA2 High Assurance Server CA,OU=www.digicert.com,O=DigiCert Inc,C=US’:\n  Unable to locally verify the issuer's authority.\nHTTP request sent, awaiting response... 200 OK\nLength: 252515 (247K) [text/plain]\nSaving to: ‘world-countries.json’\n\nworld-countries.jso 100%[===================>] 246.60K   376KB/s    in 0.7s    \n\n2020-02-29 17:53:19 (376 KB/s) - ‘world-countries.json’ saved [252515/252515]\n\n\n\n\n\nCreating india.json correspdonding to Indian data\n\nimport json\ne = json.load(open('world-countries.json','r'))\njson.dump(e['features'][73], open('india.json','w'))\n\n\nimport folium\n\nfolium_map = folium.Map(width = '60%',height=800,location=[20, 77],\n                        zoom_start=5,\n                        tiles=\"Stamen Terrain\",min_lat=7, max_lat=35, min_lon=73, max_lon=90)\nfor x in df.iterrows():\n    name = x[0]\n    lat, lon = x[1]['latitude'], x[1]['longitude']\n    folium.CircleMarker([lat, lon], radius=5, color='#000000',fill_color='#D3D3D3' , fill_opacity=1).add_to(folium_map)\n\nfolium.GeoJson('india.json').add_to(folium_map)\n\n<folium.features.GeoJson at 0x11e497bd0>\n\n\n\nfolium_map.save(\"map.html\")\n\n\nThere you go!Till next time."
  },
  {
    "objectID": "posts/2020-03-28-active_learning_with_bayesian_linear_regression.html",
    "href": "posts/2020-03-28-active_learning_with_bayesian_linear_regression.html",
    "title": "Active Learning with Bayesian Linear Regression",
    "section": "",
    "text": "Creating scikit-learn like class with fit predict methods for BLR\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import rc\nimport warnings\nwarnings.filterwarnings('ignore')\nseed = 0 # random seed for train_test_split\n\n\nclass BLR():\n  def __init__(self,S0, M0): # M0 -> prior mean, S0 -> prior covariance matrix\n    self.S0 = S0\n    self.M0 = M0\n\n  def fit(self,x,y, return_self = False):\n    self.x = x\n    self.y = y\n\n    # Maximum likelihood estimation for sigma parameter\n    theta_mle = np.linalg.pinv(x.T@x)@(x.T@y)\n    sigma_2_mle = np.linalg.norm(y - x@theta_mle)**2\n    sigma_mle = np.sqrt(sigma_2_mle)\n\n    # Calculating predicted mean and covariance matrix for theta\n    self.SN = np.linalg.pinv(np.linalg.pinv(self.S0) + (sigma_mle**-2)*x.T@x)\n    self.MN = self.SN@(np.linalg.pinv(self.S0)@self.M0 + (sigma_mle**-2)*(x.T@y).squeeze())\n\n    # Calculating predicted mean and covariance matrix for data\n    self.pred_var = x@self.SN@x.T\n    self.y_hat_map = x@self.MN\n    if return_self:\n      return (self.y_hat_map, self.pred_var)\n    \n  def predict(self, x):\n    self.pred_var = x@self.SN@x.T\n    self.y_hat_map = x@self.MN\n    return (self.y_hat_map, self.pred_var)\n\n  def plot(self, s=1): # s -> size of dots for scatter plot\n    individual_var = self.pred_var.diagonal()\n    plt.figure()\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.plot(self.x[:,1], self.y_hat_map, color='black', label='model')\n    plt.fill_between(self.x[:,1], self.y_hat_map-individual_var, self.y_hat_map+individual_var, alpha=0.4, color='black', label='uncertainty')\n    plt.scatter(self.x[:,1], self.y, label='actual data',s=s)\n    plt.title('MAE is '+str(np.mean(np.abs(self.y - self.y_hat_map))))\n    plt.legend()\n\n\n\nCreating & visualizing dataset\nTo start with, let’s create a random dataset with degree 3 polynomial function with some added noise.\n\\[\\begin{equation}\nY = (5X^3 - 4X^2 + 3X - 2) + \\mathcal{N}(0,1)\n\\end{equation}\\]\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, )\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\nWe’ll try to fit a degree 5 polynomial function to our data.\n\nX = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1))\nN_features = X.shape[1]\n\n\nplt.scatter(X[:,1], Y, s=0.5, label = 'data points')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\nLearning a BLR model on the entire data\nWe’ll take \\(M_0\\) (prior mean) as zero vector initially, assuming that we do not have any prior knowledge about \\(M_0\\). We’re taking \\(S_0\\) (prior covariance) as the identity matrix, assuming that all coefficients are completely independent of each other.\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\n\n\n\nVisualising the fit\n\nmodel.plot(s=0.5)\n\n\n\n\nThis doesn’t look like a good fit, right? Let’s set the prior closer to the real values and visualize the fit again.\n\n\nVisualising the fit after changing the prior\n\nnp.random.seed(seed)\nS0 = np.eye(N_features)\nM0 = np.array([-2, 3, -4, 5, 0, 0]) + np.random.randn(N_features, )\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\nmodel.plot(s=0.5)\n\n\n\n\nHmm, better. Now let’s see how it fits after reducing the noise and setting the prior mean to zero vector again.\n\n\nVisualising the fit after reducing the noise\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodel = BLR(S0, M0)\n\n\nmodel.fit(X, Y)\nmodel.plot(s=0.5)\n\n\n\n\nWhen the noise was high, the model tended to align with the prior. After keeping the prior closer to the original coefficients, the model was improved as expected. From the last plot, we can say that as noise reduces from the data, the impact of the prior reduces, and the model tries to fit the data more precisely. Therefore, we can say that when data is too noisy or insufficient, a wisely chosen prior can produce a precise fit.\n\n\nIntuition to Active Learning (Uncertainty Sampling) with an example\nLet’s take the case where we want to train a machine learning model to classify if a person is infected with COVID-19 or not, but the testing facilities for the same are not available so widely. We may have very few amounts of data for detected positive and detected negative patients. Now, we want our model to be highly confident or least uncertain about its results; otherwise, it may create havoc for wrongly classified patients, but, our bottleneck is labeled data. Thanks to active learning techniques, we can overcome this problem smartly. How?\nWe train our model with existing data and test it on all the suspected patients’ data. Let’s say we have an uncertainty measure or confidence level about each tested data point (distance from the decision boundary in case of SVM, variance in case of Gaussian processes, or Bayesian Linear Regression). We can choose a patient for which our model is least certain, and send him to COVID-19 testing facilities (assuming that we can send only one patient at a time). Now, we can include his data to the train set and test the model on everyone else. By following the same procedure repeatedly, we can increase the size of our train data and confidence of the model without sending everyone randomly for testing.\nThis method is called Uncertainty Sampling in Active Learning. Now let’s formally define Active Learning. From Wikipedia,\nActive learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.\nNow, we’ll go through the active learning procedure step by step.\n\n\nTrain set, test set, and pool. What is what?\nThe train set includes labeled data points. The pool includes potential data points to query for a label, and the test set includes labeled data points to check the performance of our model. Here, we cannot actually do a query to anyone, so we assume that we do not have labels for the pool while training, and after each iteration, we include a data point from the pool set to the train set for which our model has the highest uncertainty.\nSo, the algorithm can be represented as the following,\n\nTrain the model with the train set.\nTest the performance on the test set (This should keep improving).\n\nTest the model with the pool.\nQuery for the most uncertain datapoint from the pool.\nAdd that datapoint into the train set.\nRepeat step 1 to step 5 for \\(K\\) iterations (\\(K\\) ranges from \\(0\\) to the pool size).\n\n\n\nCreating initial train set, test set, and pool\nLet’s take half of the dataset as the test set, and from another half, we will start with some points as the train set and remaining as the pool. Let’s start with 2 data points as the train set.\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nX = PolynomialFeatures(degree=5, include_bias=True).fit_transform(X_init.reshape(-1,1))\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed)\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=2, random_state=seed)\n\nVisualizing train, test and pool.\n\nplt.scatter(test_X[:,1], test_Y, label='test set',color='r', s=2)\nplt.scatter(train_X[:,1], train_Y, label='train set',marker='s',color='k', s=50)\nplt.scatter(pool_X[:,1], pool_Y, label='pool',color='b', s=2)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()\n\n\n\n\nLet’s initialize a few dictionaries to keep track of each iteration.\n\ntrain_X_iter = {} # to store train points at each iteration\ntrain_Y_iter = {} # to store corresponding labels to the train set at each iteration\nmodels = {} # to store the models at each iteration\nestimations = {} # to store the estimations on the test set at each iteration\ntest_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration\n\n\n\nTraining & testing initial learner on train set (Iteration 0)\nNow we will train the model for the initial train set, which is iteration 0.\n\ntrain_X_iter[0] = train_X\ntrain_Y_iter[0] = train_Y\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodels[0] = BLR(S0, M0)\n\n\nmodels[0].fit(train_X_iter[0], train_Y_iter[0])\n\nCreating a plot method to visualize train, test and pool with estimations and uncertainty.\n\ndef plot(ax, model, init_title=''):\n  # Plotting the pool\n  ax.scatter(pool_X[:,1], pool_Y, label='pool',s=1,color='r',alpha=0.4)\n  \n  # Plotting the test data\n  ax.scatter(test_X[:,1], test_Y, label='test data',s=1, color='b', alpha=0.4)\n  \n  # Combining the test & the pool\n  test_pool_X, test_pool_Y = np.append(test_X,pool_X, axis=0), np.append(test_Y,pool_Y)\n  \n  # Sorting test_pool for plotting\n  sorted_inds = np.argsort(test_pool_X[:,1])\n  test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds]\n  \n  # Plotting test_pool with uncertainty\n  model.predict(test_pool_X)\n  individual_var = model.pred_var.diagonal()\n  ax.plot(test_pool_X[:,1], model.y_hat_map, color='black', label='model')\n  ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var\n                  , alpha=0.2, color='black', label='uncertainty')\n  \n  # Plotting the train data\n  ax.scatter(model.x[:,1], model.y,s=40, color='k', marker='s', label='train data')\n  ax.scatter(model.x[-1,1], model.y[-1],s=80, color='r', marker='o', label='last added point')\n  \n  # Plotting MAE on the test set\n  model.predict(test_X)\n  ax.set_title(init_title+' MAE is '+str(np.mean(np.abs(test_Y - model.y_hat_map))))\n  ax.set_xlabel('x')\n  ax.set_ylabel('y')\n  ax.legend()\n\nPlotting the estimations and uncertainty.\n\nfig, ax = plt.subplots()\nplot(ax, models[0])\n\n\n\n\nLet’s check the maximum uncertainty about any point for the model.\n\nmodels[0].pred_var.diagonal().max()\n\n4.8261426545316604e-29\n\n\nOops!! There is almost no uncertainty in the model. Why? let’s try again with more train points.\n\ntrain_pool_X, test_X, train_pool_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state=seed)\ntrain_X, pool_X, train_Y, pool_Y = train_test_split(train_pool_X, train_pool_Y, train_size=7, random_state=seed)\n\n\ntrain_X_iter[0] = train_X\ntrain_Y_iter[0] = train_Y\n\n\nS0 = np.eye(N_features)\nM0 = np.zeros((N_features, ))\nmodels[0] = BLR(S0, M0)\n\n\nmodels[0].fit(train_X_iter[0], train_Y_iter[0])\n\n\nfig, ax = plt.subplots()\nplot(ax, models[0])\n\n\n\n\nNow uncertainty is visible, and currently, it’s high at the left-most points. We are trying to fit a degree 5 polynomial here. So our linear regression coefficients are 6, including the bias. If we choose train points equal to or lesser than 6, our model perfectly fits the train points and has no uncertainty. Choosing train points more than 6 induces uncertainty in the model.\nLet’s evaluate the performance on the test set.\n\nestimations[0], _ = models[0].predict(test_X)\ntest_mae_error[0] = np.mean(np.abs(test_Y - estimations[0]))\n\nMean Absolute Error (MAE) on the test set is\n\ntest_mae_error[0]\n\n0.5783654195019617\n\n\n\n\nMoving the most uncertain point from the pool to the train set\nIn the previous plot, we saw that the model was least certain about the left-most point. We’ll move that point from the pool to the train set and see the effect.\n\nesimations_pool, _ = models[0].predict(pool_X)\n\nFinding out a point having the most uncertainty.\n\nin_var = models[0].pred_var.diagonal().argmax()\nto_add_x = pool_X[in_var,:]\nto_add_y = pool_Y[in_var]\n\nAdding the point from the pool to the train set.\n\ntrain_X_iter[1] = np.vstack([train_X_iter[0], to_add_x])\ntrain_Y_iter[1] = np.append(train_Y_iter[0], to_add_y)\n\nDeleting the point from the pool.\n\npool_X = np.delete(pool_X, in_var, axis=0)\npool_Y = np.delete(pool_Y, in_var)\n\n\n\nTraining again and visualising the results (Iteration 1)\nThis time, we will pass previously learnt prior to the next iteration.\n\nS0 = np.eye(N_features)\nmodels[1] = BLR(S0, models[0].MN)\n\n\nmodels[1].fit(train_X_iter[1], train_Y_iter[1])\n\n\nestimations[1], _ = models[1].predict(test_X)\ntest_mae_error[1] = np.mean(np.abs(test_Y - estimations[1]))\n\nMAE on the test set is\n\ntest_mae_error[1]\n\n0.5779411133071186\n\n\nVisualizing the results.\n\nfig, ax = plt.subplots()\nplot(ax, models[1])\n\n\n\n\nBefore & after adding most uncertain point\n\nfig, ax = plt.subplots(1,2, figsize=(13.5,4.5))\nplot(ax[0], models[0],'Before')\nplot(ax[1], models[1],'After')\n\n\n\n\nWe can see that including most uncertain point into the train set has produced a better fit and MAE for test set has been reduced. Also, uncertainty has reduced at the left part of the data but it has increased a bit on the right part of the data.\nNow let’s do this for few more iterations in a loop and visualise the results.\n\n\nActive learning procedure\n\nnum_iterations = 20\npoints_added_x= np.zeros((num_iterations+1, N_features))\n\npoints_added_y=[]\n\nprint(\"Iteration, Cost\\n\")\nprint(\"-\"*40)\n\nfor iteration in range(2, num_iterations+1):\n    # Making predictions on the pool set based on model learnt in the respective train set \n    estimations_pool, var = models[iteration-1].predict(pool_X)\n    \n    # Finding the point from the pool with highest uncertainty\n    in_var = var.diagonal().argmax()\n    to_add_x = pool_X[in_var,:]\n    to_add_y = pool_Y[in_var]\n    points_added_x[iteration-1,:] = to_add_x\n    points_added_y.append(to_add_y)\n    \n    # Adding the point to the train set from the pool\n    train_X_iter[iteration] = np.vstack([train_X_iter[iteration-1], to_add_x])\n    train_Y_iter[iteration] = np.append(train_Y_iter[iteration-1], to_add_y)\n    \n    # Deleting the point from the pool\n    pool_X = np.delete(pool_X, in_var, axis=0)\n    pool_Y = np.delete(pool_Y, in_var)\n    \n    # Training on the new set\n    models[iteration] = BLR(S0, models[iteration-1].MN)\n    models[iteration].fit(train_X_iter[iteration], train_Y_iter[iteration])\n    \n    estimations[iteration], _ = models[iteration].predict(test_X)\n    test_mae_error[iteration]= pd.Series(estimations[iteration] - test_Y.squeeze()).abs().mean()\n    print(iteration, (test_mae_error[iteration]))\n\nIteration, Cost\n\n----------------------------------------\n2 0.49023173501654815\n3 0.4923391714942153\n4 0.49040074812746753\n5 0.49610198614600165\n6 0.5015282102751122\n7 0.5051264429971314\n8 0.5099913097301352\n9 0.504455016053513\n10 0.5029219102020734\n11 0.5009762782262487\n12 0.5004883097883343\n13 0.5005169638980388\n14 0.5002731089932334\n15 0.49927485683909884\n16 0.49698416490822594\n17 0.49355398855432897\n18 0.49191185613804617\n19 0.491164833699368\n20 0.4908067530719673\n\n\n\npd.Series(test_mae_error).plot(style='ko-')\nplt.xlim((-0.5, num_iterations+0.5))\nplt.ylabel(\"MAE on test set\")\nplt.xlabel(\"# Points Queried\")\nplt.show()\n\n\n\n\nThe plot above shows that MAE on the test set fluctuates a bit initially then reduces gradually as we keep including more points from the pool to the train set. Let’s visualise fits for all the iterations. We’ll discuss this behaviour after that.\n\n\nVisualizing active learning procedure\n\nprint('Initial model')\nprint('Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}'.format(*models[0].MN[::-1]))\nprint('\\nFinal model')\nprint('Y = {0:0.2f} X^5 + {1:0.2f} X^4 + {2:0.2f} X^3 + {3:0.2f} X^2 + {4:0.2f} X + {5:0.2f}'.format(*models[num_iterations].MN[::-1]))\n\nInitial model\nY = 1.89 X^5 + 1.54 X^4 + 0.84 X^3 + -6.48 X^2 + 4.74 X + -1.63\n\nFinal model\nY = 2.50 X^5 + 3.11 X^4 + 0.83 X^3 + -7.08 X^2 + 4.47 X + -1.58\n\n\n\ndef update(iteration):\n    ax.cla()\n    plot(ax, models[iteration])\n    fig.tight_layout()\n\n\nfig, ax = plt.subplots()\nanim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations+1, 1), interval=250)\nplt.close()\nrc('animation', html='jshtml')\n\n\nanim\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can see that the point having highest uncertainty was chosen in first iteration and it produced the near optimal fit. After that, error reduced gradually.\nNow, let’s put everything together and create a class for active learning procedure\n\n\nCreating a class for active learning procedure\n\nclass ActiveL():\n  def __init__(self, X, y, S0=None, M0=None, test_size=0.5, degree = 5, iterations = 20, seed=1):\n    self.X_init = X\n    self.y = y\n    self.S0 = S0\n    self.M0 = M0\n    self.train_X_iter = {} # to store train points at each iteration\n    self.train_Y_iter = {} # to store corresponding labels to the train set at each iteration\n    self.models = {} # to store the models at each iteration\n    self.estimations = {} # to store the estimations on the test set at each iteration\n    self.test_mae_error = {} # to store MAE(Mean Absolute Error) at each iteration\n    self.test_size = test_size\n    self.degree = degree\n    self.iterations = iterations\n    self.seed = seed\n    self.train_size = degree + 2\n\n  def data_preperation(self):\n    # Adding polynomial features\n    self.X = PolynomialFeatures(degree=self.degree).fit_transform(self.X_init)\n    N_features = self.X.shape[1]\n    \n    # Splitting into train, test and pool\n    train_pool_X, self.test_X, train_pool_Y, self.test_Y = train_test_split(self.X, self.y, \n                                                                            test_size=self.test_size,\n                                                                            random_state=self.seed)\n    self.train_X, self.pool_X, self.train_Y, self.pool_Y = train_test_split(train_pool_X, train_pool_Y, \n                                                                            train_size=self.train_size, \n                                                                            random_state=self.seed)\n    \n    # Setting BLR prior incase of not given\n    if self.M0 == None:\n      self.M0 = np.zeros((N_features, ))\n    if self.S0 == None:\n      self.S0 = np.eye(N_features)\n    \n  def main(self):\n    # Training for iteration 0\n    self.train_X_iter[0] = self.train_X\n    self.train_Y_iter[0] = self.train_Y\n    self.models[0] = BLR(self.S0, self.M0)\n    self.models[0].fit(self.train_X, self.train_Y)\n\n    # Running loop for all iterations\n    for iteration in range(1, self.iterations+1):\n      # Making predictions on the pool set based on model learnt in the respective train set \n      estimations_pool, var = self.models[iteration-1].predict(self.pool_X)\n      \n      # Finding the point from the pool with highest uncertainty\n      in_var = var.diagonal().argmax()\n      to_add_x = self.pool_X[in_var,:]\n      to_add_y = self.pool_Y[in_var]\n      \n      # Adding the point to the train set from the pool\n      self.train_X_iter[iteration] = np.vstack([self.train_X_iter[iteration-1], to_add_x])\n      self.train_Y_iter[iteration] = np.append(self.train_Y_iter[iteration-1], to_add_y)\n      \n      # Deleting the point from the pool\n      self.pool_X = np.delete(self.pool_X, in_var, axis=0)\n      self.pool_Y = np.delete(self.pool_Y, in_var)\n      \n      # Training on the new set\n      self.models[iteration] = BLR(self.S0, self.models[iteration-1].MN)\n      self.models[iteration].fit(self.train_X_iter[iteration], self.train_Y_iter[iteration])\n      \n      self.estimations[iteration], _ = self.models[iteration].predict(self.test_X)\n      self.test_mae_error[iteration]= pd.Series(self.estimations[iteration] - self.test_Y.squeeze()).abs().mean()\n\n  def _plot_iter_MAE(self, ax, iteration):\n    ax.plot(list(self.test_mae_error.values())[:iteration+1], 'ko-')\n    ax.set_title('MAE on test set over iterations')\n    ax.set_xlim((-0.5, self.iterations+0.5))\n    ax.set_ylabel(\"MAE on test set\")\n    ax.set_xlabel(\"# Points Queried\")\n  \n  def _plot(self, ax, model):\n    # Plotting the pool\n    ax.scatter(self.pool_X[:,1], self.pool_Y, label='pool',s=1,color='r',alpha=0.4)\n    \n    # Plotting the test data\n    ax.scatter(self.test_X[:,1], self.test_Y, label='test data',s=1, color='b', alpha=0.4)\n    \n    # Combining test_pool\n    test_pool_X, test_pool_Y = np.append(self.test_X, self.pool_X, axis=0), np.append(self.test_Y, self.pool_Y)\n    \n    # Sorting test_pool\n    sorted_inds = np.argsort(test_pool_X[:,1])\n    test_pool_X, test_pool_Y = test_pool_X[sorted_inds], test_pool_Y[sorted_inds]\n    \n    # Plotting test_pool with uncertainty\n    preds, var = model.predict(test_pool_X)\n    individual_var = var.diagonal()\n    ax.plot(test_pool_X[:,1], model.y_hat_map, color='black', label='model')\n    ax.fill_between(test_pool_X[:,1], model.y_hat_map-individual_var, model.y_hat_map+individual_var\n                    , alpha=0.2, color='black', label='uncertainty')\n    \n    # plotting the train data\n    ax.scatter(model.x[:,1], model.y,s=10, color='k', marker='s', label='train data')\n    ax.scatter(model.x[-1,1], model.y[-1],s=80, color='r', marker='o', label='last added point')\n    \n    # plotting MAE\n    preds, var = model.predict(self.test_X)\n    ax.set_title('MAE is '+str(np.mean(np.abs(self.test_Y - preds))))\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.legend()\n    \n  def visualise_AL(self):\n    fig, ax = plt.subplots(1,2,figsize=(13,5))\n    def update(iteration):\n      ax[0].cla()\n      ax[1].cla()\n      self._plot(ax[0], self.models[iteration])\n      self._plot_iter_MAE(ax[1], iteration)\n      fig.tight_layout()\n\n    print('Initial model')\n    print('Y = '+' + '.join(['{0:0.2f}'.format(self.models[0].MN[i])+' X^'*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)]))\n    print('\\nFinal model')\n    print('Y = '+' + '.join(['{0:0.2f}'.format(self.models[self.iterations].MN[i])+' X^'*min(i,1)+str(i)*min(i,1) for i in range(self.degree+1)]))\n\n    anim = FuncAnimation(fig, update, frames=np.arange(0, self.iterations+1, 1), interval=250)\n    plt.close()\n\n    rc('animation', html='jshtml')\n    return anim\n\n\n\nVisualizing a different polynomial fit on the same dataset\nLet’s try to fit a degree 7 polynomial to the same data now.\n\nnp.random.seed(seed)\nX_init = np.linspace(-1, 1, 1000)\nnoise = np.random.randn(1000, ) * 0.5\nY = (5 * X_init**3 - 4 * X_init**2 + 3 * X_init - 2) + noise\n\n\nmodel = ActiveL(X_init.reshape(-1,1), Y, degree=7, iterations=20, seed=seed)\n\n\nmodel.data_preperation()\nmodel.main()\nmodel.visualise_AL()\n\nInitial model\nY = -1.92 + 3.79 X^1 + -1.81 X^2 + -0.43 X^3 + -0.51 X^4 + -0.27 X^5 + -0.18 X^6 + -0.11 X^7\n\nFinal model\nY = -1.79 + 4.86 X^1 + -5.38 X^2 + 0.50 X^3 + -0.17 X^4 + 1.19 X^5 + 1.83 X^6 + 1.31 X^7\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nWe can clearly see that model was fitting the train points well and uncertainty was high at the left-most position. After first iteration, the left-most point was added to the train set and MAE reduced significantly. Similar phenomeneon happened at iteration 2 with the right-most point. After that error kept reducing at slower rate gradually because fit was near optimal after just 2 iterations.\n\n\nActive learning for diabetes dataset from the Scikit-learn module\nLet’s run our model for diabetes data from sklearn module. The data have various features like age, sex, weight etc. of diabetic people and target is increment in disease after one year. We’ll choose only ‘weight’ feature, which seems to have more correlation with the target.\nWe’ll try to fit degree 1 polynomial to this data, as our data seems to have a linear fit. First, let’s check the performance of Scikit-learn linear regression model.\n\nX, Y = datasets.load_diabetes(return_X_y=True)\nX = X[:, 2].reshape(-1,1) # Choosing only feature 2 which seems more relevent to linear regression\n\n# Normalizing\nX = (X - X.min())/(X.max() - X.min())\nY = (Y - Y.min())/(Y.max() - Y.min())\n\nVisualizing the dataset.\n\nplt.scatter(X, Y)\nplt.xlabel('Weight of the patients')\nplt.ylabel('Increase in the disease after a year')\nplt.show()\n\n\n\n\nLet’s fit the Scikit-learn linear regression model with 50% train-test split.\n\nfrom sklearn.linear_model import LinearRegression\ntrain_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size = 0.5, random_state = seed)\n\n\nclf = LinearRegression()\n\n\nclf.fit(train_X, train_Y)\npred_Y = clf.predict(test_X)\n\nVisualizing the fit & MAE.\n\nplt.scatter(X, Y, label='data', s=5)\nplt.plot(test_X, pred_Y, label='model', color='r')\nplt.xlabel('Weight of the patients')\nplt.ylabel('Increase in the disease after a year')\nplt.title('MAE is '+str(np.mean(np.abs(pred_Y - test_Y))))\nplt.legend()\nplt.show()\n\n\n\n\nNow we’ll fit the same data with our BLR model\n\nmodel = ActiveL(X.reshape(-1,1), Y, degree=1, iterations=20, seed=seed)\n\n\nmodel.data_preperation()\nmodel.main()\nmodel.visualise_AL()\n\nInitial model\nY = 0.41 + 0.16 X^1\n\nFinal model\nY = 0.13 + 0.86 X^1\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\nInitially, the fit is leaning towards zero slope, which is the influence of bias due to a low number of training points. It’s interesting to see that our initial train points tend to make a vertical fit, but the model doesn’t get carried away by that and stabilizes the self with prior.\n\nprint('MAE for Scikit-learn Linear Regression is',np.mean(np.abs(pred_Y - test_Y)))\nprint('MAE for Bayesian Linear Regression is', model.test_mae_error[20])\n\nMAE for Scikit-learn Linear Regression is 0.15424985705353944\nMAE for Bayesian Linear Regression is 0.15738001811804758\n\n\nAt the end, results of sklearn linear regression and our active learning based BLR model are comparable even though we’ve used only 20 points to train our model over 221 points used by sklearn. This is because active learning enables us to choose those datapoints for training, which are going to contribute the most towards a precise fit."
  },
  {
    "objectID": "posts/2021-09-03-param-learning-sgd.html",
    "href": "posts/2021-09-03-param-learning-sgd.html",
    "title": "Learning Gaussian Process regression parameters using mini-batch stochastic gradient descent",
    "section": "",
    "text": "In our previous post we had mentioned (for the noiseless case):\nGiven train data \\[\nD=\\left(x_{i}, y_{i}\\right), i=1: N\n\\] Given a test set \\(X_{*}\\) of size \\(N_{*} \\times d\\) containing \\(N_{*}\\) points in \\(\\mathbb{R}^{d},\\) we want to predict function outputs \\(y_{*}\\) We can write: \\[\n\\left(\\begin{array}{l}\ny \\\\\ny_{*}\n\\end{array}\\right) \\sim \\mathcal{N}\\left(\\left(\\begin{array}{l}\n\\mu \\\\\n\\mu_{*}\n\\end{array}\\right),\\left(\\begin{array}{cc}\nK & K_{*} \\\\\nK_{*}^{T} & K_{* *}\n\\end{array}\\right)\\right)\n\\] where \\[\n\\begin{aligned}\nK &=\\operatorname{Ker}(X, X) \\in \\mathbb{R}^{N \\times N} \\\\\nK_{*} &=\\operatorname{Ker}\\left(X, X_{*}\\right) \\in \\mathbb{R}^{N \\times N} \\\\\nK_{* *} &=\\operatorname{Ker}\\left(X_{*}, X_{*}\\right) \\in \\mathbb{R}^{N_{*} \\times N_{*}}\n\\end{aligned}\n\\]\nThus, from the property of conditioning of multivariate Gaussian, we know that:\n\\[y \\sim \\mathcal{N}_N(\\mu, K)\\]\nWe will assume \\(\\mu\\) to be zero. Thus, we have for the train data, the following expression:\n\\[y \\sim \\mathcal{N}_N(0, K)\\]\nFor the noisy case, we have:\n\\[y \\sim \\mathcal{N}_N(0, K + \\sigma_{noise}^2\\mathcal{I}_N)\\]\nFrom this expression, we can write the log-likelihood of data computed over the kernel parameters \\(\\theta\\) as:\n\\[\\mathcal{LL}(\\theta) = \\log(\\frac{\\exp((-1/2)(y-0)^T (K+\\sigma_{noise}^2\\mathcal{I}_N)^{-1}(y-0))}{(2\\pi)^{N/2}|(K+\\sigma_{noise}^2\\mathcal{I}_N)|^{1/2}})\\]\nThus, we can write:\n\\[\\mathcal{LL}(\\theta) =\\log P(\\mathbf{y} | X, \\theta)=-\\frac{1}{2} \\mathbf{y}^{\\top} M^{-1} \\mathbf{y}-\\frac{1}{2} \\log |M|-\\frac{N}{2} \\log 2 \\pi\\]\nwhere \\[M = K + \\sigma_{noise}^2\\mathcal{I}_N\\]"
  },
  {
    "objectID": "posts/2021-09-03-param-learning-sgd.html#choosing-n-neighbors-for-sgd-batch",
    "href": "posts/2021-09-03-param-learning-sgd.html#choosing-n-neighbors-for-sgd-batch",
    "title": "Learning Gaussian Process regression parameters using mini-batch stochastic gradient descent",
    "section": "Choosing N-Neighbors for SGD batch",
    "text": "Choosing N-Neighbors for SGD batch\n\nsigma = 2.\nl = 2.\nnoise = 0.5\nlr = 1e-2\nnum_iter = 1500\nnp.random.seed(0)\nnll_arr = np.zeros(num_iter)\nfor iteration in range(num_iter):\n    # Sample 1 point\n    idx = np.random.randint(X.shape[0], size=1)\n    x_val = X[idx]\n\n    K = batch_size \n    a = np.abs(X - x_val).flatten()\n    idx = np.argpartition(a,K)[:K] \n    X_subset = X[idx, :]\n    Y_subset = Y[idx, :]\n     \n    nll_arr[iteration] = nll(X_subset, Y_subset, sigma, l, noise)\n    del_sigma, del_l, del_noise = grad_objective(X_subset, Y_subset, sigma, l, noise)\n    sigma = sigma - lr*del_sigma\n    l = l - lr*del_l\n    noise = noise - lr*del_noise\n    \n    lr = lr/(iteration+1)\n\n\nplt.plot(nll_arr)\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"NLL\")\n\nText(0, 0.5, 'NLL')\n\n\n\n\n\n\nApplying gradient descent and visualising the learnt function\n\nsigma = 2.\nl = 2.\nnoise = 0.5\nlr = 1e-2\nnum_iter = 100\nbatch_size = 5\nnll_arr = np.zeros(num_iter)\nfig, ax = plt.subplots()\nnp.random.seed(0)\nfor iteration in range(num_iter):\n    # Sample 1 point\n    idx = np.random.randint(X.shape[0], size=1)\n    x_val = X[idx]\n\n    K = batch_size \n    a = np.abs(X - x_val).flatten()\n    idx = np.argpartition(a,K)[:K] \n    X_subset = X[idx, :]\n    Y_subset = Y[idx, :]\n     \n    nll_arr[iteration] = nll(X_subset, Y_subset, sigma, l, noise)\n    del_sigma, del_l, del_noise = grad_objective(X_subset, Y_subset, sigma, l, noise)\n    sigma = sigma - lr*del_sigma\n    l = l - lr*del_l\n    noise = noise - lr*del_noise\n    k.lengthscale = l\n    k.variance = sigma**2\n    m = GPy.models.GPRegression(X, Y, k, normalizer=False)\n    m.Gaussian_noise = noise**2\n    m.plot(ax=ax, alpha=0.2)['dataplot'];\n    ax.scatter(X_subset, Y_subset, color='green', marker='*', s = 50)\n    plt.ylim((0, 6))\n    #plt.title(f\"Iteration: {iteration:04}, Objective :{nll_arr[iteration]:04.2f}\")\n    plt.savefig(f\"/Users/nipun/Desktop/gp_learning/{iteration:04}.png\")\n    ax.clear()\n    lr = lr/(iteration+1)\nplt.clf()\n\n<Figure size 432x288 with 0 Axes>\n\n\n\nprint(sigma**2, l, noise)\n\n4.239252534833201 2.0031950532157596 0.30136335707188894\n\n\n\n!convert -delay 40 -loop 0 /Users/nipun/Desktop/gp_learning/*.png gp-learning-new.gif"
  },
  {
    "objectID": "posts/2022-02-04-sampling-normal.html",
    "href": "posts/2022-02-04-sampling-normal.html",
    "title": "Sampling from univariate and multivariate normal distributions using Box-Muller transform",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nimport tensorflow_probability as tfp\nimport pandas as pd\ntfd = tfp.distributions\ntfl = tfp.layers\ntfb = tfp.bijectors\n\nsns.reset_defaults()\nsns.set_context(context='talk',font_scale=1)\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nSampling from a univariate normal\nThe goal here is to sample from \\(\\mathcal{N}(\\mu, \\sigma^2)\\). The key idea is to use samples from a uniform distribution to first get samples for a standard normal \\(\\mathcal{N}(0, 1)\\) and then apply an affine transformation to get samples for \\(\\mathcal{N}(\\mu, \\sigma^2)\\).\n\nSampling from uniform distribution\n\nU = tf.random.uniform((1000, 2))\nU1, U2 = U[:, 0], U[:, 1]\n\n2022-02-04 12:00:15.559198: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\nApplying the Box-Muller transform\n\nX1 = tf.sqrt(-2*tf.math.log(U1))*tf.cos(2*np.pi*U2)\nX2 = tf.sqrt(-2*tf.math.log(U1))*tf.sin(2*np.pi*U2)\n\n\nX = tf.concat((X1, X2), axis=0)\nX.shape\n\nTensorShape([2000])\n\n\n\n\nPlotting the obtained standard normal\n\nsns.kdeplot(X, bw_adjust=2)\nsns.despine()\n\n\n\n\n\nplt.hist(X.numpy(), bins=20, density=True)\nsns.despine()\n\n\n\n\n\n\nSampling from \\(\\mathcal{N}(\\mu, \\sigma^2)\\)\nWe apply the affine transformation.\n\\(X \\sim \\mathcal{N}(0, 1)\\)\n\\(Y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) is equivalent to \\(Y \\sim \\mu + \\sigma X\\)\n\nmu = 2.\nsigma = 2.\nY = mu + sigma*X\n\n\nax = sns.kdeplot(X, label=r'$\\mathcal{N}(0, 1)$', bw_adjust=2)\nsns.kdeplot(Y, label=r'$\\mathcal{N}(2, 4)$', bw_adjust=2)\nsns.despine()\nplt.legend()\n\n<matplotlib.legend.Legend at 0x1b2c0a340>\n\n\n\n\n\n\n\n\nSampling from multivariate normal\nLike before, we first sample from standard multivariate normal and then apply an affine transformation to get for our desired multivariate normal.\nThe important thing to note in the generation of the standard multivariate normal samples is that the individial random variables are independent of each other given the identity covariance matrix. Thus, we can independently generate the samples for individual random variable.\n\nU_2D_Samples = tf.random.uniform((2, 1000, 2))\n\n\nU11, U12, U21, U22 = U_2D_Samples[0, :, 0], U_2D_Samples[0, :, 1],U_2D_Samples[1, :, 0],U_2D_Samples[1, :, 1]\n\n\ndef generate(U1, U2):\n    X1 = tf.sqrt(-2*tf.math.log(U1))*tf.cos(2*np.pi*U2)\n    X2 = tf.sqrt(-2*tf.math.log(U1))*tf.sin(2*np.pi*U2)\n    X = tf.concat((X1, X2), axis=0)\n    return X\n\n\nX_1 = tf.reshape(generate(U11, U12), (-1, 1))\nX_2 = tf.reshape(generate(U21, U22), (-1, 1))\n\n\nX = tf.concat((X_1, X_2), axis=1)\n\n\nX\n\n<tf.Tensor: shape=(2000, 2), dtype=float32, numpy=\narray([[-1.2652589 , -1.4106055 ],\n       [ 0.09925841, -0.12048604],\n       [ 0.73987466,  1.8815755 ],\n       ...,\n       [-0.05203251,  1.1814289 ],\n       [-0.04060707, -0.14595209],\n       [-0.7659936 ,  1.505757  ]], dtype=float32)>\n\n\n\nPlotting samples from generated standard 2d normal\n\nsns.kdeplot(x=X[:, 0],\n            y = X[:, 1],zorder=0, n_levels=10, shade=True, \n    cbar=True, thresh=0.001, cmap='viridis',bw_adjust=5,  cbar_kws={'format': '%.3f', })\n\nplt.gca().set_aspect('equal')\nsns.despine()\n\n\n\n\n\n\nApplying the affine transformation\nThe main difference in the 1d and multivariate case is that instead of using \\(\\sigma\\), we use the \\(L\\) cholesky matrix.\n\nmean_2d = tf.constant([3., -1.])\ncov = tf.constant([[2., 0.5],\n                  [0.5, 1.]])\n\n\nL = tf.linalg.cholesky(cov)\n\n\nY_2d = mean_2d + X@tf.transpose(L)\n\n\nY_2d.shape\n\nTensorShape([2000, 2])\n\n\n\nfig, ax = plt.subplots(ncols=2, sharey=True, figsize=(8, 4))\nsns.kdeplot(x=Y_2d[:, 0],\n            y = Y_2d[:, 1],zorder=0, n_levels=10, shade=True, \n    cbar=True, thresh=0.001, cmap='viridis',bw_adjust=5, ax=ax[0], cbar_kws={'format': '%.3f', })\n\nsns.kdeplot(x=X[:, 0],\n            y = X[:, 1],zorder=0, n_levels=10, shade=True, \n    cbar=True, thresh=0.001, cmap='viridis',bw_adjust=5, ax=ax[1], cbar_kws={'format': '%.3f', })\n\nax[0].set_aspect('equal')\nax[1].set_aspect('equal')\n\nsample_mean_tr = tf.reduce_mean(Y_2d, axis=0).numpy()\nsample_mean_tr_rounded  = np.around(sample_mean_tr, 2)\n\ncov_tr = tfp.stats.covariance(Y_2d).numpy()\ncov_tr_rounded =np.around(cov_tr, 2)\n\n\nsample_mean = tf.reduce_mean(X, axis=0).numpy()\nsample_mean_rounded  = np.around(sample_mean, 2)\n\ncov = tfp.stats.covariance(X).numpy()\ncov_rounded =np.around(cov, 2)\n\n\nax[0].set_title(fr\"$\\mu$ = {sample_mean_tr_rounded}\"+\"\\n\"+ fr\"$\\Sigma$ = {cov_tr_rounded}\")\nax[1].set_title(fr\"$\\mu$ = {sample_mean_rounded}\"+\"\\n\"+ fr\"$\\Sigma$ = {cov_rounded}\")\n\n\n\nsns.despine()\nfig.tight_layout()\n\n\n\n\nReferences\n\nhttps://www.youtube.com/watch?v=DSWM7-9gK7s&list=PLISXH-iEM4Jm5B_J9p1oUNGDAUeCFZLkJ&index=38\nhttps://www.youtube.com/watch?v=4fVQrH65aWU\nhttps://en.wikipedia.org/wiki/Box–Muller_transform\nhttps://en.wikipedia.org/wiki/Affine_transformation"
  },
  {
    "objectID": "posts/2017-04-20-parafac-out-tensor.html",
    "href": "posts/2017-04-20-parafac-out-tensor.html",
    "title": "Out of Tensor factorisation",
    "section": "",
    "text": "General Tensor Factorisation\n\nGeneral tensor factorisation for a 3d tensor A (M X N X O) would produce 3 factors- X (M X K), Y (N X K) and Z (O X K). The \\(A_{ijl}\\) entry can be found as (Khatri product) :\n\\[ A_{ijl} = \\sum_k{X_{ik}Y_{jk}Z_{lk}}\\]\n\n\nLearning \\(X_M\\) factor\nHowever, we’d assume that the \\(M^{th}\\) entry isn’t a part of this decomposition. So, how do we obtain the X factors correspondonding to \\(M^{th}\\) entry? We learn the Y and Z factors from the tensor A (excluding the \\(M^{th}\\) row entries). We assume the Y and Z learnt to be shared across the entries across rows of A (1 through M).\n\nThe above figure shows the latent factor for X (\\(X_{M}\\)) corresponding to the \\(M^{th}\\) entry of X that we wish to learn. On the LHS, we see the matrix corresponding to \\(A_{M}\\). The highlighted entry of \\(A_{M}\\) is created by element-wise multiplication of \\(X_M, Y_0, Z_0\\) and then summing. Thus, each of the N X O entries of \\(A_M\\) are created by multiplying \\(X_M\\) with a row from Y and a row from Z. In general,\n\\[A_{M, n, o}  = \\sum_k{X_{M, k} \\times Y_{n, k} \\times Z_{o, k}}\\]\nNow, to learn \\(X_M\\), we plan to use least squares. For that, we need to reduce the problem into \\(\\alpha x = \\beta\\) We do this as follows:\n\nWe flatten out the A_M matrix into a vector containing N X O entries and call it \\(\\beta\\)\nWe create a matrix by element-wise multiplication of each row of Y with each row of Z to create \\(\\alpha\\) of shape (N X O, K)\n\nWe can now write,\n\\[ \\alpha X_M^T \\approx \\beta \\] Thus, X_M^T = Least Squares (\\(\\alpha, \\beta\\))\nOfcourse, \\(\\beta\\) can have missing entries, which we mask out. Thus, we can write:\n\\(X_M^T\\) = Least Squares (\\(\\alpha [Mask], \\beta [Mask]\\))\nIn case we’re doing a non-negative tensor factorisation, we can instead learn \\(X_M^T\\) as follows: \\(X_M^T\\) = Non-negative Least Squares (\\(\\alpha [Mask], \\beta [Mask]\\))\n\n\nCode example\n\nimport tensorly\nfrom tensorly.decomposition import parafac, non_negative_parafac\nimport numpy as np\n\n\nCreating the tensor to be decomposed\n\nM, N, O = 10, 4, 3 #user, movie, feature\nt = np.arange(M*N*O).reshape(M, N, O).astype('float32')\nt[0] #First entry\n\narray([[  0.,   1.,   2.],\n       [  3.,   4.,   5.],\n       [  6.,   7.,   8.],\n       [  9.,  10.,  11.]], dtype=float32)\n\n\n\n\nSetting a few entries of the last user to be unavailable/unknown\n\nt_orig = t.copy() # creating a copy\nt[-1,:,:][0, 0] = np.NAN\nt[-1,:,:][2, 2] = np.NAN\nt[-1,:,:]\n\narray([[  nan,  109.,  110.],\n       [ 111.,  112.,  113.],\n       [ 114.,  115.,   nan],\n       [ 117.,  118.,  119.]], dtype=float32)\n\n\n\n\nStandard Non-negative PARAFAC decomposition\n\nK = 2\n# Notice, we factorise a tensor with one less user. thus, t[:-1, :, :]\nX, Y, Z = non_negative_parafac(t[:-1,:,:], rank=K)\n\n\nX.shape, Y.shape, Z.shape\n\n((9, 2), (4, 2), (3, 2))\n\n\n\nY\n\narray([[ 0.48012616,  1.13542261],\n       [ 0.49409014,  2.98947262],\n       [ 0.5072998 ,  5.03375154],\n       [ 0.52051081,  7.07682331]])\n\n\n\nZ\n\narray([[ 0.57589198,  1.55655956],\n       [ 0.58183329,  1.7695134 ],\n       [ 0.58778163,  1.98182137]])\n\n\n\n\nCreating \\(\\alpha\\) by element-wise multiplication of Y, Z and reshaping\n\nalpha = np.einsum('nk, ok -> nok', Y, Z).reshape((N*O, K))\nprint alpha\n\nprint \"\\nShape of alpha = \", alpha.shape\n\n[[  0.27650081   1.76735291]\n [  0.27935339   2.00914552]\n [  0.28220934   2.25020479]\n [  0.28454255   4.65329218]\n [  0.28747809   5.28991186]\n [  0.29041711   5.92460074]\n [  0.29214989   7.83533407]\n [  0.29516391   8.9072908 ]\n [  0.29818151   9.9759964 ]\n [  0.299758    11.01549695]\n [  0.30285052  12.52253365]\n [  0.30594669  14.02499969]]\n\nShape of alpha =  (12, 2)\n\n\n\nfrom scipy.optimize import nnls\n\n\n\nCreating \\(\\beta\\)\n\nbeta = t[-1,:,:].reshape(N*O, 1)\nmask = ~np.isnan(beta).flatten()\nbeta[mask].reshape(-1, 1)\n\narray([[ 109.],\n       [ 110.],\n       [ 111.],\n       [ 112.],\n       [ 113.],\n       [ 114.],\n       [ 115.],\n       [ 117.],\n       [ 118.],\n       [ 119.]], dtype=float32)\n\n\n\n\nLearning \\(X_M\\)\n\nX_M = nnls(alpha[mask], beta[mask].reshape(-1, ))[0].reshape((1, K))\nX_M\n\narray([[ 389.73825036,    0.        ]])\n\n\n\n\nComparing X_M with other entries from X\n\nX\n\narray([[  7.40340055e-01,   7.62705972e-01],\n       [  4.14288653e+01,   7.57249713e-01],\n       [  8.51282259e+01,   6.56239315e-01],\n       [  1.29063811e+02,   5.46019997e-01],\n       [  1.73739412e+02,   4.06496594e-01],\n       [  2.19798887e+02,   2.11453297e-01],\n       [  2.64609697e+02,   6.54705290e-02],\n       [  3.01392149e+02,   2.39700484e-01],\n       [  3.39963876e+02,   3.41824756e-01]])\n\n\nIt seems that the first column captures the increasing trend of values in the tensor\n\n\nPredicting missing entries using tensor multiplication\n\nnp.round(np.einsum('ir, jr, kr -> ijk', X_M, Y, Z))\n\narray([[[ 108.,  109.,  110.],\n        [ 111.,  112.,  113.],\n        [ 114.,  115.,  116.],\n        [ 117.,  118.,  119.]]])\n\n\n\n\nActual entries\n\nt_orig[-1, :, :]\n\narray([[ 108.,  109.,  110.],\n       [ 111.,  112.,  113.],\n       [ 114.,  115.,  116.],\n       [ 117.,  118.,  119.]], dtype=float32)\n\n\nNot bad! We’re exactly there!"
  },
  {
    "objectID": "posts/2013-04-01-download_weather.html",
    "href": "posts/2013-04-01-download_weather.html",
    "title": "Downloading weather data",
    "section": "",
    "text": "import datetime\nimport pandas as pd\nimport forecastio\nimport getpass\n\n\n# Enter your API here\napi_key = getpass.getpass()\n\n········\n\n\n\nlen(api_key)\n\n32\n\n\nAustin’s Latitude and longitude\n\nlat = 30.25\nlng = -97.25\n\nLet us see the forecast for 1 Jan 2015\n\ndate = datetime.datetime(2015,1,1)\n\n\nforecast = forecastio.load_forecast(api_key, lat, lng, time=date, units=\"us\")\n\n\nforecast\n\n<forecastio.models.Forecast at 0x10319ce50>\n\n\n\nhourly = forecast.hourly()\n\n\nhourly.data\n\n[<forecastio.models.ForecastioDataPoint at 0x1068643d0>,\n <forecastio.models.ForecastioDataPoint at 0x106864bd0>,\n <forecastio.models.ForecastioDataPoint at 0x106864ad0>,\n <forecastio.models.ForecastioDataPoint at 0x106864cd0>,\n <forecastio.models.ForecastioDataPoint at 0x106864fd0>,\n <forecastio.models.ForecastioDataPoint at 0x106864d10>,\n <forecastio.models.ForecastioDataPoint at 0x100734e10>,\n <forecastio.models.ForecastioDataPoint at 0x1061e3450>,\n <forecastio.models.ForecastioDataPoint at 0x1061e3350>,\n <forecastio.models.ForecastioDataPoint at 0x1068b3250>,\n <forecastio.models.ForecastioDataPoint at 0x1068b3110>,\n <forecastio.models.ForecastioDataPoint at 0x1068b3150>,\n <forecastio.models.ForecastioDataPoint at 0x1068b3190>,\n <forecastio.models.ForecastioDataPoint at 0x1068b31d0>,\n <forecastio.models.ForecastioDataPoint at 0x1068b3210>,\n <forecastio.models.ForecastioDataPoint at 0x1068b3fd0>,\n <forecastio.models.ForecastioDataPoint at 0x1068b3dd0>,\n <forecastio.models.ForecastioDataPoint at 0x1068b3e10>,\n <forecastio.models.ForecastioDataPoint at 0x1068b3e50>,\n <forecastio.models.ForecastioDataPoint at 0x1068b3f50>,\n <forecastio.models.ForecastioDataPoint at 0x1068c84d0>,\n <forecastio.models.ForecastioDataPoint at 0x1068c8390>,\n <forecastio.models.ForecastioDataPoint at 0x1068c8510>,\n <forecastio.models.ForecastioDataPoint at 0x1068c8550>]\n\n\nExtracting data for a single hour.\n\nhourly.data[0].d\n\n{u'apparentTemperature': 32.57,\n u'dewPoint': 33.39,\n u'humidity': 0.79,\n u'icon': u'clear-night',\n u'precipIntensity': 0,\n u'precipProbability': 0,\n u'pressure': 1032.61,\n u'summary': u'Clear',\n u'temperature': 39.46,\n u'time': 1420005600,\n u'visibility': 10,\n u'windBearing': 21,\n u'windSpeed': 10.95}\n\n\nLet us say that we want to use the temperature and humidity only.\n\nattributes = [\"temperature\", \"humidity\"]\n\n\ntimes = []\ndata = {}\nfor attr in attributes:\n    data[attr] = []\n\nNow, let us download hourly data for 30 days staring January 1 this year.\n\nstart = datetime.datetime(2015, 1, 1)\nfor offset in range(1, 60):\n    forecast = forecastio.load_forecast(api_key, lat, lng, time=start+datetime.timedelta(offset), units=\"us\")\n    h = forecast.hourly()\n    d = h.data\n    for p in d:\n        times.append(p.time)\n        for attr in attributes:\n            data[attr].append(p.d[attr])\n\nNow, let us create a Pandas data frame for this time series data.\n\ndf = pd.DataFrame(data, index=times)\n\n\ndf.head()\n\n\n\n\n  \n    \n      \n      humidity\n      temperature\n    \n  \n  \n    \n      2015-01-01 11:30:00\n      0.73\n      38.74\n    \n    \n      2015-01-01 12:30:00\n      0.74\n      38.56\n    \n    \n      2015-01-01 13:30:00\n      0.75\n      38.56\n    \n    \n      2015-01-01 14:30:00\n      0.79\n      37.97\n    \n    \n      2015-01-01 15:30:00\n      0.80\n      37.78\n    \n  \n\n\n\n\nNow, we need to fix the timezone.\n\ndf = df.tz_localize(\"Asia/Kolkata\").tz_convert(\"US/Central\")\n\n\ndf.head()\n\n\n\n\n  \n    \n      \n      humidity\n      temperature\n    \n  \n  \n    \n      2015-01-01 00:00:00-06:00\n      0.73\n      38.74\n    \n    \n      2015-01-01 01:00:00-06:00\n      0.74\n      38.56\n    \n    \n      2015-01-01 02:00:00-06:00\n      0.75\n      38.56\n    \n    \n      2015-01-01 03:00:00-06:00\n      0.79\n      37.97\n    \n    \n      2015-01-01 04:00:00-06:00\n      0.80\n      37.78\n    \n  \n\n\n\n\nI’ll now export this file to a CSV to use it for following demonstrations on aggregations on time series.\n\ndf.to_csv(\"weather.csv\")\n\nA quick validation of our downloaded data.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n\ndf.plot(subplots=True);"
  },
  {
    "objectID": "posts/2022-01-28-tfp-linear-regression.html",
    "href": "posts/2022-01-28-tfp-linear-regression.html",
    "title": "Linear Regression in Tensorflow Probability",
    "section": "",
    "text": "np.random.seed(42)\nx = np.linspace(-0.5, 1, 100)\ny = 5*x + 4 + 2*np.multiply(x, np.random.randn(100))\n\n\nplt.scatter(x, y, s=20, alpha=0.6)\nsns.despine()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nText(0, 0.5, 'y')\n\n\n\n\n\n\nModel 1: Vanilla Linear Regression\n\nmodel = Sequential([\n    Dense(input_shape=(1,), units=1, name='D1')])\n\n2022-02-01 09:37:25.292936: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n D1 (Dense)                  (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nmodel.compile(loss='mse', optimizer='adam')\nmodel.fit(x, y, epochs=4000, verbose=0)\n\n<keras.callbacks.History at 0x1a36573a0>\n\n\n\nmodel.get_layer('D1').weights\n\n[<tf.Variable 'D1/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[4.929521]], dtype=float32)>,\n <tf.Variable 'D1/bias:0' shape=(1,) dtype=float32, numpy=array([3.997371], dtype=float32)>]\n\n\n\nplt.scatter(x, y, s=20, alpha=0.6)\nsns.despine()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\npred_m1 = model.predict(x)\nplt.plot(x, pred_m1)\n\n\n\n\n\n\nModel 2\n\nmodel_2 = Sequential([\n    Dense(input_shape=(1,), units=1, name='M2_D1'),\n    tfl.DistributionLambda(lambda loc: tfd.Normal(loc=loc, scale=1.), name='M2_Likelihood')])\n\n2022-02-01 09:37:33.529583: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n\n\n\nmodel_2.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n M2_D1 (Dense)               (None, 1)                 2         \n                                                                 \n M2_Likelihood (Distribution  ((None, 1),              0         \n Lambda)                      (None, 1))                         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nm2_untrained_weight = model_2.get_layer('M2_D1').weights\n\n\nplt.scatter(x, y, s=20, alpha=0.6)\nsns.despine()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nm_2 = model_2(x)\nplt.plot(x, m_2.sample(40).numpy()[:, :, 0].T, color='k', alpha=0.05);\nplt.plot(x, m_2.mean().numpy().flatten(), color='k')\n\n\n\n\n\ndef plot(model):\n    plt.scatter(x, y, s=20, alpha=0.6)\n    sns.despine()\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    m = model(x)\n    m_s = m.stddev().numpy().flatten()\n    m_m = m.mean().numpy().flatten()\n\n    plt.plot(x, m_m , color='k')\n    plt.fill_between(x, m_m-m_s, m_m+m_s, color='k', alpha=0.4)\n\n\nplot(model_2)\n\n\n\n\n\ndef nll(y_true, y_pred):\n    # y_pred is distribution\n    return -y_pred.log_prob(y_true)\n\n\nmodel_2.compile(loss=nll, optimizer='adam')\nmodel_2.fit(x, y, epochs=4000, verbose=0)\n\n<keras.callbacks.History at 0x1a3b96880>\n\n\n\nplot(model_2)\n\n\n\n\n\n\nModel 3\n\nmodel_3 = Sequential([\n    Dense(input_shape=(1,), units=2, name='M3_D1'),\n    tfl.DistributionLambda(lambda t: tfd.Normal(loc=t[..., 0], scale=tf.exp(t[..., 1])), name='M3_Likelihood')])\n\n\nmodel_3.get_layer('M3_D1').weights\n\n[<tf.Variable 'M3_D1/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[0.04476571, 0.55212975]], dtype=float32)>,\n <tf.Variable 'M3_D1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n\n\n\nmodel_3.compile(loss=nll, optimizer='adam')\nmodel_3.fit(x, y, epochs=4000, verbose=0)\n\n<keras.callbacks.History at 0x1a3d20190>\n\n\n\nplot(model_3)\n\n\n\n\nGood reference https://tensorchiefs.github.io/bbs/files/21052019-bbs-Beate-uncertainty.pdf\nAt this point, we see that scale or sigma is a linear function of input x.\n\n#### Model 4\n\n\nmodel_4 = Sequential([\n    Dense(input_shape=(1,), units=2, name='M4_D1', activation='relu'),\n    Dense(units=2, name='M4_D2',  activation='relu'),\n    Dense(units=2, name='M4_D3'),\n    tfl.DistributionLambda(lambda t: tfd.Normal(loc=t[..., 0], scale=tf.math.softplus(t[..., 1])), name='M4_Likelihood')])\n\n\nmodel_4.compile(loss=nll, optimizer='adam')\nmodel_4.fit(x, y, epochs=4000, verbose=0)\n\n<keras.callbacks.History at 0x1a3cd0a60>\n\n\n\nplot(model_4)\n\n\n\n\n\n\nModel 5\nFollow: https://juanitorduz.github.io/tfp_lm/\n\n\nModel 6\nDense Variational\n\ndef prior(kernel_size, bias_size, dtype=None):\n    n = kernel_size + bias_size\n    # Independent Normal Distribution\n    return lambda t: tfd.Independent(tfd.Normal(loc=tf.zeros(n, dtype=dtype),\n                                                scale=1),\n                                     reinterpreted_batch_ndims=1)\n\ndef posterior(kernel_size, bias_size, dtype=None):\n    n = kernel_size + bias_size\n    return Sequential([\n        tfl.VariableLayer(tfl.IndependentNormal.params_size(n), dtype=dtype),\n        tfl.IndependentNormal(n)\n    ])\n\n\nN = len(x)\nmodel_6 = Sequential([\n    # Requires posterior and prior distribution\n    # Add kl_weight for weight regularization\n    #tfl.DenseVariational(16, posterior, prior, kl_weight=1/N, activation='relu', input_shape=(1, )),\n    tfl.DenseVariational(2, posterior, prior, kl_weight=1/N, input_shape=(1,)),\n    tfl.IndependentNormal(1)\n])\n\n\nmodel_6.compile(loss=nll, optimizer='adam')\n\n\nmodel_6.fit(x, y, epochs=5000, verbose=0)\n\n<keras.callbacks.History at 0x1a61a9ee0>\n\n\n\nplot(model_6)\n\n\n\n\n\nN = len(x)\nmodel_7 = Sequential([\n    # Requires posterior and prior distribution\n    # Add kl_weight for weight regularization\n    tfl.DenseVariational(16, posterior, prior, kl_weight=1/N, activation='relu', input_shape=(1, )),\n    tfl.DenseVariational(2, posterior, prior, kl_weight=1/N),\n    tfl.IndependentNormal(1)\n])\n\nmodel_7.compile(loss=nll, optimizer='adam')\n\n\nmodel_7.fit(x, y, epochs=5000, verbose=0)\n\n<keras.callbacks.History at 0x1a669da90>\n\n\n\nplot(model_7)\n\n\n\n\nhttps://livebook.manning.com/book/probabilistic-deep-learning-with-python/chapter-8/123"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "ML\n\n\n\n\nAdagrad optimizer for matrix factorisation\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsetup\n\n\n\n\nMy iPad computing setup\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nacademia\n\n\n\n\nSome personal reflections..\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nSimulating a continuous HMM\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nUsing GPy and some interactive visualisations for understanding GPR and applying on a real world data set\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nMCMC simulations for coin tosses!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nA programming introduction to Gaussian Processes.\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\nJAX\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nFrom the ground up!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsetup\n\n\n\n\nMy Mac Setup\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nA programming introduction to recommender systems using Keras!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nvisualisation\n\n\n\n\nSame graphic using different libraries!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nNeural networks to learn the embeddings! and how to combine them\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualisation\n\n\n\n\nPandas excellence in timeseries!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nWhat if we start from some prior!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsetup\n\n\n\n\nRunning Python scripts on server over ssh and getting back content\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsustainability\n\n\n\n\nHow is the world changing over the years!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nHow to learn the parameters of a GP\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nA programming introduction to HMMs for unfair casino problem\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\nTFP\n\n\nTF\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nvisualisation\n\n\n\n\nTowards amazing plots in research papers!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nA programming introduction to query by committee strategy for active learning\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nSignal processing for unequal time series!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\nTFP\n\n\nTF\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsetup\n\n\n\n\nSome of my shortcuts on the iPad\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nvisualisation\n\n\n\n\nExploring data in Matplotlib\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nConstrained NMF using CVXPY!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsetup\n\n\n\n\nBlurring an image selectively using Affinity Photo\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nMaximize based on what you know, re-estimate!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\nTFP\n\n\nTF\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nUsing GPy and some interactive visualisations for understanding GPR and applying on a real world data set\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nHow the simple least squares can be used in more ways than you thought!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nA programming introduction to Bayesian Linear Regression.\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nacademia\n\n\n\n\nHashMaps for programming interviews\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nWhat if we to predict for entries not within the matrix?!\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nProgrammatic introduction\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nDenoising\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nA programming introduction to NNs.\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmarkdown\n\n\n\n\nA minimal example of using markdown with fastpages.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\nLA\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nSimple scripts for downloading weather data\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nair quality\n\n\n\n\nAQ sensing in India\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nA programming introduction to Active Learning with Bayesian Linear Regression.\n\n\n\n\n\n\nZeel Patel & Nipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nHow to learn the parameters of a GP\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nOut of tensor factorisation\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualisation\n\n\n\n\nSimple scripts for downloading weather data\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nNipun Batra\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]